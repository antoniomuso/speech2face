{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antoniomuso/speech2face/blob/master/Decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qjzbm77dScuZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "31ceb990-2c26-45bc-db82-f316d3c9285a"
      },
      "source": [
        "! pip3 install face_recognition\n",
        "! pip3 install tensorflow-gpu==1.15\n",
        "\n",
        "import numpy as np\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, models, transforms\n",
        "from PIL import Image\n",
        "import face_recognition\n",
        "\n",
        "PATH = \"http://www.robots.ox.ac.uk/~albanie/models/pytorch-mcn/vgg_face_dag.pth\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting face_recognition\n",
            "  Downloading https://files.pythonhosted.org/packages/1e/95/f6c9330f54ab07bfa032bf3715c12455a381083125d8880c43cbe76bb3d0/face_recognition-1.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from face_recognition) (1.18.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from face_recognition) (7.0.0)\n",
            "Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.6/dist-packages (from face_recognition) (19.18.0)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.6/dist-packages (from face_recognition) (7.1.2)\n",
            "Collecting face-recognition-models>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cf/3b/4fd8c534f6c0d1b80ce0973d01331525538045084c73c153ee6df20224cf/face_recognition_models-0.3.0.tar.gz (100.1MB)\n",
            "\u001b[K     |████████████████████████████████| 100.2MB 42kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: face-recognition-models\n",
            "  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566172 sha256=4e2a3f9887e1de15330eef28b37b82064ea474a6a6ab0c4e9d7bc61230beaf47\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/99/18/59c6c8f01e39810415c0e63f5bede7d83dfb0ffc039865465f\n",
            "Successfully built face-recognition-models\n",
            "Installing collected packages: face-recognition-models, face-recognition\n",
            "Successfully installed face-recognition-1.3.0 face-recognition-models-0.3.0\n",
            "Collecting tensorflow-gpu==1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/ad/933140e74973fb917a194ab814785e7c23680ca5dee6d663a509fe9579b6/tensorflow_gpu-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (411.5MB)\n",
            "\u001b[K     |████████████████████████████████| 411.5MB 44kB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.12.1)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.7MB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.15.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 39.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (3.3.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.18.5)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.1.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 38.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.31.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.34.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (3.12.4)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.1.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.9.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15) (2.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (49.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.2.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.1.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=e3cf2da66848409fbe1372f25cb4f130c560ac86a451d1eae2ad80a979cd244f\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement tensorboard<3,>=2.3.0, but you'll have tensorboard 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement tensorflow-estimator<2.4.0,>=2.3.0, but you'll have tensorflow-estimator 1.15.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: keras-applications, gast, tensorboard, tensorflow-estimator, tensorflow-gpu\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qG2j8B-YYBK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "b242010b-a301-4749-8b71-93499b7a4c7b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paGzcQyzSlz2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download Dataset\n",
        "# Test Set\n",
        "# ! wget -x --load-cookies \"/content/drive/My Drive/Speech2Face/cookies.txt\" -O \"/content/drive/My Drive/Speech2Face/vggface2_test.tar.gz\" http://zeus.robots.ox.ac.uk/vgg_face2/get_file?fname=vggface2_test.tar.gz\n",
        "\n",
        "#! tar -zxvf \"/content/drive/My Drive/Speech2Face/vggface2_test.tar.gz\" -C \"/content/drive/My Drive/Speech2Face/data_test\"\n",
        "# Training Set \n",
        "# ! wget -x --load-cookies \"/content/drive/My Drive/Speech2Face/cookies.txt\" -O \"/content/drive/My Drive/Speech2Face/vggface2_test.tar.gz\" http://zeus.robots.ox.ac.uk/vgg_face2/get_file?fname=vggface2_test.tar.gz"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fK2477dIYhE4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "b3ab1a85-324c-4441-eb5c-e8f00120113d"
      },
      "source": [
        "#VGG-16 Face Encoder Class\n",
        "! pip install torchfile\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchfile\n",
        "\n",
        "\n",
        "\n",
        "class VGG_16(nn.Module):\n",
        "    \"\"\"\n",
        "    Main Class\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Constructor\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.block_size = [2, 2, 3, 3, 3]\n",
        "        self.conv_1_1 = nn.Conv2d(3, 64, 3, stride=1, padding=1)\n",
        "        self.conv_1_2 = nn.Conv2d(64, 64, 3, stride=1, padding=1)\n",
        "        self.conv_2_1 = nn.Conv2d(64, 128, 3, stride=1, padding=1)\n",
        "        self.conv_2_2 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
        "        self.conv_3_1 = nn.Conv2d(128, 256, 3, stride=1, padding=1)\n",
        "        self.conv_3_2 = nn.Conv2d(256, 256, 3, stride=1, padding=1)\n",
        "        self.conv_3_3 = nn.Conv2d(256, 256, 3, stride=1, padding=1)\n",
        "        self.conv_4_1 = nn.Conv2d(256, 512, 3, stride=1, padding=1)\n",
        "        self.conv_4_2 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
        "        self.conv_4_3 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
        "        self.conv_5_1 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
        "        self.conv_5_2 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
        "        self.conv_5_3 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
        "        self.fc6 = nn.Linear(512 * 7 * 7, 4096)\n",
        "        self.fc7 = nn.Linear(4096, 4096)\n",
        "        self.fc8 = nn.Linear(4096, 2622)\n",
        "\n",
        "    def load_weights(self, path=\"/content/drive/My Drive/Speech2Face/alt/vgg_weights/vgg_face_torch/VGG_FACE.t7\"):\n",
        "\n",
        "        \"\"\" Function to load luatorch pretrained\n",
        "        Args:\n",
        "            path: path for the luatorch pretrained\n",
        "        \"\"\"\n",
        "        model = torchfile.load(path)\n",
        "        counter = 1\n",
        "        block = 1\n",
        "        for i, layer in enumerate(model.modules):\n",
        "            if layer.weight is not None:\n",
        "                if block <= 5:\n",
        "                    self_layer = getattr(self, \"conv_%d_%d\" % (block, counter))\n",
        "                    counter += 1\n",
        "                    if counter > self.block_size[block - 1]:\n",
        "                        counter = 1\n",
        "                        block += 1\n",
        "                    self_layer.weight.data[...] = torch.tensor(layer.weight).view_as(self_layer.weight)[...]\n",
        "                    self_layer.bias.data[...] = torch.tensor(layer.bias).view_as(self_layer.bias)[...]\n",
        "                else:\n",
        "                    self_layer = getattr(self, \"fc%d\" % (block))\n",
        "                    block += 1\n",
        "                    self_layer.weight.data[...] = torch.tensor(layer.weight).view_as(self_layer.weight)[...]\n",
        "                    self_layer.bias.data[...] = torch.tensor(layer.bias).view_as(self_layer.bias)[...]\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" Pytorch forward\n",
        "        Args:\n",
        "            x: input image (224x224)\n",
        "        Returns: class logits\n",
        "        \"\"\"\n",
        "        x = F.relu(self.conv_1_1(x))\n",
        "        x = F.relu(self.conv_1_2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv_2_1(x))\n",
        "        x = F.relu(self.conv_2_2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv_3_1(x))\n",
        "        x = F.relu(self.conv_3_2(x))\n",
        "        x = F.relu(self.conv_3_3(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv_4_1(x))\n",
        "        x = F.relu(self.conv_4_2(x))\n",
        "        x = F.relu(self.conv_4_3(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv_5_1(x))\n",
        "        x = F.relu(self.conv_5_2(x))\n",
        "        x = F.relu(self.conv_5_3(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc6(x))\n",
        "        x = F.dropout(x, 0.5, self.training)\n",
        "        #return self.fc7(x) #new added\n",
        "        #return F.relu(self.fc7(x))\n",
        "\n",
        "        x = F.relu(self.fc7(x))\n",
        "        x = F.dropout(x, 0.5, self.training)\n",
        "\n",
        "        x = F.normalize(x, p=2, dim=1)\n",
        "        return x\n",
        "\n",
        "        #return self.fc8(x) \n",
        "\n",
        "\n",
        "    def forward_two(self, x):\n",
        "        \"\"\" Pytorch forward\n",
        "        Args:\n",
        "            x: input image (224x224)\n",
        "        Returns: class logits\n",
        "        \"\"\"\n",
        "        x = F.relu(self.conv_1_1(x))\n",
        "        x = F.relu(self.conv_1_2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv_2_1(x))\n",
        "        x = F.relu(self.conv_2_2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv_3_1(x))\n",
        "        x = F.relu(self.conv_3_2(x))\n",
        "        x = F.relu(self.conv_3_3(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv_4_1(x))\n",
        "        x = F.relu(self.conv_4_2(x))\n",
        "        x = F.relu(self.conv_4_3(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv_5_1(x))\n",
        "        x = F.relu(self.conv_5_2(x))\n",
        "        x = F.relu(self.conv_5_3(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc6(x))\n",
        "        x = F.dropout(x, 0.5, self.training)\n",
        "        x = F.relu(self.fc7(x))\n",
        "        x = F.dropout(x, 0.5, self.training)\n",
        "        return self.fc8(x)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchfile\n",
            "  Downloading https://files.pythonhosted.org/packages/91/af/5b305f86f2d218091af657ddb53f984ecbd9518ca9fe8ef4103a007252c9/torchfile-0.1.0.tar.gz\n",
            "Building wheels for collected packages: torchfile\n",
            "  Building wheel for torchfile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchfile: filename=torchfile-0.1.0-cp36-none-any.whl size=5712 sha256=db2d0ad4c9895db7bef978a5a2d81cfaab7e6d8b0b6d36e8058cf6898d819955\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/c3/d6/9a1cc8f3a99a0fc1124cae20153f36af59a6e683daca0a0814\n",
            "Successfully built torchfile\n",
            "Installing collected packages: torchfile\n",
            "Successfully installed torchfile-0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wc8SxYnBMVoT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from os.path import join\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "from time import time\n",
        "import face_recognition\n",
        "from random import randint\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "class Decoder_Dataset(Dataset):\n",
        "     \n",
        "\n",
        "    def __init__(self, Folder, VggL, sample, dev = \"cuda\"):\n",
        "\n",
        "        self.root     = Folder\n",
        "        self.dev      = dev \n",
        "        self.names    = os.listdir(self.root) \n",
        "        self.sample   = sample if sample < 20 else 20\n",
        "        self.a_length = len(os.listdir(self.root)) \n",
        "        self.length   = self.a_length * self.sample\n",
        "        self.img_name = self.root + \"{}.jpg\"\n",
        "        self.vgg_features = VggL\n",
        "        self.vgg = VGG_16()\n",
        "        self.vgg.load_weights()\n",
        "        self.vgg.cuda()\n",
        "        self.vgg.eval()\n",
        "        print(self.a_length)\n",
        "\n",
        "    def image_out(self,path, mean =[0.485, 0.456, 0.406] , std = [0.229, 0.224, 0.225]):#[0.485, 0.456, 0.406]\n",
        "        img = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n",
        "        # image = Image.open(path)\n",
        "        count = 0\n",
        "        resultant = []\n",
        "        try:\n",
        "            faceLocation = face_recognition.face_locations(img)[count]\n",
        "            x,y1,x1,y = faceLocation\n",
        "            img = img[x:x1,y:y1]\n",
        "            landmark = face_recognition.face_landmarks(img)\n",
        "            for i in list(landmark[0].keys()):\n",
        "                resultant += landmark[0][i]\n",
        "            landmark = np.ravel(np.array(resultant))\n",
        "            # img = cv2.resize(img, (224,224),  interpolation = cv2.INTER_AREA)\n",
        "            loader = transforms.Compose([\n",
        "                 transforms.ToPILImage(),                      \n",
        "                 transforms.Resize((224,224), interpolation = cv2.INTER_AREA),\n",
        "                 transforms.ToTensor(),\n",
        "                 transforms.Normalize(mean, std),\n",
        "             ])\n",
        "            \n",
        "            img = loader(img)\n",
        "            \n",
        "            img = torch.reshape(img, (-1, 3, 224, 224))\n",
        "            img = img.to(device)\n",
        "            biden_encoding = self.vgg(img)\n",
        "            # cv2_imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "            \n",
        "        except IndexError:\n",
        "            return None, None, None\n",
        "        \n",
        "        \n",
        "        return  biden_encoding, img.reshape(1,224,224,3), torch.tensor(landmark)\n",
        "\n",
        "\n",
        "\n",
        "    def pseudo_idx(self,idx):\n",
        "        if idx < self.a_length:\n",
        "            return idx\n",
        "        else:\n",
        "            ## return self.pseudo_idx(idx - self.a_length) ## RECURSION\n",
        "            return idx // self.sample \n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        while True:\n",
        "            index = idx\n",
        "            index = self.pseudo_idx(index)\n",
        "            name  = index \n",
        "            path =  self.img_name.format(name)\n",
        "            # print(path)\n",
        "\n",
        "            if not os.path.exists(path):\n",
        "                idx = randint(0, self.a_length) # IF FILE PATH DOESNOT EXISTS\n",
        "                continue\n",
        "\n",
        "            feature, image, landmark= self.image_out(path)\n",
        "            if image is None:\n",
        "                idx = randint(0, self.a_length) # IF FILE PATH DOESNOT EXISTS\n",
        "                continue\n",
        "\n",
        "            image = image.view(3,224,224).float()\n",
        "            return   feature.float(), image, landmark.float()"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYgo0KW3MYoN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############### THIS CELL IS COPIED FROM DEEP SPEECH MOZILLA #########################\n",
        "\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tfv1\n",
        "from tensorflow.compat import dimension_value\n",
        "from tensorflow.contrib.image import dense_image_warp\n",
        "from tensorflow.contrib.image import interpolate_spline\n",
        "\n",
        "from tensorflow.python.framework import ops\n",
        "from tensorflow.python.ops import array_ops\n",
        "\n",
        "def _to_float32(value):\n",
        "    return tf.cast(value, tf.float32)\n",
        "\n",
        "def _to_int32(value):\n",
        "    return tf.cast(value, tf.int32)\n",
        "\n",
        "def _get_grid_locations(image_height, image_width):\n",
        "    \"\"\"Wrapper for np.meshgrid.\"\"\"\n",
        "    tfv1.assert_type(image_height, tf.int32)\n",
        "    tfv1.assert_type(image_width, tf.int32)\n",
        "\n",
        "    y_range = tf.range(image_height)\n",
        "    x_range = tf.range(image_width)\n",
        "    y_grid, x_grid = tf.meshgrid(y_range, x_range, indexing='ij')\n",
        "    return tf.stack((y_grid, x_grid), -1)\n",
        "\n",
        "\n",
        "def _expand_to_minibatch(tensor, batch_size):\n",
        "    \"\"\"Tile arbitrarily-sized np_array to include new batch dimension.\"\"\"\n",
        "    ndim = tf.size(tf.shape(tensor))\n",
        "    ones = tf.ones((ndim,), tf.int32)\n",
        "\n",
        "    tiles = tf.concat(([batch_size], ones), 0)\n",
        "    return tf.tile(tf.expand_dims(tensor, 0), tiles)\n",
        "\n",
        "\n",
        "def _get_boundary_locations(image_height, image_width, num_points_per_edge):\n",
        "    \"\"\"Compute evenly-spaced indices along edge of image.\"\"\"\n",
        "    image_height_end = _to_float32(tf.math.subtract(image_height, 1))\n",
        "    image_width_end = _to_float32(tf.math.subtract(image_width, 1))\n",
        "    y_range = tf.linspace(0.0, image_height_end, num_points_per_edge + 2)\n",
        "    x_range = tf.linspace(0.0, image_height_end, num_points_per_edge + 2)\n",
        "    ys, xs = tf.meshgrid(y_range, x_range, indexing='ij')\n",
        "    is_boundary = tf.logical_or(\n",
        "        tf.logical_or(tf.equal(xs, 0.0), tf.equal(xs, image_width_end)),\n",
        "        tf.logical_or(tf.equal(ys, 0.0), tf.equal(ys, image_height_end)))\n",
        "    return tf.stack([tf.boolean_mask(ys, is_boundary), tf.boolean_mask(xs, is_boundary)], axis=-1)\n",
        "\n",
        "\n",
        "def _add_zero_flow_controls_at_boundary(control_point_locations,\n",
        "                                        control_point_flows, image_height,\n",
        "                                        image_width, boundary_points_per_edge):\n",
        "    \"\"\"Add control points for zero-flow boundary conditions.\n",
        "     Augment the set of control points with extra points on the\n",
        "     boundary of the image that have zero flow.\n",
        "    Args:\n",
        "      control_point_locations: input control points\n",
        "      control_point_flows: their flows\n",
        "      image_height: image height\n",
        "      image_width: image width\n",
        "      boundary_points_per_edge: number of points to add in the middle of each\n",
        "                             edge (not including the corners).\n",
        "                             The total number of points added is\n",
        "                             4 + 4*(boundary_points_per_edge).\n",
        "    Returns:\n",
        "      merged_control_point_locations: augmented set of control point locations\n",
        "      merged_control_point_flows: augmented set of control point flows\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size = dimension_value(tf.shape(control_point_locations)[0])\n",
        "\n",
        "    boundary_point_locations = _get_boundary_locations(image_height, image_width,\n",
        "                                                       boundary_points_per_edge)\n",
        "    boundary_point_shape = tf.shape(boundary_point_locations)\n",
        "    boundary_point_flows = tf.zeros([boundary_point_shape[0], 2])\n",
        "\n",
        "    minbatch_locations = _expand_to_minibatch(boundary_point_locations, batch_size)\n",
        "    type_to_use = control_point_locations.dtype\n",
        "    boundary_point_locations = tf.cast(minbatch_locations, type_to_use)\n",
        "\n",
        "    minbatch_flows = _expand_to_minibatch(boundary_point_flows, batch_size)\n",
        "\n",
        "    boundary_point_flows = tf.cast(minbatch_flows, type_to_use)\n",
        "\n",
        "    merged_control_point_locations = tf.concat(\n",
        "        [control_point_locations, boundary_point_locations], 1)\n",
        "\n",
        "    merged_control_point_flows = tf.concat(\n",
        "        [control_point_flows, boundary_point_flows], 1)\n",
        "\n",
        "    return merged_control_point_locations, merged_control_point_flows\n",
        "\n",
        "\n",
        "def sparse_image_warp(image,\n",
        "                      source_control_point_locations,\n",
        "                      dest_control_point_locations,\n",
        "                      interpolation_order=2,\n",
        "                      regularization_weight=0.0,\n",
        "                      num_boundary_points=0,\n",
        "                      name='sparse_image_warp'):\n",
        "    \"\"\"Image warping using correspondences between sparse control points.\n",
        "    Apply a non-linear warp to the image, where the warp is specified by\n",
        "    the source and destination locations of a (potentially small) number of\n",
        "    control points. First, we use a polyharmonic spline\n",
        "    (`tf.contrib.image.interpolate_spline`) to interpolate the displacements\n",
        "    between the corresponding control points to a dense flow field.\n",
        "    Then, we warp the image using this dense flow field\n",
        "    (`tf.contrib.image.dense_image_warp`).\n",
        "    Let t index our control points. For regularization_weight=0, we have:\n",
        "    warped_image[b, dest_control_point_locations[b, t, 0],\n",
        "                    dest_control_point_locations[b, t, 1], :] =\n",
        "    image[b, source_control_point_locations[b, t, 0],\n",
        "             source_control_point_locations[b, t, 1], :].\n",
        "    For regularization_weight > 0, this condition is met approximately, since\n",
        "    regularized interpolation trades off smoothness of the interpolant vs.\n",
        "    reconstruction of the interpolant at the control points.\n",
        "    See `tf.contrib.image.interpolate_spline` for further documentation of the\n",
        "    interpolation_order and regularization_weight arguments.\n",
        "    Args:\n",
        "      image: `[batch, height, width, channels]` float `Tensor`\n",
        "      source_control_point_locations: `[batch, num_control_points, 2]` float\n",
        "        `Tensor`\n",
        "      dest_control_point_locations: `[batch, num_control_points, 2]` float\n",
        "        `Tensor`\n",
        "      interpolation_order: polynomial order used by the spline interpolation\n",
        "      regularization_weight: weight on smoothness regularizer in interpolation\n",
        "      num_boundary_points: How many zero-flow boundary points to include at\n",
        "        each image edge.Usage:\n",
        "          num_boundary_points=0: don't add zero-flow points\n",
        "          num_boundary_points=1: 4 corners of the image\n",
        "          num_boundary_points=2: 4 corners and one in the middle of each edge\n",
        "            (8 points total)\n",
        "          num_boundary_points=n: 4 corners and n-1 along each edge\n",
        "      name: A name for the operation (optional).\n",
        "      Note that image and offsets can be of type tf.half, tf.float32, or\n",
        "      tf.float64, and do not necessarily have to be the same type.\n",
        "    Returns:\n",
        "      warped_image: `[batch, height, width, channels]` float `Tensor` with same\n",
        "        type as input image.\n",
        "      flow_field: `[batch, height, width, 2]` float `Tensor` containing the dense\n",
        "        flow field produced by the interpolation.\n",
        "    \"\"\"\n",
        "\n",
        "    image = ops.convert_to_tensor(image)\n",
        "    source_control_point_locations = ops.convert_to_tensor(\n",
        "        source_control_point_locations)\n",
        "    dest_control_point_locations = ops.convert_to_tensor(\n",
        "        dest_control_point_locations)\n",
        "\n",
        "    control_point_flows = (\n",
        "        dest_control_point_locations - source_control_point_locations)\n",
        "\n",
        "    clamp_boundaries = num_boundary_points > 0\n",
        "    boundary_points_per_edge = num_boundary_points - 1\n",
        "\n",
        "    with ops.name_scope(name):\n",
        "        image_shape = tf.shape(image)\n",
        "        batch_size, image_height, image_width = image_shape[0], image_shape[1], image_shape[2]\n",
        "\n",
        "        # This generates the dense locations where the interpolant\n",
        "        # will be evaluated.\n",
        "        grid_locations = _get_grid_locations(image_height, image_width)\n",
        "\n",
        "        flattened_grid_locations = tf.reshape(grid_locations,\n",
        "                                              [tf.multiply(image_height, image_width), 2])\n",
        "\n",
        "        # flattened_grid_locations = constant_op.constant(\n",
        "        #     _expand_to_minibatch(flattened_grid_locations, batch_size), image.dtype)\n",
        "        flattened_grid_locations = _expand_to_minibatch(flattened_grid_locations, batch_size)\n",
        "        flattened_grid_locations = tf.cast(flattened_grid_locations, dtype=image.dtype)\n",
        "\n",
        "        if clamp_boundaries:\n",
        "            (dest_control_point_locations,\n",
        "             control_point_flows) = _add_zero_flow_controls_at_boundary(\n",
        "                 dest_control_point_locations, control_point_flows, image_height,\n",
        "                 image_width, boundary_points_per_edge)\n",
        "\n",
        "        flattened_flows = interpolate_spline(\n",
        "            dest_control_point_locations, control_point_flows,\n",
        "            flattened_grid_locations, interpolation_order, regularization_weight)\n",
        "\n",
        "        dense_flows = array_ops.reshape(flattened_flows,\n",
        "                                        [batch_size, image_height, image_width, 2])\n",
        "\n",
        "        warped_image = dense_image_warp(image, dense_flows)\n",
        "\n",
        "        return warped_image, dense_flows"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EQvfmnDMh1v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def image_warping(src_img, src_landmarks, dest_landmarks):\n",
        "    # expanded_src_landmarks = np.expand_dims(np.float32(src_landmarks), axis=0)\n",
        "    # expanded_dest_landmarks = np.expand_dims(np.float32(dest_landmarks), axis=0)\n",
        "    # expanded_src_img = np.expand_dims(np.float32(src_img) / 255, axis=0)\n",
        "\n",
        "    warped_img, dense_flows = sparse_image_warp(src_img,\n",
        "                          src_landmarks,\n",
        "                          dest_landmarks,\n",
        "                          interpolation_order=1,\n",
        "                          regularization_weight=0.1,\n",
        "                          num_boundary_points=2,\n",
        "                          name='sparse_image_warp')\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        out_img = sess.run(warped_img)\n",
        "        warp_img = np.uint8(out_img[:, :, :, :] * 255)\n",
        "    \n",
        "    return warp_img\n",
        "    \n",
        "def face_landmark(img):\n",
        "    X = np.zeros((img.shape[0], 72 ,2))\n",
        "    flag = []\n",
        "    for i in range(img.shape[0]):\n",
        "\n",
        "        landmark = face_recognition.face_landmarks(img[i].reshape(224,224,3))\n",
        "        resultant = []\n",
        "        try:\n",
        "            for j in list(landmark[0].keys()):\n",
        "                resultant += landmark[0][j] \n",
        "        except IndexError:\n",
        "            flag.append(i)\n",
        "            continue\n",
        "        X[i] = np.array(resultant)\n",
        "    return  X, flag"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrbRKR8qMqSo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "from torch import nn\n",
        "\n",
        "class DECODER(nn.Module):\n",
        "    def __init__(self, phase):\n",
        "        super(DECODER, self).__init__()\n",
        "        self.phase = phase\n",
        "        self.fc3 = nn.Linear(4096, 1000)\n",
        "        self.ReLU = nn.ReLU()\n",
        "        #self.fc_bn3 = nn.BatchNorm1d(1000)\n",
        "\n",
        "\n",
        "        self.fc4 = nn.Linear(1000, 14 * 14 * 64)\n",
        "        self.fc_bn4 = nn.BatchNorm1d(14 * 14 * 64)\n",
        "        def TransConv( i, kernal = 5, stride = 2, inp = None):\n",
        "            if not inp:\n",
        "                inp = max(256//2**(i-1), 32)\n",
        "\n",
        "            layer =  nn.Sequential(\n",
        "                nn.ConvTranspose2d(inp, max(256//2**i, 32), \n",
        "                                kernal, stride=stride, padding=2, output_padding=1, \n",
        "                                dilation=1, padding_mode='zeros'),\n",
        "                nn.ReLU(),\n",
        "                nn.BatchNorm2d(max(256//2**i, 32)))\n",
        "            return layer\n",
        "        self.T1_ = TransConv(1, inp = 64)\n",
        "        self.T2_ = TransConv(2)\n",
        "        self.T3_ = TransConv(3)\n",
        "        self.T4_ = TransConv(4)\n",
        "    \n",
        "        self.ConvLast = nn.Sequential(\n",
        "            nn.Conv2d(32, 3, (1,1), stride=1),\n",
        "            nn.BatchNorm2d(3),\n",
        "            nn.ReLU())\n",
        "\n",
        "\n",
        "        self.layerLandmark1 = nn.Linear(1000, 800)\n",
        "        self.layerLandmark2 = nn.Linear(800, 600)\n",
        "        self.layerLandmark3 = nn.Linear(600, 400)\n",
        "        self.layerLandmark4 = nn.Linear(400, 200)\n",
        "        self.layerLandmark5 = nn.Linear(200, 144)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        L1 = self.fc3(x)\n",
        "        L1 = self.ReLU(L1)\n",
        "\n",
        "\n",
        "        L2 = self.layerLandmark1(L1)\n",
        "        L2 = self.ReLU(L2)\n",
        "\n",
        "        L3 = self.layerLandmark2(L2)\n",
        "        L3 = self.ReLU(L3)\n",
        "\n",
        "        L4 = self.layerLandmark3(L3)\n",
        "        L4 = self.ReLU(L4)\n",
        "\n",
        "        L5 = self.layerLandmark4(L4)\n",
        "        L5 = self.ReLU(L5)\n",
        "\n",
        "        L6 = self.layerLandmark5(L5)\n",
        "        outL = self.ReLU(L6)\n",
        "\n",
        "\n",
        "        # B1 = self.fc_bn3(L1) \n",
        "        T0 = self.fc4(L1) \n",
        "        T0 = self.ReLU(T0)\n",
        "        # T0 = self.fc_bn4(T0)\n",
        "        T0 = T0.view(-1,64,14,14)\n",
        "\n",
        "\n",
        "\n",
        "        T1 = self.T1_(T0)\n",
        "        T2 = self.T2_(T1)\n",
        "        T3 = self.T3_(T2)\n",
        "        T4 = self.T4_(T3)\n",
        "\n",
        "        outT = self.ConvLast(T4)\n",
        "        if self.phase == \"train\":\n",
        "            return outL,  outT \n",
        "        elif self.phase == \"eval\":\n",
        "            img = outT.cpu().detach().numpy().reshape(-1, 224, 224, 3)*255\n",
        "            outL = outL.cpu().detach().numpy()\n",
        "            outL = np.dstack((outL[:,0::2],outL[:,1::2]))\n",
        "            #print(\"land np img np \", outL_.shape, img_.shape)\n",
        "            img = (img.reshape(-1,224,224,3)*255).astype(np.uint8)\n",
        "            #print(\"img_t \",img_t.shape, img_t[0])\n",
        "            src, flag = face_landmark(img)\n",
        "            if flag:\n",
        "                for r in flag:\n",
        "                    src[r] = outL[r]\n",
        "\n",
        "            return image_warping(img.astype(np.float32), src.astype(np.float32), outL.astype(np.float32))\n",
        "\n",
        "            \n",
        "        # outN = outT.numpy()\n",
        "        # outLN = outL.numpy()\n",
        "        # src, flag = face_landmark(outN)\n",
        "        # if flag:\n",
        "        #     for r in flag:\n",
        "        #         src[r] = outLN[r]\n",
        "        \n",
        "        # IMG = torch.from_numpy(image_warping(outN, src, outLN))\n",
        "        \n",
        "        # if self.phase == \"train\":\n",
        "        #     return outL,  outT , VGGL(IMG)\n",
        "        # if self.phase == \"test\":\n",
        "        #     return IMG"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQLzfoqsNdJ6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 761
        },
        "outputId": "52713ea1-4f61-47d7-9851-1fbb69b447fc"
      },
      "source": [
        "\n",
        "from torchsummary import summary\n",
        "device = \"cuda\" #torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = DECODER(\"train\")\n",
        "model = model.cuda()\n",
        "summary(model, input_size=(4096,))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                 [-1, 1000]       4,097,000\n",
            "              ReLU-2                 [-1, 1000]               0\n",
            "            Linear-3                  [-1, 800]         800,800\n",
            "              ReLU-4                  [-1, 800]               0\n",
            "            Linear-5                  [-1, 600]         480,600\n",
            "              ReLU-6                  [-1, 600]               0\n",
            "            Linear-7                  [-1, 400]         240,400\n",
            "              ReLU-8                  [-1, 400]               0\n",
            "            Linear-9                  [-1, 200]          80,200\n",
            "             ReLU-10                  [-1, 200]               0\n",
            "           Linear-11                  [-1, 144]          28,944\n",
            "             ReLU-12                  [-1, 144]               0\n",
            "           Linear-13                [-1, 12544]      12,556,544\n",
            "             ReLU-14                [-1, 12544]               0\n",
            "  ConvTranspose2d-15          [-1, 128, 28, 28]         204,928\n",
            "             ReLU-16          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-17          [-1, 128, 28, 28]             256\n",
            "  ConvTranspose2d-18           [-1, 64, 56, 56]         204,864\n",
            "             ReLU-19           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-20           [-1, 64, 56, 56]             128\n",
            "  ConvTranspose2d-21         [-1, 32, 112, 112]          51,232\n",
            "             ReLU-22         [-1, 32, 112, 112]               0\n",
            "      BatchNorm2d-23         [-1, 32, 112, 112]              64\n",
            "  ConvTranspose2d-24         [-1, 32, 224, 224]          25,632\n",
            "             ReLU-25         [-1, 32, 224, 224]               0\n",
            "      BatchNorm2d-26         [-1, 32, 224, 224]              64\n",
            "           Conv2d-27          [-1, 3, 224, 224]              99\n",
            "      BatchNorm2d-28          [-1, 3, 224, 224]               6\n",
            "             ReLU-29          [-1, 3, 224, 224]               0\n",
            "================================================================\n",
            "Total params: 18,771,761\n",
            "Trainable params: 18,771,761\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.02\n",
            "Forward/backward pass size (MB): 56.51\n",
            "Params size (MB): 71.61\n",
            "Estimated Total Size (MB): 128.14\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeIkEyJ8NkgU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8dec6b51-a6bc-4676-82c0-60ff53bd3199"
      },
      "source": [
        "import os\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.009)\n",
        "criterion_1 = nn.MSELoss()\n",
        "criterion_2 = nn.L1Loss()\n",
        "criterion_3 = nn.CosineEmbeddingLoss()\n",
        "alpha = 0.0002\n",
        "beta  = 1.0\n",
        "#gamma = 1.5\n",
        "l = 10\n",
        "BATCH = 20\n",
        "WORKER = 0\n",
        "SAMPLE = 20\n",
        "num_epochs = 5\n",
        "sim = torch.ones((BATCH,1, 128 ))\n",
        "train_dataset = Decoder_Dataset(\"/content/drive/My Drive/Speech2Face/data_test/faces/\", sample = SAMPLE, VggL = \"vgg16\")\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "\t\t                                           batch_size=BATCH, \n",
        "\t\t                                           num_workers =WORKER,\n",
        "\t\t                                           shuffle=False)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3630\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPstvvpLN1He",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#! mkdir checkpoint"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YymSd2y4TBUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nvidia_smi\n",
        "import psutil\n",
        "import platform\n",
        "\n",
        "nvidia_smi.nvmlInit()\n",
        "handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Blqj7ckOBDZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 926
        },
        "outputId": "935f2bd7-c514-4a91-8a32-ccf5c477b802"
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(\"Epoch\", epoch)\n",
        "    running_loss = 0.0\n",
        "    running_loss_1 = 0.0\n",
        "    running_loss_2 = 0.0\n",
        "    start_time = time()\n",
        "    for  i,(feature, img, landm) in enumerate(train_loader):\n",
        "        \n",
        "        # Move tensors to the configured device\n",
        "        img = img.to(device)\n",
        "        landm = landm.to(device)\n",
        "        #print(\"input\",img.size(), landm.size())\n",
        "        feature = feature.to(device)\n",
        "        #print(\"vgg\",feature.size())\n",
        "        # Forward pass\n",
        "        outL, outT = model(feature)\n",
        "        #print(\"output \", outL.size(), outT.size())\n",
        "        \n",
        "        outL = outL.squeeze(1)\n",
        "\n",
        "        loss_1 = criterion_1(outL, landm)\n",
        "        loss_2 = criterion_2(outT, img)\n",
        "        running_loss_1 += loss_1.item()\n",
        "        running_loss_2 += loss_2.item()\n",
        "\n",
        "        # img_ = outT.cpu().detach().numpy()\n",
        "        # outL_ = outL.cpu().detach().numpy()\n",
        "        # outL_ = np.dstack((outL_[:,0::2],outL_[:,1::2]))\n",
        "        # #print(\"land np img np \", outL_.shape, img_.shape)\n",
        "        # img_t = (img_.reshape(-1,224,224,3)*255).astype(np.uint8)\n",
        "        # #print(\"img_t \",img_t.shape, img_t[0])\n",
        "        # if epoch in [0, 1]:\n",
        "        #     src, flag    = face_landmark(img_t)\n",
        "        #     if flag:\n",
        "        #         for f in flag:\n",
        "        #             src[f] = outL_[f]\n",
        "        # else:\n",
        "\n",
        "        #     outL_ = landm.cpu().detach().numpy()\n",
        "\n",
        "        # img_ = image_warping(img_.astype(np.float32), src.astype(np.float32), outL_.astype(np.float32)).to(device)\n",
        "\n",
        "        # # print(img_.size(), type(img_))\n",
        "        # # print(feature.size())\n",
        "        # feature_out = vgg16(img_)\n",
        "        # # print(feature_out.size(), sim.size())\n",
        "\n",
        "        # loss_3 = criterion_3(feature_out.view(BATCH,1,4096).to(device), feature.view(BATCH,1,4096), sim.view(BATCH,1,4096).to(device))\n",
        "        # running_loss_3 += loss_3.item()\n",
        "\n",
        "\n",
        "\n",
        "        loss = (alpha*loss_1 + beta*loss_2 )*100\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i % l == 0:\n",
        "            # writer.add_scalar('training loss',running_loss/  data[\"show\"],epoch * len(train_loader) + i)\n",
        "            res = nvidia_smi.nvmlDeviceGetUtilizationRates(handle)\n",
        "            print('{} : Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, gpu: {}%, gpu-mem: {}%, RAM: {}% , RAM Mem {}%, running_loss: {:.4f}, MSEloss {:.4f}, MAEloss {:.4f},  time: {:.4f},  '.format(datetime.now() ,epoch+1, num_epochs, i,int((train_dataset.__len__()/BATCH)),  loss.item(),res.gpu, res.memory,psutil.cpu_percent(),psutil.virtual_memory()[2],running_loss/(BATCH*l),running_loss_1/(BATCH*l), running_loss_2/(BATCH*l), (time()- start_time)))\n",
        "            #print(f'gpu: {res.gpu}%, gpu-mem: {res.memory}%, RAM: {psutil.cpu_percent()}% , RAM Mem {psutil.virtual_memory()[2]}%')\n",
        "            running_loss = 0.0\n",
        "            start_time = time()\n",
        "            running_loss_1 = 0.0\n",
        "            running_loss_2 = 0.0\n",
        "            result = outT.cpu().detach().numpy()[0].squeeze().astype(np.uint8)*255\n",
        "\n",
        "            cv2_imshow(np.reshape(result, (224,224, 3)))\n",
        "    \n",
        "    torch.save({'epoch': epoch,'model_state_dict': model.state_dict(),'optimizer_state_dict': optimizer.state_dict(), 'loss': loss}, \"/content/drive/My Drive/Speech2Face/check_poit_decoder/epoch_{}.pth\".format(epoch + 1))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "2020-08-14 09:08:20.976910 : Epoch [1/5], Step [0/3630], Loss: 170.3514, gpu: 93%, gpu-mem: 60%, RAM: 8.6% , RAM Mem 18.1%, running_loss: 0.8518, MSEloss 21.9833, MAEloss 0.0041,  time: 3.1461,  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAIAAACVT/22AAAEOklEQVR4nO3da3PaOhAA0E3+/x/NTO/No+RVYu/9gBEyOKXT3gZLnMMkLB6SsWb9WkmGCAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD+wBgZH+XVMIvyAusDAMBVyYUI+Fved085xiZj94iXHHNTdsC33Ic/ImL8+lXkV2T5NV9EM+QLLiwzdntih9ei8yZ118B55rprXp+yPOeUvPoRVSSRq3Z8cDk9jrbvtD2dtO45h4wxMvI+Ix/HXUuHvB/3u98mx3wurd2WvxyW/t2qdZnAmDbO3MeHejA7amO/2Yv5qfAQ9ZU+WLndcTSrS5hDUdiw88Vf4w2M5TZ2kLorcHrOW37s38NKLZWBp8vb1911aEbshv4eSs33VKrCh4zcTCfEMTblmPoeGT/m/2L9llazkVX/RZn1bncoCqt3fPEa/WV9NWeWvBL31UZYtfrIOS3pZg/8rPrrpoF73V2lXYOl3s959SeRbZGvVhzmg34vu99zjvlUekNfctzPE40oxV9dBo7RYMabW+EzluaD9tZGAP6UMwPrpfiDr7SNLCOCEa/HI38Z8RYZb2VXPHyGzEc4m6zIQgeo7AAAAEuGyP2QYOxme+7Kh9foaOjv6sgQAADwU+MsGhaWKysAAAAAAAAAAAAAAAAAAAAAAAAAAKAPPmsUAADgIobyNRDvkeXb4Mfd98hPsaINAOAycha5JmvKZ8mTRi7ndl/9Pcc27qd4iMfyzfD3McbLFH/E66wqbEWW5zFyepWRpbJNu2MLshw5c3pEed6/Ay4gf/KK9buSjNUnv/q0qJxYvyy/c7qQqS9e6p/29dGKSn2JUl93jtVO2GGzO3Fbv8jIiJveumLGeJyit7iLISJuIuJbvMfdtPwj7ktR+BBjvE/xENuvXNHfVtcMQ4xlJxxLUZhV1ENOu1KXe/uU1XGUZcdFIXyRn5WBNsg1u72O9NxExPz4mNUrp72VO675cuqGn/eJ7pccRw1pcqXPm/e6HAYjxpLJiOZz179DGuddaO0b4inepvgutvHPFL/GXZkbeh/v1UjhS1n+9slGvDIZp10wh+h0+YqbcoXqM11WZWDdJ3ooA+WOL7W0ufVTBt6efwvrti8A9+YDEy1vnFdjnrKlorC3wYkunZ8PKnkt6SNTs/mg/07xEA9l6O9bDPE0xdt4LM1+jY9SOWYMCx1R63HaBXNcFNYTZXo5Ld5cegX+b5/PBz1dDhdzviqEi3JPUrvmt7AYym2O4q8tV9APmqWY28ZTGdb7Xt0kOMamDP1tYiwFYj0ftI0N+HjmhAkxjTlOX2+3QwDAGQqIHsgdq5DxEcMUv1SfBLON76Uo/Gzkr6VPiNEF0xnjfgAA8IeGKtpXd9s43PWXbdV8AAAAAAAAAAAAAADwuf8A3w/X2wABjPwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=224x224 at 0x7FC3A3945978>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-14 09:10:18.415865 : Epoch [1/5], Step [10/3630], Loss: 190.4684, gpu: 93%, gpu-mem: 56%, RAM: 13.3% , RAM Mem 18.0%, running_loss: 8.8644, MSEloss 254.4976, MAEloss 0.0377,  time: 117.4385,  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAIAAACVT/22AAAAqUlEQVR4nO3BMQEAAADCoPVPbQlPoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAvgZM/gABE/clzwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=224x224 at 0x7FC3A392E470>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-648fe1f60e82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mrunning_loss_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m  \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlandm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# Move tensors to the configured device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-93a3eff189d0>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlandmark\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma_length\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# IF FILE PATH DOESNOT EXISTS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-93a3eff189d0>\u001b[0m in \u001b[0;36mimage_out\u001b[0;34m(self, path, mean, std)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mimage_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.485\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.456\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.406\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.229\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.225\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#[0.485, 0.456, 0.406]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;31m# image = Image.open(path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jg7ZJ4Leeau0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = DECODER('eval')\n",
        "state = torch.load('/content/drive/My Drive/Speech2Face/check_poit_decoder/epoch_15.pth')\n",
        "model.load_state_dict(state['model_state_dict'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEGpXCDXecvg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"/content/drive/My Drive/Speech2Face/data_test/test/n000078/0002_02.jpg\"\n",
        "img = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n",
        "faceLoc = face_recognition.face_locations(img)\n",
        "\n",
        "x,y1,x1,y = faceLoc[0]\n",
        "img = img[x:x1,y:y1]\n",
        "Image.fromarray(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC9CvkPJe77Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img = cv2.resize(img, (224,224), interpolation = cv2.INTER_AREA)\n",
        "biden_encoding = face_recognition.face_encodings(img)[0]\n",
        "encoded = torch.reshape(torch.from_numpy(biden_encoding).float(), (-1, 128))\n",
        "\n",
        "outT = model(encoded) \n",
        "out = torch.tensor(outT)\n",
        "out = np.squeeze(out)\n",
        "out = out.reshape(224,224,3)\n",
        "\n",
        "Image.fromarray(out.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}