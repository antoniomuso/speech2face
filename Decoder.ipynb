{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antoniomuso/speech2face/blob/master/Decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qjzbm77dScuZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "outputId": "d991253a-3b96-4fd3-acf8-26a0ec00aad1"
      },
      "source": [
        "! pip3 install face_recognition\n",
        "! pip3 install tensorflow-gpu==1.15\n",
        "\n",
        "import numpy as np\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, models, transforms\n",
        "from PIL import Image\n",
        "import face_recognition\n",
        "\n",
        "PATH = \"http://www.robots.ox.ac.uk/~albanie/models/pytorch-mcn/vgg_face_dag.pth\""
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: face_recognition in /usr/local/lib/python3.6/dist-packages (1.3.0)\n",
            "Requirement already satisfied: face-recognition-models>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from face_recognition) (0.3.0)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.6/dist-packages (from face_recognition) (7.1.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from face_recognition) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from face_recognition) (1.18.5)\n",
            "Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.6/dist-packages (from face_recognition) (19.18.0)\n",
            "Requirement already satisfied: tensorflow-gpu==1.15 in /usr/local/lib/python3.6/dist-packages (1.15.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.12.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.18.5)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.2.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.9.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.15.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.8.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.34.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (3.12.4)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.0.8)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.2.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (49.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.2.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qG2j8B-YYBK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "31d1669c-3447-4fcb-cbbc-d3ad24455654"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paGzcQyzSlz2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download Dataset\n",
        "# Test Set\n",
        "# ! wget -x --load-cookies \"/content/drive/My Drive/Speech2Face/cookies.txt\" -O \"/content/drive/My Drive/Speech2Face/vggface2_test.tar.gz\" http://zeus.robots.ox.ac.uk/vgg_face2/get_file?fname=vggface2_test.tar.gz\n",
        "\n",
        "#! tar -zxvf \"/content/drive/My Drive/Speech2Face/vggface2_test.tar.gz\" -C \"/content/drive/My Drive/Speech2Face/data_test\"\n",
        "# Training Set \n",
        "# ! wget -x --load-cookies \"/content/drive/My Drive/Speech2Face/cookies.txt\" -O \"/content/drive/My Drive/Speech2Face/vggface2_test.tar.gz\" http://zeus.robots.ox.ac.uk/vgg_face2/get_file?fname=vggface2_test.tar.gz"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fK2477dIYhE4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ff537a8e-75a1-401c-bf1d-c7919164a047"
      },
      "source": [
        "#VGG-16 Face Encoder Class\n",
        "! pip install torchfile\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchfile\n",
        "\n",
        "\n",
        "\n",
        "class VGG_16(nn.Module):\n",
        "    \"\"\"\n",
        "    Main Class\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Constructor\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.block_size = [2, 2, 3, 3, 3]\n",
        "        self.conv_1_1 = nn.Conv2d(3, 64, 3, stride=1, padding=1)\n",
        "        self.conv_1_2 = nn.Conv2d(64, 64, 3, stride=1, padding=1)\n",
        "        self.conv_2_1 = nn.Conv2d(64, 128, 3, stride=1, padding=1)\n",
        "        self.conv_2_2 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
        "        self.conv_3_1 = nn.Conv2d(128, 256, 3, stride=1, padding=1)\n",
        "        self.conv_3_2 = nn.Conv2d(256, 256, 3, stride=1, padding=1)\n",
        "        self.conv_3_3 = nn.Conv2d(256, 256, 3, stride=1, padding=1)\n",
        "        self.conv_4_1 = nn.Conv2d(256, 512, 3, stride=1, padding=1)\n",
        "        self.conv_4_2 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
        "        self.conv_4_3 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
        "        self.conv_5_1 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
        "        self.conv_5_2 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
        "        self.conv_5_3 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
        "        self.fc6 = nn.Linear(512 * 7 * 7, 4096)\n",
        "        self.fc7 = nn.Linear(4096, 4096)\n",
        "        self.fc8 = nn.Linear(4096, 2622)\n",
        "\n",
        "    def load_weights(self, path=\"/content/drive/My Drive/Speech2Face/alt/vgg_weights/vgg_face_torch/VGG_FACE.t7\"):\n",
        "\n",
        "        \"\"\" Function to load luatorch pretrained\n",
        "        Args:\n",
        "            path: path for the luatorch pretrained\n",
        "        \"\"\"\n",
        "        model = torchfile.load(path)\n",
        "        counter = 1\n",
        "        block = 1\n",
        "        for i, layer in enumerate(model.modules):\n",
        "            if layer.weight is not None:\n",
        "                if block <= 5:\n",
        "                    self_layer = getattr(self, \"conv_%d_%d\" % (block, counter))\n",
        "                    counter += 1\n",
        "                    if counter > self.block_size[block - 1]:\n",
        "                        counter = 1\n",
        "                        block += 1\n",
        "                    self_layer.weight.data[...] = torch.tensor(layer.weight).view_as(self_layer.weight)[...]\n",
        "                    self_layer.bias.data[...] = torch.tensor(layer.bias).view_as(self_layer.bias)[...]\n",
        "                else:\n",
        "                    self_layer = getattr(self, \"fc%d\" % (block))\n",
        "                    block += 1\n",
        "                    self_layer.weight.data[...] = torch.tensor(layer.weight).view_as(self_layer.weight)[...]\n",
        "                    self_layer.bias.data[...] = torch.tensor(layer.bias).view_as(self_layer.bias)[...]\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" Pytorch forward\n",
        "        Args:\n",
        "            x: input image (224x224)\n",
        "        Returns: class logits\n",
        "        \"\"\"\n",
        "        x = F.relu(self.conv_1_1(x))\n",
        "        x = F.relu(self.conv_1_2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv_2_1(x))\n",
        "        x = F.relu(self.conv_2_2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv_3_1(x))\n",
        "        x = F.relu(self.conv_3_2(x))\n",
        "        x = F.relu(self.conv_3_3(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv_4_1(x))\n",
        "        x = F.relu(self.conv_4_2(x))\n",
        "        x = F.relu(self.conv_4_3(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv_5_1(x))\n",
        "        x = F.relu(self.conv_5_2(x))\n",
        "        x = F.relu(self.conv_5_3(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc6(x))\n",
        "        x = F.dropout(x, 0.5, self.training)\n",
        "        #return self.fc7(x) #new added\n",
        "        #return F.relu(self.fc7(x))\n",
        "\n",
        "        x = F.relu(self.fc7(x))\n",
        "        x = F.dropout(x, 0.5, self.training)\n",
        "\n",
        "        x = F.normalize(x, p=2, dim=1)\n",
        "        return x\n",
        "\n",
        "        #return self.fc8(x) \n",
        "\n",
        "\n",
        "    def forward_two(self, x):\n",
        "        \"\"\" Pytorch forward\n",
        "        Args:\n",
        "            x: input image (224x224)\n",
        "        Returns: class logits\n",
        "        \"\"\"\n",
        "        x = F.relu(self.conv_1_1(x))\n",
        "        x = F.relu(self.conv_1_2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv_2_1(x))\n",
        "        x = F.relu(self.conv_2_2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv_3_1(x))\n",
        "        x = F.relu(self.conv_3_2(x))\n",
        "        x = F.relu(self.conv_3_3(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv_4_1(x))\n",
        "        x = F.relu(self.conv_4_2(x))\n",
        "        x = F.relu(self.conv_4_3(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv_5_1(x))\n",
        "        x = F.relu(self.conv_5_2(x))\n",
        "        x = F.relu(self.conv_5_3(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc6(x))\n",
        "        x = F.dropout(x, 0.5, self.training)\n",
        "        x = F.relu(self.fc7(x))\n",
        "        x = F.dropout(x, 0.5, self.training)\n",
        "        return self.fc8(x)"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchfile in /usr/local/lib/python3.6/dist-packages (0.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wc8SxYnBMVoT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from os.path import join\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "from time import time\n",
        "import face_recognition\n",
        "from random import randint\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "class Decoder_Dataset(Dataset):\n",
        "     \n",
        "\n",
        "    def __init__(self, Folder, VggL, sample, dev = \"cuda\"):\n",
        "\n",
        "        self.root     = Folder\n",
        "        self.dev      = dev \n",
        "        self.names    = os.listdir(self.root) \n",
        "        self.sample   = sample if sample < 20 else 20\n",
        "        self.a_length = len(os.listdir(self.root)) \n",
        "        self.length   = self.a_length * self.sample\n",
        "        self.img_name = self.root + \"{}.jpg\"\n",
        "        self.vgg_features = VggL\n",
        "        self.vgg = VGG_16()\n",
        "        self.vgg.load_weights()\n",
        "        self.vgg.eval()\n",
        "        print(self.a_length)\n",
        "\n",
        "    def image_out(self,path, mean =[0.485, 0.456, 0.406] , std = [0.229, 0.224, 0.225]):#[0.485, 0.456, 0.406]\n",
        "        img = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n",
        "        image = Image.open(path)\n",
        "        count = 0\n",
        "        resultant = []\n",
        "        try:\n",
        "            faceLocation = face_recognition.face_locations(img)[count]\n",
        "            x,y1,x1,y = faceLocation\n",
        "            img = img[x:x1,y:y1]\n",
        "            landmark = face_recognition.face_landmarks(img)\n",
        "            for i in list(landmark[0].keys()):\n",
        "                resultant += landmark[0][i]\n",
        "            landmark = np.ravel(np.array(resultant))\n",
        "            img = cv2.resize(img, (224,224),  interpolation = cv2.INTER_AREA)\n",
        "\n",
        "            loader = transforms.Compose([\n",
        "                 transforms.Resize((224,224), interpolation = cv2.INTER_AREA),\n",
        "                 transforms.ToTensor(),\n",
        "                 transforms.Normalize(mean, std),\n",
        "             ])\n",
        "            \n",
        "            img_t = loader(image)\n",
        "            \n",
        "            img_t = torch.reshape(img_t, (-1, 3, 224, 224))\n",
        "            biden_encoding = self.vgg(img_t)\n",
        "            # cv2_imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "            \n",
        "        except IndexError:\n",
        "            return None, None, None\n",
        "        \n",
        "        \n",
        "        return  biden_encoding.detach().numpy(), img.reshape(1,224,224,3), np.ravel(np.array(landmark))\n",
        "\n",
        "\n",
        "\n",
        "    def pseudo_idx(self,idx):\n",
        "        if idx < self.a_length:\n",
        "            return idx\n",
        "        else:\n",
        "            ## return self.pseudo_idx(idx - self.a_length) ## RECURSION\n",
        "            return idx // self.sample \n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        while True:\n",
        "            index = idx\n",
        "            index = self.pseudo_idx(index)\n",
        "            name  = index \n",
        "            path =  self.img_name.format(name)\n",
        "            # print(path)\n",
        "\n",
        "            if not os.path.exists(path):\n",
        "                idx = randint(0, self.a_length) # IF FILE PATH DOESNOT EXISTS\n",
        "                continue\n",
        "\n",
        "            feature, image, landmark= self.image_out(path)\n",
        "            if image is None:\n",
        "                idx = randint(0, self.a_length) # IF FILE PATH DOESNOT EXISTS\n",
        "                continue\n",
        "\n",
        "            image = torch.from_numpy(image).view(3,224,224).float()\n",
        "            return   torch.from_numpy(feature).float(), image,torch.from_numpy(landmark).float()"
      ],
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYgo0KW3MYoN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############### THIS CELL IS COPIED FROM DEEP SPEECH MOZILLA #########################\n",
        "\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tfv1\n",
        "from tensorflow.compat import dimension_value\n",
        "from tensorflow.contrib.image import dense_image_warp\n",
        "from tensorflow.contrib.image import interpolate_spline\n",
        "\n",
        "from tensorflow.python.framework import ops\n",
        "from tensorflow.python.ops import array_ops\n",
        "\n",
        "def _to_float32(value):\n",
        "    return tf.cast(value, tf.float32)\n",
        "\n",
        "def _to_int32(value):\n",
        "    return tf.cast(value, tf.int32)\n",
        "\n",
        "def _get_grid_locations(image_height, image_width):\n",
        "    \"\"\"Wrapper for np.meshgrid.\"\"\"\n",
        "    tfv1.assert_type(image_height, tf.int32)\n",
        "    tfv1.assert_type(image_width, tf.int32)\n",
        "\n",
        "    y_range = tf.range(image_height)\n",
        "    x_range = tf.range(image_width)\n",
        "    y_grid, x_grid = tf.meshgrid(y_range, x_range, indexing='ij')\n",
        "    return tf.stack((y_grid, x_grid), -1)\n",
        "\n",
        "\n",
        "def _expand_to_minibatch(tensor, batch_size):\n",
        "    \"\"\"Tile arbitrarily-sized np_array to include new batch dimension.\"\"\"\n",
        "    ndim = tf.size(tf.shape(tensor))\n",
        "    ones = tf.ones((ndim,), tf.int32)\n",
        "\n",
        "    tiles = tf.concat(([batch_size], ones), 0)\n",
        "    return tf.tile(tf.expand_dims(tensor, 0), tiles)\n",
        "\n",
        "\n",
        "def _get_boundary_locations(image_height, image_width, num_points_per_edge):\n",
        "    \"\"\"Compute evenly-spaced indices along edge of image.\"\"\"\n",
        "    image_height_end = _to_float32(tf.math.subtract(image_height, 1))\n",
        "    image_width_end = _to_float32(tf.math.subtract(image_width, 1))\n",
        "    y_range = tf.linspace(0.0, image_height_end, num_points_per_edge + 2)\n",
        "    x_range = tf.linspace(0.0, image_height_end, num_points_per_edge + 2)\n",
        "    ys, xs = tf.meshgrid(y_range, x_range, indexing='ij')\n",
        "    is_boundary = tf.logical_or(\n",
        "        tf.logical_or(tf.equal(xs, 0.0), tf.equal(xs, image_width_end)),\n",
        "        tf.logical_or(tf.equal(ys, 0.0), tf.equal(ys, image_height_end)))\n",
        "    return tf.stack([tf.boolean_mask(ys, is_boundary), tf.boolean_mask(xs, is_boundary)], axis=-1)\n",
        "\n",
        "\n",
        "def _add_zero_flow_controls_at_boundary(control_point_locations,\n",
        "                                        control_point_flows, image_height,\n",
        "                                        image_width, boundary_points_per_edge):\n",
        "    \"\"\"Add control points for zero-flow boundary conditions.\n",
        "     Augment the set of control points with extra points on the\n",
        "     boundary of the image that have zero flow.\n",
        "    Args:\n",
        "      control_point_locations: input control points\n",
        "      control_point_flows: their flows\n",
        "      image_height: image height\n",
        "      image_width: image width\n",
        "      boundary_points_per_edge: number of points to add in the middle of each\n",
        "                             edge (not including the corners).\n",
        "                             The total number of points added is\n",
        "                             4 + 4*(boundary_points_per_edge).\n",
        "    Returns:\n",
        "      merged_control_point_locations: augmented set of control point locations\n",
        "      merged_control_point_flows: augmented set of control point flows\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size = dimension_value(tf.shape(control_point_locations)[0])\n",
        "\n",
        "    boundary_point_locations = _get_boundary_locations(image_height, image_width,\n",
        "                                                       boundary_points_per_edge)\n",
        "    boundary_point_shape = tf.shape(boundary_point_locations)\n",
        "    boundary_point_flows = tf.zeros([boundary_point_shape[0], 2])\n",
        "\n",
        "    minbatch_locations = _expand_to_minibatch(boundary_point_locations, batch_size)\n",
        "    type_to_use = control_point_locations.dtype\n",
        "    boundary_point_locations = tf.cast(minbatch_locations, type_to_use)\n",
        "\n",
        "    minbatch_flows = _expand_to_minibatch(boundary_point_flows, batch_size)\n",
        "\n",
        "    boundary_point_flows = tf.cast(minbatch_flows, type_to_use)\n",
        "\n",
        "    merged_control_point_locations = tf.concat(\n",
        "        [control_point_locations, boundary_point_locations], 1)\n",
        "\n",
        "    merged_control_point_flows = tf.concat(\n",
        "        [control_point_flows, boundary_point_flows], 1)\n",
        "\n",
        "    return merged_control_point_locations, merged_control_point_flows\n",
        "\n",
        "\n",
        "def sparse_image_warp(image,\n",
        "                      source_control_point_locations,\n",
        "                      dest_control_point_locations,\n",
        "                      interpolation_order=2,\n",
        "                      regularization_weight=0.0,\n",
        "                      num_boundary_points=0,\n",
        "                      name='sparse_image_warp'):\n",
        "    \"\"\"Image warping using correspondences between sparse control points.\n",
        "    Apply a non-linear warp to the image, where the warp is specified by\n",
        "    the source and destination locations of a (potentially small) number of\n",
        "    control points. First, we use a polyharmonic spline\n",
        "    (`tf.contrib.image.interpolate_spline`) to interpolate the displacements\n",
        "    between the corresponding control points to a dense flow field.\n",
        "    Then, we warp the image using this dense flow field\n",
        "    (`tf.contrib.image.dense_image_warp`).\n",
        "    Let t index our control points. For regularization_weight=0, we have:\n",
        "    warped_image[b, dest_control_point_locations[b, t, 0],\n",
        "                    dest_control_point_locations[b, t, 1], :] =\n",
        "    image[b, source_control_point_locations[b, t, 0],\n",
        "             source_control_point_locations[b, t, 1], :].\n",
        "    For regularization_weight > 0, this condition is met approximately, since\n",
        "    regularized interpolation trades off smoothness of the interpolant vs.\n",
        "    reconstruction of the interpolant at the control points.\n",
        "    See `tf.contrib.image.interpolate_spline` for further documentation of the\n",
        "    interpolation_order and regularization_weight arguments.\n",
        "    Args:\n",
        "      image: `[batch, height, width, channels]` float `Tensor`\n",
        "      source_control_point_locations: `[batch, num_control_points, 2]` float\n",
        "        `Tensor`\n",
        "      dest_control_point_locations: `[batch, num_control_points, 2]` float\n",
        "        `Tensor`\n",
        "      interpolation_order: polynomial order used by the spline interpolation\n",
        "      regularization_weight: weight on smoothness regularizer in interpolation\n",
        "      num_boundary_points: How many zero-flow boundary points to include at\n",
        "        each image edge.Usage:\n",
        "          num_boundary_points=0: don't add zero-flow points\n",
        "          num_boundary_points=1: 4 corners of the image\n",
        "          num_boundary_points=2: 4 corners and one in the middle of each edge\n",
        "            (8 points total)\n",
        "          num_boundary_points=n: 4 corners and n-1 along each edge\n",
        "      name: A name for the operation (optional).\n",
        "      Note that image and offsets can be of type tf.half, tf.float32, or\n",
        "      tf.float64, and do not necessarily have to be the same type.\n",
        "    Returns:\n",
        "      warped_image: `[batch, height, width, channels]` float `Tensor` with same\n",
        "        type as input image.\n",
        "      flow_field: `[batch, height, width, 2]` float `Tensor` containing the dense\n",
        "        flow field produced by the interpolation.\n",
        "    \"\"\"\n",
        "\n",
        "    image = ops.convert_to_tensor(image)\n",
        "    source_control_point_locations = ops.convert_to_tensor(\n",
        "        source_control_point_locations)\n",
        "    dest_control_point_locations = ops.convert_to_tensor(\n",
        "        dest_control_point_locations)\n",
        "\n",
        "    control_point_flows = (\n",
        "        dest_control_point_locations - source_control_point_locations)\n",
        "\n",
        "    clamp_boundaries = num_boundary_points > 0\n",
        "    boundary_points_per_edge = num_boundary_points - 1\n",
        "\n",
        "    with ops.name_scope(name):\n",
        "        image_shape = tf.shape(image)\n",
        "        batch_size, image_height, image_width = image_shape[0], image_shape[1], image_shape[2]\n",
        "\n",
        "        # This generates the dense locations where the interpolant\n",
        "        # will be evaluated.\n",
        "        grid_locations = _get_grid_locations(image_height, image_width)\n",
        "\n",
        "        flattened_grid_locations = tf.reshape(grid_locations,\n",
        "                                              [tf.multiply(image_height, image_width), 2])\n",
        "\n",
        "        # flattened_grid_locations = constant_op.constant(\n",
        "        #     _expand_to_minibatch(flattened_grid_locations, batch_size), image.dtype)\n",
        "        flattened_grid_locations = _expand_to_minibatch(flattened_grid_locations, batch_size)\n",
        "        flattened_grid_locations = tf.cast(flattened_grid_locations, dtype=image.dtype)\n",
        "\n",
        "        if clamp_boundaries:\n",
        "            (dest_control_point_locations,\n",
        "             control_point_flows) = _add_zero_flow_controls_at_boundary(\n",
        "                 dest_control_point_locations, control_point_flows, image_height,\n",
        "                 image_width, boundary_points_per_edge)\n",
        "\n",
        "        flattened_flows = interpolate_spline(\n",
        "            dest_control_point_locations, control_point_flows,\n",
        "            flattened_grid_locations, interpolation_order, regularization_weight)\n",
        "\n",
        "        dense_flows = array_ops.reshape(flattened_flows,\n",
        "                                        [batch_size, image_height, image_width, 2])\n",
        "\n",
        "        warped_image = dense_image_warp(image, dense_flows)\n",
        "\n",
        "        return warped_image, dense_flows"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EQvfmnDMh1v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def image_warping(src_img, src_landmarks, dest_landmarks):\n",
        "    # expanded_src_landmarks = np.expand_dims(np.float32(src_landmarks), axis=0)\n",
        "    # expanded_dest_landmarks = np.expand_dims(np.float32(dest_landmarks), axis=0)\n",
        "    # expanded_src_img = np.expand_dims(np.float32(src_img) / 255, axis=0)\n",
        "\n",
        "    warped_img, dense_flows = sparse_image_warp(src_img,\n",
        "                          src_landmarks,\n",
        "                          dest_landmarks,\n",
        "                          interpolation_order=1,\n",
        "                          regularization_weight=0.1,\n",
        "                          num_boundary_points=2,\n",
        "                          name='sparse_image_warp')\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        out_img = sess.run(warped_img)\n",
        "        warp_img = np.uint8(out_img[:, :, :, :] * 255)\n",
        "    \n",
        "    return warp_img\n",
        "    \n",
        "def face_landmark(img):\n",
        "    X = np.zeros((img.shape[0], 72 ,2))\n",
        "    flag = []\n",
        "    for i in range(img.shape[0]):\n",
        "\n",
        "        landmark = face_recognition.face_landmarks(img[i].reshape(224,224,3))\n",
        "        resultant = []\n",
        "        try:\n",
        "            for j in list(landmark[0].keys()):\n",
        "                resultant += landmark[0][j] \n",
        "        except IndexError:\n",
        "            flag.append(i)\n",
        "            continue\n",
        "        X[i] = np.array(resultant)\n",
        "    return  X, flag"
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrbRKR8qMqSo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "from torch import nn\n",
        "\n",
        "class DECODER(nn.Module):\n",
        "    def __init__(self, phase):\n",
        "        super(DECODER, self).__init__()\n",
        "        self.phase = phase\n",
        "        self.fc3 = nn.Linear(4096, 1000)\n",
        "        self.ReLU = nn.ReLU()\n",
        "        #self.fc_bn3 = nn.BatchNorm1d(1000)\n",
        "\n",
        "\n",
        "        self.fc4 = nn.Linear(1000, 14 * 14 * 64)\n",
        "        self.fc_bn4 = nn.BatchNorm1d(14 * 14 * 64)\n",
        "        def TransConv( i, kernal = 5, stride = 2, inp = None):\n",
        "            if not inp:\n",
        "                inp = max(256//2**(i-1), 32)\n",
        "\n",
        "            layer =  nn.Sequential(\n",
        "                nn.ConvTranspose2d(inp, max(256//2**i, 32), \n",
        "                                kernal, stride=stride, padding=2, output_padding=1, \n",
        "                                dilation=1, padding_mode='zeros'),\n",
        "                nn.ReLU(),\n",
        "                nn.BatchNorm2d(max(256//2**i, 32)))\n",
        "            return layer\n",
        "        self.T1_ = TransConv(1, inp = 64)\n",
        "        self.T2_ = TransConv(2)\n",
        "        self.T3_ = TransConv(3)\n",
        "        self.T4_ = TransConv(4)\n",
        "    \n",
        "        self.ConvLast = nn.Sequential(\n",
        "            nn.Conv2d(32, 3, (1,1), stride=1),\n",
        "            nn.BatchNorm2d(3),\n",
        "            nn.ReLU())\n",
        "\n",
        "\n",
        "        self.layerLandmark1 = nn.Linear(1000, 800)\n",
        "        self.layerLandmark2 = nn.Linear(800, 600)\n",
        "        self.layerLandmark3 = nn.Linear(600, 400)\n",
        "        self.layerLandmark4 = nn.Linear(400, 200)\n",
        "        self.layerLandmark5 = nn.Linear(200, 144)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        L1 = self.fc3(x)\n",
        "        L1 = self.ReLU(L1)\n",
        "\n",
        "\n",
        "        L2 = self.layerLandmark1(L1)\n",
        "        L2 = self.ReLU(L2)\n",
        "\n",
        "        L3 = self.layerLandmark2(L2)\n",
        "        L3 = self.ReLU(L3)\n",
        "\n",
        "        L4 = self.layerLandmark3(L3)\n",
        "        L4 = self.ReLU(L4)\n",
        "\n",
        "        L5 = self.layerLandmark4(L4)\n",
        "        L5 = self.ReLU(L5)\n",
        "\n",
        "        L6 = self.layerLandmark5(L5)\n",
        "        outL = self.ReLU(L6)\n",
        "\n",
        "\n",
        "        # B1 = self.fc_bn3(L1) \n",
        "        T0 = self.fc4(L1) \n",
        "        T0 = self.ReLU(T0)\n",
        "        # T0 = self.fc_bn4(T0)\n",
        "        T0 = T0.view(-1,64,14,14)\n",
        "\n",
        "\n",
        "\n",
        "        T1 = self.T1_(T0)\n",
        "        T2 = self.T2_(T1)\n",
        "        T3 = self.T3_(T2)\n",
        "        T4 = self.T4_(T3)\n",
        "\n",
        "        outT = self.ConvLast(T4)\n",
        "        if self.phase == \"train\":\n",
        "            return outL,  outT \n",
        "        elif self.phase == \"eval\":\n",
        "            img = outT.cpu().detach().numpy().reshape(-1, 224, 224, 3)*255\n",
        "            outL = outL.cpu().detach().numpy()\n",
        "            outL = np.dstack((outL[:,0::2],outL[:,1::2]))\n",
        "            #print(\"land np img np \", outL_.shape, img_.shape)\n",
        "            img = (img.reshape(-1,224,224,3)*255).astype(np.uint8)\n",
        "            #print(\"img_t \",img_t.shape, img_t[0])\n",
        "            src, flag = face_landmark(img)\n",
        "            if flag:\n",
        "                for r in flag:\n",
        "                    src[r] = outL[r]\n",
        "\n",
        "            return image_warping(img.astype(np.float32), src.astype(np.float32), outL.astype(np.float32))\n",
        "\n",
        "            \n",
        "        # outN = outT.numpy()\n",
        "        # outLN = outL.numpy()\n",
        "        # src, flag = face_landmark(outN)\n",
        "        # if flag:\n",
        "        #     for r in flag:\n",
        "        #         src[r] = outLN[r]\n",
        "        \n",
        "        # IMG = torch.from_numpy(image_warping(outN, src, outLN))\n",
        "        \n",
        "        # if self.phase == \"train\":\n",
        "        #     return outL,  outT , VGGL(IMG)\n",
        "        # if self.phase == \"test\":\n",
        "        #     return IMG"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQLzfoqsNdJ6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 761
        },
        "outputId": "1bcea97e-4296-4529-8be1-074b191d6cf6"
      },
      "source": [
        "\n",
        "from torchsummary import summary\n",
        "device = \"cuda\" #torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = DECODER(\"train\")\n",
        "model = model.cuda()\n",
        "summary(model, input_size=(4096,))"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                 [-1, 1000]       4,097,000\n",
            "              ReLU-2                 [-1, 1000]               0\n",
            "            Linear-3                  [-1, 800]         800,800\n",
            "              ReLU-4                  [-1, 800]               0\n",
            "            Linear-5                  [-1, 600]         480,600\n",
            "              ReLU-6                  [-1, 600]               0\n",
            "            Linear-7                  [-1, 400]         240,400\n",
            "              ReLU-8                  [-1, 400]               0\n",
            "            Linear-9                  [-1, 200]          80,200\n",
            "             ReLU-10                  [-1, 200]               0\n",
            "           Linear-11                  [-1, 144]          28,944\n",
            "             ReLU-12                  [-1, 144]               0\n",
            "           Linear-13                [-1, 12544]      12,556,544\n",
            "             ReLU-14                [-1, 12544]               0\n",
            "  ConvTranspose2d-15          [-1, 128, 28, 28]         204,928\n",
            "             ReLU-16          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-17          [-1, 128, 28, 28]             256\n",
            "  ConvTranspose2d-18           [-1, 64, 56, 56]         204,864\n",
            "             ReLU-19           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-20           [-1, 64, 56, 56]             128\n",
            "  ConvTranspose2d-21         [-1, 32, 112, 112]          51,232\n",
            "             ReLU-22         [-1, 32, 112, 112]               0\n",
            "      BatchNorm2d-23         [-1, 32, 112, 112]              64\n",
            "  ConvTranspose2d-24         [-1, 32, 224, 224]          25,632\n",
            "             ReLU-25         [-1, 32, 224, 224]               0\n",
            "      BatchNorm2d-26         [-1, 32, 224, 224]              64\n",
            "           Conv2d-27          [-1, 3, 224, 224]              99\n",
            "      BatchNorm2d-28          [-1, 3, 224, 224]               6\n",
            "             ReLU-29          [-1, 3, 224, 224]               0\n",
            "================================================================\n",
            "Total params: 18,771,761\n",
            "Trainable params: 18,771,761\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.02\n",
            "Forward/backward pass size (MB): 56.51\n",
            "Params size (MB): 71.61\n",
            "Estimated Total Size (MB): 128.14\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeIkEyJ8NkgU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c33ed165-1412-4bce-9489-abe250fae75b"
      },
      "source": [
        "import os\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.009)\n",
        "criterion_1 = nn.MSELoss()\n",
        "criterion_2 = nn.L1Loss()\n",
        "criterion_3 = nn.CosineEmbeddingLoss()\n",
        "alpha = 0.0002\n",
        "beta  = 1.0\n",
        "#gamma = 1.5\n",
        "l = 10\n",
        "BATCH = 20\n",
        "WORKER = 0\n",
        "SAMPLE = 20\n",
        "num_epochs = 5\n",
        "sim = torch.ones((BATCH,1, 128 ))\n",
        "train_dataset = Decoder_Dataset(\"/content/drive/My Drive/Speech2Face/data_test/faces/\", sample = SAMPLE, VggL = \"vgg16\")\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "\t\t                                           batch_size=BATCH, \n",
        "\t\t                                           num_workers =WORKER,\n",
        "\t\t                                           shuffle=False)"
      ],
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3630\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPstvvpLN1He",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#! mkdir checkpoint"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YymSd2y4TBUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nvidia_smi\n",
        "import psutil\n",
        "import platform\n",
        "\n",
        "nvidia_smi.nvmlInit()\n",
        "handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Blqj7ckOBDZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "9f8b3fe9-62fb-4171-bba5-cd4fcb352fd3"
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(\"Epoch\", epoch)\n",
        "    running_loss = 0.0\n",
        "    running_loss_1 = 0.0\n",
        "    running_loss_2 = 0.0\n",
        "    start_time = time()\n",
        "    for  i,(feature, img, landm) in enumerate(train_loader):\n",
        "        \n",
        "        # Move tensors to the configured device\n",
        "        img = img.to(device)\n",
        "        landm = landm.to(device)\n",
        "        #print(\"input\",img.size(), landm.size())\n",
        "        feature = feature.to(device)\n",
        "        #print(\"vgg\",feature.size())\n",
        "        # Forward pass\n",
        "        outL, outT = model(feature)\n",
        "        #print(\"output \", outL.size(), outT.size())\n",
        "\n",
        "        loss_1 = criterion_1(outL, landm)\n",
        "        loss_2 = criterion_2(outT, img)\n",
        "        running_loss_1 += loss_1.item()\n",
        "        running_loss_2 += loss_2.item()\n",
        "\n",
        "        # img_ = outT.cpu().detach().numpy()\n",
        "        # outL_ = outL.cpu().detach().numpy()\n",
        "        # outL_ = np.dstack((outL_[:,0::2],outL_[:,1::2]))\n",
        "        # #print(\"land np img np \", outL_.shape, img_.shape)\n",
        "        # img_t = (img_.reshape(-1,224,224,3)*255).astype(np.uint8)\n",
        "        # #print(\"img_t \",img_t.shape, img_t[0])\n",
        "        # if epoch in [0, 1]:\n",
        "        #     src, flag    = face_landmark(img_t)\n",
        "        #     if flag:\n",
        "        #         for f in flag:\n",
        "        #             src[f] = outL_[f]\n",
        "        # else:\n",
        "\n",
        "        #     outL_ = landm.cpu().detach().numpy()\n",
        "\n",
        "        # img_ = image_warping(img_.astype(np.float32), src.astype(np.float32), outL_.astype(np.float32)).to(device)\n",
        "\n",
        "        # # print(img_.size(), type(img_))\n",
        "        # # print(feature.size())\n",
        "        # feature_out = vgg16(img_)\n",
        "        # # print(feature_out.size(), sim.size())\n",
        "\n",
        "        # loss_3 = criterion_3(feature_out.view(BATCH,1,4096).to(device), feature.view(BATCH,1,4096), sim.view(BATCH,1,4096).to(device))\n",
        "        # running_loss_3 += loss_3.item()\n",
        "\n",
        "\n",
        "\n",
        "        loss = (alpha*loss_1 + beta*loss_2 )*100\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i % l == 0:\n",
        "            # writer.add_scalar('training loss',running_loss/  data[\"show\"],epoch * len(train_loader) + i)\n",
        "            res = nvidia_smi.nvmlDeviceGetUtilizationRates(handle)\n",
        "            print('{} : Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, gpu: {}%, gpu-mem: {}%, RAM: {}% , RAM Mem {}%, running_loss: {:.4f}, MSEloss {:.4f}, MAEloss {:.4f},  time: {:.4f},  '.format(datetime.now() ,epoch+1, num_epochs, i,int((train_dataset.__len__()/BATCH)),  loss.item(),res.gpu, res.memory,psutil.cpu_percent(),psutil.virtual_memory()[2],running_loss/(BATCH*l),running_loss_1/(BATCH*l), running_loss_2/(BATCH*l), (time()- start_time)))\n",
        "            #print(f'gpu: {res.gpu}%, gpu-mem: {res.memory}%, RAM: {psutil.cpu_percent()}% , RAM Mem {psutil.virtual_memory()[2]}%')\n",
        "            running_loss = 0.0\n",
        "            start_time = time()\n",
        "            running_loss_1 = 0.0\n",
        "            running_loss_2 = 0.0\n",
        "            result = outT.cpu().detach().numpy()[0].squeeze().astype(np.uint8)*255\n",
        "\n",
        "            cv2_imshow(np.reshape(result, (224,224, 3)))\n",
        "    \n",
        "    torch.save({'epoch': epoch,'model_state_dict': model.state_dict(),'optimizer_state_dict': optimizer.state_dict(), 'loss': loss}, \"/content/drive/My Drive/Speech2Face/check_poit_decoder/epoch_{}.pth\".format(epoch + 1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "2020-08-13 11:10:00.377091 : Epoch [1/5], Step [0/3630], Loss: 9733.4492, gpu: 0%, gpu-mem: 0%, RAM: 23.5% , RAM Mem 31.7%, running_loss: 48.6672, MSEloss 22.5599, MAEloss 0.4822,  time: 10.5049,  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([20, 144])) that is different to the input size (torch.Size([20, 1, 144])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=224x224 at 0x7F4A9C168358>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-13 11:11:48.617881 : Epoch [1/5], Step [10/3630], Loss: 11828.2773, gpu: 65%, gpu-mem: 29%, RAM: 49.5% , RAM Mem 31.7%, running_loss: 533.7236, MSEloss 493.0832, MAEloss 5.2386,  time: 108.2408,  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=224x224 at 0x7F4A9C168470>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jg7ZJ4Leeau0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = DECODER('eval')\n",
        "state = torch.load('/content/drive/My Drive/Speech2Face/check_poit_decoder/epoch_15.pth')\n",
        "model.load_state_dict(state['model_state_dict'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEGpXCDXecvg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"/content/drive/My Drive/Speech2Face/data_test/test/n000078/0002_02.jpg\"\n",
        "img = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n",
        "faceLoc = face_recognition.face_locations(img)\n",
        "\n",
        "x,y1,x1,y = faceLoc[0]\n",
        "img = img[x:x1,y:y1]\n",
        "Image.fromarray(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC9CvkPJe77Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img = cv2.resize(img, (224,224), interpolation = cv2.INTER_AREA)\n",
        "biden_encoding = face_recognition.face_encodings(img)[0]\n",
        "encoded = torch.reshape(torch.from_numpy(biden_encoding).float(), (-1, 128))\n",
        "\n",
        "outT = model(encoded) \n",
        "out = torch.tensor(outT)\n",
        "out = np.squeeze(out)\n",
        "out = out.reshape(224,224,3)\n",
        "\n",
        "Image.fromarray(out.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}