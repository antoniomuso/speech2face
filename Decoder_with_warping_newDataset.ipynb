{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antoniomuso/speech2face/blob/master/Decoder_with_warping_newDataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYdMtmJooq6M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ccd0d9f9-9815-49e3-e5b6-44b26a4215a8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWWYuEBuoTYg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! rm -r Face_Feature\n",
        "\n",
        "# ! ls -l Face_Feature/Faces | wc -l\n",
        "! pip install face_recognition\n",
        "! pip3 install tensorflow-gpu==1.15"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZVImpbC5Pmg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "bc8ed3af-1d4c-4bdb-a9ab-796ab1518837"
      },
      "source": [
        "! cp /content/drive/My\\ Drive/Speech2Face/face_features_10_per_actor.zip .\n",
        "! unzip face_features_10_per_actor.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  face_features_10_per_actor.zip\n",
            "replace content/Face_Feature/Faces/John_Turturro_11427.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRiw-0J86VA8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4XC5nS4thFS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !wget http://www.robots.ox.ac.uk/~vgg/research/CMBiometrics/data/zippedFaces.tar.gz \n",
        "# tar zxvf zippedFaces.tar.gz -C \"/content/drive/My Drive/Speech2Face/vox1_dataset/\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eFsvXPivKgI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !curl --user voxceleb1912:0s42xuw6 -o \"/content/drive/My Drive/Speech2Face/ff/vox.zip\" http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_test_wav.zip"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VepCRxxbJlAW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import  torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from face_recognition import face_landmarks, face_locations, face_encodings\n",
        "from face_recognition import load_image_file\n",
        "from random import randint\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "\n",
        "import os\n",
        "from os.path import join\n",
        "from os import listdir\n",
        "\n",
        "from tqdm import tqdm\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rQZIr9MJlR4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reproducibility stuff\n",
        "\n",
        "import random\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(0)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6SgTIZoqwYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This dataset use .npy embedding VGG\n",
        "def load_metadata(data_file_path):\n",
        "    data_list = []\n",
        "    with open(data_file_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            data_list.append(line.rstrip(\"\\n\").split(\",\"))\n",
        "    return data_list\n",
        "\n",
        "\n",
        "class _Normalize_Tensor(object):\n",
        "    def __init__(self, color_mean, color_std):\n",
        "        self.color_mean = color_mean\n",
        "        self.color_std = color_std\n",
        "\n",
        "    def __call__(self, img):\n",
        "        # Convert image to Tensor\n",
        "        img = transforms.functional.to_tensor(img)\n",
        "\n",
        "        # Normalize image by the parameter of pre-trained face-encoder\n",
        "        img = transforms.functional.normalize(\n",
        "            img, self.color_mean, self.color_std)\n",
        "\n",
        "        return img\n",
        "\n",
        "color_mean, color_std = [129.186279296875, 104.76238250732422, 93.59396362304688], [1,1,1]\n",
        "color_mean = [tmp / 255.0 for tmp in color_mean]\n",
        "color_std = [tmp / 255.0 for tmp in color_std]\n",
        "\n",
        "class EmbedImagePairs(Dataset):\n",
        "    def __init__(self, data_list_path, size=64):\n",
        "        super().__init__()\n",
        "        self.face_features = np.load(join(data_list_path,'facefeature.npy'))\n",
        "        self.image_path = join(data_list_path, 'Faces')\n",
        "        self.images =  listdir(join(data_list_path, 'Faces'))\n",
        "        \n",
        "        self.size = size\n",
        "\n",
        "\n",
        "        self.transform_fd = transforms.Compose([\n",
        "            #transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "        self.transform_fe = transforms.Compose([\n",
        "            _Normalize_Tensor(color_mean, color_std)\n",
        "        ])        \n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # Load face-image\n",
        "        image_p = self.images[idx]\n",
        "        features_index = int(image_p.split('_')[-1].split('.')[0])\n",
        "\n",
        "        image = Image.open(join(self.image_path,image_p))\n",
        "        image = image.resize((224,224))\n",
        "\n",
        "        #display(image)\n",
        "\n",
        "        #tr = transforms.ToTensor()(image)\n",
        "        #print(\"TR\", tr.shape, tr.dtype)\n",
        "\n",
        "        #np_image = self.transform_fd(image).numpy()\n",
        "        #print(\"1\",np_image.dtype, np_image.shape)\n",
        "        \n",
        "        # Localize Face\n",
        "        np_image = np.array(image)\n",
        "        #print(\"np\",np_image.dtype, np_image.shape)\n",
        "\n",
        "        #ts = torch.from_numpy(np_image)\n",
        "        #print(\"TS\", ts.dtype, ts.shape)\n",
        "\n",
        "        norm = self.transform_fe(np_image)\n",
        "        \n",
        "        #import matplotlib.pyplot as plt\n",
        "        #plt.imshow(norm.permute(1, 2, 0))\n",
        "\n",
        "        #print(\"norm\", norm.dtype, norm.shape)\n",
        "\n",
        "        #tf = self.transform_fd(image)\n",
        "        #print(\"TF\", tf.dtype, tf.shape)\n",
        "\n",
        "        # print(np_image.shape)\n",
        "        try:\n",
        "\n",
        "          resultant = []\n",
        "          #np_image = np_image[x:x1,y:y1]\n",
        "          landmark = face_landmarks(np_image)\n",
        "          for i in list(landmark[0].keys()):\n",
        "            resultant += landmark[0][i]\n",
        "          landmark = np.ravel(np.array(resultant))\n",
        "\n",
        "        except IndexError:\n",
        "          return self[randint(0, len(self))]\n",
        "\n",
        "        face_vectors = torch.tensor(self.face_features[features_index])\n",
        "\n",
        "        return face_vectors.float(), torch.tensor(landmark).float(), norm\n",
        "\n",
        "#input_path = '/content/content/Face_Feature/'\n",
        "#print('Image Pairing')\n",
        "#dataset = EmbedImagePairs(input_path)\n",
        "#print(dataset[0])"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Io7T0B-dq8eG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Vgg_face_dag(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Vgg_face_dag, self).__init__()\n",
        "        self.meta = {'mean': [129.186279296875, 104.76238250732422, 93.59396362304688],\n",
        "                     'std': [1, 1, 1],\n",
        "                     'imageSize': [224, 224, 3]}\n",
        "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu1_1 = nn.ReLU(inplace=True)\n",
        "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu1_2 = nn.ReLU(inplace=True)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
        "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu2_1 = nn.ReLU(inplace=True)\n",
        "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu2_2 = nn.ReLU(inplace=True)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
        "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu3_1 = nn.ReLU(inplace=True)\n",
        "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu3_2 = nn.ReLU(inplace=True)\n",
        "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu3_3 = nn.ReLU(inplace=True)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
        "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu4_1 = nn.ReLU(inplace=True)\n",
        "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu4_2 = nn.ReLU(inplace=True)\n",
        "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu4_3 = nn.ReLU(inplace=True)\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
        "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu5_1 = nn.ReLU(inplace=True)\n",
        "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu5_2 = nn.ReLU(inplace=True)\n",
        "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu5_3 = nn.ReLU(inplace=True)\n",
        "        self.pool5 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
        "        self.fc6 = nn.Linear(in_features=25088, out_features=4096, bias=True)\n",
        "        self.relu6 = nn.ReLU(inplace=True)\n",
        "        self.dropout6 = nn.Dropout(p=0.5)\n",
        "        self.fc7 = nn.Linear(in_features=4096, out_features=4096, bias=True)\n",
        "        self.relu7 = nn.ReLU(inplace=True)\n",
        "        self.dropout7 = nn.Dropout(p=0.5)\n",
        "        self.fc8 = nn.Linear(in_features=4096, out_features=2622, bias=True)\n",
        "\n",
        "    def forward(self, x0, is_fc8=False):\n",
        "        x1 = self.conv1_1(x0)\n",
        "        x2 = self.relu1_1(x1)\n",
        "        x3 = self.conv1_2(x2)\n",
        "        x4 = self.relu1_2(x3)\n",
        "        x5 = self.pool1(x4)\n",
        "        x6 = self.conv2_1(x5)\n",
        "        x7 = self.relu2_1(x6)\n",
        "        x8 = self.conv2_2(x7)\n",
        "        x9 = self.relu2_2(x8)\n",
        "        x10 = self.pool2(x9)\n",
        "        x11 = self.conv3_1(x10)\n",
        "        x12 = self.relu3_1(x11)\n",
        "        x13 = self.conv3_2(x12)\n",
        "        x14 = self.relu3_2(x13)\n",
        "        x15 = self.conv3_3(x14)\n",
        "        x16 = self.relu3_3(x15)\n",
        "        x17 = self.pool3(x16)\n",
        "        x18 = self.conv4_1(x17)\n",
        "        x19 = self.relu4_1(x18)\n",
        "        x20 = self.conv4_2(x19)\n",
        "        x21 = self.relu4_2(x20)\n",
        "        x22 = self.conv4_3(x21)\n",
        "        x23 = self.relu4_3(x22)\n",
        "        x24 = self.pool4(x23)\n",
        "        x25 = self.conv5_1(x24)\n",
        "        x26 = self.relu5_1(x25)\n",
        "        x27 = self.conv5_2(x26)\n",
        "        x28 = self.relu5_2(x27)\n",
        "        x29 = self.conv5_3(x28)\n",
        "        x30 = self.relu5_3(x29)\n",
        "        x31_preflatten = self.pool5(x30)\n",
        "        x31 = x31_preflatten.view(x31_preflatten.size(0), -1)\n",
        "        x32 = self.fc6(x31)\n",
        "        x33 = self.relu6(x32)\n",
        "        x34 = self.dropout6(x33)\n",
        "        x35 = self.fc7(x34)\n",
        "        x36 = self.relu7(x35)\n",
        "        x37 = self.dropout7(x36)\n",
        "        if is_fc8:\n",
        "            x38 = self.fc8(x37)\n",
        "        else:\n",
        "            x38 = x37\n",
        "        return x38\n",
        "\n",
        "\n",
        "def vgg_face_dag(weights_path=None, **kwargs):\n",
        "    \"\"\"\n",
        "    load imported model instance\n",
        "    Args:\n",
        "        weights_path (str): If set, loads model weights from the given path\n",
        "    \"\"\"\n",
        "    model = Vgg_face_dag()\n",
        "    if weights_path:\n",
        "        state_dict = torch.load(weights_path)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5xUu6b9rBsl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DECODER(nn.Module):\n",
        "    def __init__(self, phase):\n",
        "        super(DECODER, self).__init__()\n",
        "        self.phase = phase\n",
        "        self.fc3 = nn.Linear(4096, 1000)\n",
        "        self.ReLU = nn.ReLU()\n",
        "        #self.fc_bn3 = nn.BatchNorm1d(1000)\n",
        "\n",
        "\n",
        "        self.fc4 = nn.Linear(1000, 14 * 14 * 256)\n",
        "        self.fc_bn4 = nn.BatchNorm1d(14 * 14 * 256)\n",
        "        def TransConv( i, kernal = 5, stride = 2, inp = None):\n",
        "            if not inp:\n",
        "                inp = max(256//2**(i-1), 32)\n",
        "\n",
        "            layer =  nn.Sequential(\n",
        "                nn.ConvTranspose2d(inp, max(256//2**i, 32), \n",
        "                                kernal, stride=stride, padding=2, output_padding=1, \n",
        "                                dilation=1, padding_mode='zeros'),\n",
        "                nn.ReLU(),\n",
        "                nn.BatchNorm2d(max(256//2**i, 32)))\n",
        "            return layer\n",
        "        self.T1_ = TransConv(1, inp = 256)\n",
        "        self.T2_ = TransConv(2)\n",
        "        self.T3_ = TransConv(3)\n",
        "        self.T4_ = TransConv(4)\n",
        "    \n",
        "        self.ConvLast = nn.Sequential(\n",
        "            nn.Conv2d(32, 3, (1,1), stride=1),\n",
        "            nn.BatchNorm2d(3),\n",
        "            nn.ReLU())\n",
        "\n",
        "\n",
        "        self.layerLandmark1 = nn.Linear(1000, 800)\n",
        "        self.layerLandmark2 = nn.Linear(800, 600)\n",
        "        self.layerLandmark3 = nn.Linear(600, 400)\n",
        "        self.layerLandmark4 = nn.Linear(400, 200)\n",
        "        self.layerLandmark5 = nn.Linear(200, 144)\n",
        "\n",
        "    def forward(self, x):\n",
        "        L1 = self.fc3(x)\n",
        "        L1 = self.ReLU(L1)\n",
        "\n",
        "\n",
        "        L2 = self.layerLandmark1(L1)\n",
        "        L2 = self.ReLU(L2)\n",
        "\n",
        "        L3 = self.layerLandmark2(L2)\n",
        "        L3 = self.ReLU(L3)\n",
        "\n",
        "        L4 = self.layerLandmark3(L3)\n",
        "        L4 = self.ReLU(L4)\n",
        "\n",
        "        L5 = self.layerLandmark4(L4)\n",
        "        L5 = self.ReLU(L5)\n",
        "\n",
        "        L6 = self.layerLandmark5(L5)\n",
        "        outL = self.ReLU(L6)\n",
        "\n",
        "\n",
        "        # B1 = self.fc_bn3(L1) \n",
        "        T0 = self.fc4(L1) \n",
        "        T0 = self.ReLU(T0)\n",
        "        # T0 = self.fc_bn4(T0)\n",
        "        T0 = T0.view(-1,256,14,14)\n",
        "\n",
        "\n",
        "\n",
        "        T1 = self.T1_(T0)\n",
        "        T2 = self.T2_(T1)\n",
        "        T3 = self.T3_(T2)\n",
        "        T4 = self.T4_(T3)\n",
        "\n",
        "        outT = self.ConvLast(T4)\n",
        "        if self.phase == \"train\":\n",
        "            return outL,  outT "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6bEqmFYrNyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Compose(object):\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "        \n",
        "    def __call__(self, x):\n",
        "        for t in self.transforms:\n",
        "            x = t(x)\n",
        "        return x\n",
        "    \n",
        "class Resize(object):\n",
        "    def __init__(self, input_size):\n",
        "        self.input_size = input_size\n",
        "        \n",
        "    def __call__(self, img):\n",
        "        img = img.resize((self.input_size, self.input_size),\n",
        "                                Image.BICUBIC)\n",
        "        return img\n",
        "    \n",
        "class Normalize_Tensor(object):\n",
        "    def __init__(self, color_mean, color_std):\n",
        "        self.color_mean = color_mean\n",
        "        self.color_std = color_std\n",
        "\n",
        "    def __call__(self, img):\n",
        "\n",
        "        # PIL画像をTensorに。大きさは最大1に規格化される\n",
        "        img = transforms.functional.to_tensor(img)\n",
        "\n",
        "        # 色情報の標準化\n",
        "        img = transforms.functional.normalize(\n",
        "            img, self.color_mean, self.color_std)\n",
        "\n",
        "        return img\n",
        "\n",
        "class ImageTransform():\n",
        "    def __init__(self, input_size, color_mean, color_std):\n",
        "        self.data_transform = {\n",
        "            'train' : Compose([\n",
        "                Resize(input_size),\n",
        "                Normalize_Tensor(color_mean, color_std)\n",
        "            ]),\n",
        "            'val' : Compose([\n",
        "                Resize(input_size),\n",
        "                Normalize_Tensor(color_mean, color_std)\n",
        "            ])\n",
        "        }\n",
        "    \n",
        "    def __call_(self, phase, img):\n",
        "        return self.data_transform[phase](img)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvB-OcCG8s8m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############### THIS CELL IS COPIED FROM DEEP SPEECH MOZILLA #########################\n",
        "\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tfv1\n",
        "from tensorflow.compat import dimension_value\n",
        "from tensorflow.contrib.image import dense_image_warp\n",
        "from tensorflow.contrib.image import interpolate_spline\n",
        "\n",
        "from tensorflow.python.framework import ops\n",
        "from tensorflow.python.ops import array_ops\n",
        "\n",
        "def _to_float32(value):\n",
        "    return tf.cast(value, tf.float32)\n",
        "\n",
        "def _to_int32(value):\n",
        "    return tf.cast(value, tf.int32)\n",
        "\n",
        "def _get_grid_locations(image_height, image_width):\n",
        "    \"\"\"Wrapper for np.meshgrid.\"\"\"\n",
        "    tfv1.assert_type(image_height, tf.int32)\n",
        "    tfv1.assert_type(image_width, tf.int32)\n",
        "\n",
        "    y_range = tf.range(image_height)\n",
        "    x_range = tf.range(image_width)\n",
        "    y_grid, x_grid = tf.meshgrid(y_range, x_range, indexing='ij')\n",
        "    return tf.stack((y_grid, x_grid), -1)\n",
        "\n",
        "\n",
        "def _expand_to_minibatch(tensor, batch_size):\n",
        "    \"\"\"Tile arbitrarily-sized np_array to include new batch dimension.\"\"\"\n",
        "    ndim = tf.size(tf.shape(tensor))\n",
        "    ones = tf.ones((ndim,), tf.int32)\n",
        "\n",
        "    tiles = tf.concat(([batch_size], ones), 0)\n",
        "    return tf.tile(tf.expand_dims(tensor, 0), tiles)\n",
        "\n",
        "\n",
        "def _get_boundary_locations(image_height, image_width, num_points_per_edge):\n",
        "    \"\"\"Compute evenly-spaced indices along edge of image.\"\"\"\n",
        "    image_height_end = _to_float32(tf.math.subtract(image_height, 1))\n",
        "    image_width_end = _to_float32(tf.math.subtract(image_width, 1))\n",
        "    y_range = tf.linspace(0.0, image_height_end, num_points_per_edge + 2)\n",
        "    x_range = tf.linspace(0.0, image_height_end, num_points_per_edge + 2)\n",
        "    ys, xs = tf.meshgrid(y_range, x_range, indexing='ij')\n",
        "    is_boundary = tf.logical_or(\n",
        "        tf.logical_or(tf.equal(xs, 0.0), tf.equal(xs, image_width_end)),\n",
        "        tf.logical_or(tf.equal(ys, 0.0), tf.equal(ys, image_height_end)))\n",
        "    return tf.stack([tf.boolean_mask(ys, is_boundary), tf.boolean_mask(xs, is_boundary)], axis=-1)\n",
        "\n",
        "\n",
        "def _add_zero_flow_controls_at_boundary(control_point_locations,\n",
        "                                        control_point_flows, image_height,\n",
        "                                        image_width, boundary_points_per_edge):\n",
        "    \"\"\"Add control points for zero-flow boundary conditions.\n",
        "     Augment the set of control points with extra points on the\n",
        "     boundary of the image that have zero flow.\n",
        "    Args:\n",
        "      control_point_locations: input control points\n",
        "      control_point_flows: their flows\n",
        "      image_height: image height\n",
        "      image_width: image width\n",
        "      boundary_points_per_edge: number of points to add in the middle of each\n",
        "                             edge (not including the corners).\n",
        "                             The total number of points added is\n",
        "                             4 + 4*(boundary_points_per_edge).\n",
        "    Returns:\n",
        "      merged_control_point_locations: augmented set of control point locations\n",
        "      merged_control_point_flows: augmented set of control point flows\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size = dimension_value(tf.shape(control_point_locations)[0])\n",
        "\n",
        "    boundary_point_locations = _get_boundary_locations(image_height, image_width,\n",
        "                                                       boundary_points_per_edge)\n",
        "    boundary_point_shape = tf.shape(boundary_point_locations)\n",
        "    boundary_point_flows = tf.zeros([boundary_point_shape[0], 2])\n",
        "\n",
        "    minbatch_locations = _expand_to_minibatch(boundary_point_locations, batch_size)\n",
        "    type_to_use = control_point_locations.dtype\n",
        "    boundary_point_locations = tf.cast(minbatch_locations, type_to_use)\n",
        "\n",
        "    minbatch_flows = _expand_to_minibatch(boundary_point_flows, batch_size)\n",
        "\n",
        "    boundary_point_flows = tf.cast(minbatch_flows, type_to_use)\n",
        "\n",
        "    merged_control_point_locations = tf.concat(\n",
        "        [control_point_locations, boundary_point_locations], 1)\n",
        "\n",
        "    merged_control_point_flows = tf.concat(\n",
        "        [control_point_flows, boundary_point_flows], 1)\n",
        "\n",
        "    return merged_control_point_locations, merged_control_point_flows\n",
        "\n",
        "\n",
        "def sparse_image_warp(image,\n",
        "                      source_control_point_locations,\n",
        "                      dest_control_point_locations,\n",
        "                      interpolation_order=2,\n",
        "                      regularization_weight=0.0,\n",
        "                      num_boundary_points=0,\n",
        "                      name='sparse_image_warp'):\n",
        "    \"\"\"Image warping using correspondences between sparse control points.\n",
        "    Apply a non-linear warp to the image, where the warp is specified by\n",
        "    the source and destination locations of a (potentially small) number of\n",
        "    control points. First, we use a polyharmonic spline\n",
        "    (`tf.contrib.image.interpolate_spline`) to interpolate the displacements\n",
        "    between the corresponding control points to a dense flow field.\n",
        "    Then, we warp the image using this dense flow field\n",
        "    (`tf.contrib.image.dense_image_warp`).\n",
        "    Let t index our control points. For regularization_weight=0, we have:\n",
        "    warped_image[b, dest_control_point_locations[b, t, 0],\n",
        "                    dest_control_point_locations[b, t, 1], :] =\n",
        "    image[b, source_control_point_locations[b, t, 0],\n",
        "             source_control_point_locations[b, t, 1], :].\n",
        "    For regularization_weight > 0, this condition is met approximately, since\n",
        "    regularized interpolation trades off smoothness of the interpolant vs.\n",
        "    reconstruction of the interpolant at the control points.\n",
        "    See `tf.contrib.image.interpolate_spline` for further documentation of the\n",
        "    interpolation_order and regularization_weight arguments.\n",
        "    Args:\n",
        "      image: `[batch, height, width, channels]` float `Tensor`\n",
        "      source_control_point_locations: `[batch, num_control_points, 2]` float\n",
        "        `Tensor`\n",
        "      dest_control_point_locations: `[batch, num_control_points, 2]` float\n",
        "        `Tensor`\n",
        "      interpolation_order: polynomial order used by the spline interpolation\n",
        "      regularization_weight: weight on smoothness regularizer in interpolation\n",
        "      num_boundary_points: How many zero-flow boundary points to include at\n",
        "        each image edge.Usage:\n",
        "          num_boundary_points=0: don't add zero-flow points\n",
        "          num_boundary_points=1: 4 corners of the image\n",
        "          num_boundary_points=2: 4 corners and one in the middle of each edge\n",
        "            (8 points total)\n",
        "          num_boundary_points=n: 4 corners and n-1 along each edge\n",
        "      name: A name for the operation (optional).\n",
        "      Note that image and offsets can be of type tf.half, tf.float32, or\n",
        "      tf.float64, and do not necessarily have to be the same type.\n",
        "    Returns:\n",
        "      warped_image: `[batch, height, width, channels]` float `Tensor` with same\n",
        "        type as input image.\n",
        "      flow_field: `[batch, height, width, 2]` float `Tensor` containing the dense\n",
        "        flow field produced by the interpolation.\n",
        "    \"\"\"\n",
        "\n",
        "    image = ops.convert_to_tensor(image)\n",
        "    source_control_point_locations = ops.convert_to_tensor(\n",
        "        source_control_point_locations)\n",
        "    dest_control_point_locations = ops.convert_to_tensor(\n",
        "        dest_control_point_locations)\n",
        "\n",
        "    control_point_flows = (\n",
        "        dest_control_point_locations - source_control_point_locations)\n",
        "\n",
        "    clamp_boundaries = num_boundary_points > 0\n",
        "    boundary_points_per_edge = num_boundary_points - 1\n",
        "\n",
        "    with ops.name_scope(name):\n",
        "        image_shape = tf.shape(image)\n",
        "        batch_size, image_height, image_width = image_shape[0], image_shape[1], image_shape[2]\n",
        "\n",
        "        # This generates the dense locations where the interpolant\n",
        "        # will be evaluated.\n",
        "        grid_locations = _get_grid_locations(image_height, image_width)\n",
        "\n",
        "        flattened_grid_locations = tf.reshape(grid_locations,\n",
        "                                              [tf.multiply(image_height, image_width), 2])\n",
        "\n",
        "        # flattened_grid_locations = constant_op.constant(\n",
        "        #     _expand_to_minibatch(flattened_grid_locations, batch_size), image.dtype)\n",
        "        flattened_grid_locations = _expand_to_minibatch(flattened_grid_locations, batch_size)\n",
        "        flattened_grid_locations = tf.cast(flattened_grid_locations, dtype=image.dtype)\n",
        "\n",
        "        if clamp_boundaries:\n",
        "            (dest_control_point_locations,\n",
        "             control_point_flows) = _add_zero_flow_controls_at_boundary(\n",
        "                 dest_control_point_locations, control_point_flows, image_height,\n",
        "                 image_width, boundary_points_per_edge)\n",
        "\n",
        "        flattened_flows = interpolate_spline(\n",
        "            dest_control_point_locations, control_point_flows,\n",
        "            flattened_grid_locations, interpolation_order, regularization_weight)\n",
        "\n",
        "        dense_flows = array_ops.reshape(flattened_flows,\n",
        "                                        [batch_size, image_height, image_width, 2])\n",
        "\n",
        "        warped_image = dense_image_warp(image, dense_flows)\n",
        "\n",
        "        return warped_image, dense_flows"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvjYFqcm8t6E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def image_warping(src_img, src_landmarks, dest_landmarks):\n",
        "    # expanded_src_landmarks = np.expand_dims(np.float32(src_landmarks), axis=0)\n",
        "    # expanded_dest_landmarks = np.expand_dims(np.float32(dest_landmarks), axis=0)\n",
        "    # expanded_src_img = np.expand_dims(np.float32(src_img) / 255, axis=0)\n",
        "\n",
        "    warped_img, dense_flows = sparse_image_warp(src_img,\n",
        "                          src_landmarks,\n",
        "                          dest_landmarks,\n",
        "                          interpolation_order=1,\n",
        "                          regularization_weight=0.1,\n",
        "                          num_boundary_points=2,\n",
        "                          name='sparse_image_warp')\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        out_img = sess.run(warped_img)\n",
        "        warp_img = np.uint8(out_img[:, :, :, :])\n",
        "    \n",
        "    return warp_img\n",
        "    \n",
        "def face_landmark(img):\n",
        "    X = np.zeros((img.shape[0], 72 ,2))\n",
        "    flag = []\n",
        "    for i in range(img.shape[0]):\n",
        "\n",
        "        landmark = face_landmarks(img[i].reshape(224,224,3))\n",
        "        resultant = []\n",
        "        try:\n",
        "            for j in list(landmark[0].keys()):\n",
        "                resultant += landmark[0][j] \n",
        "        except IndexError:\n",
        "            flag.append(i)\n",
        "            continue\n",
        "        X[i] = np.array(resultant)\n",
        "    return  X, flag"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s86Vf2YTHzvp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path='/content/drive/My Drive/Speech2Face/models/face_decoder_warping/new_dataset_v5/epoch_26_steps_0.pth'\n",
        "#checkpoint_path=None\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Set Model\n",
        "model = DECODER('train').to(device)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "global_epoch = 0\n",
        "\n",
        "if checkpoint_path:\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    global_epoch = checkpoint[\"epoch\"] +1\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "\n",
        "# Set loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "criterion2 = nn.L1Loss()\n",
        "alpha = 0.0002\n",
        "beta  = 1.0\n",
        "\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9l4PFgT7NUD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_dir='/content/drive/My Drive/Speech2Face/'"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4UAJu92re1f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ba961629-fe71-402a-8ea3-3ecfd98fe2d5"
      },
      "source": [
        "#import argparse\n",
        "\n",
        "\n",
        "\n",
        "#from dataloader import EmbedImagePairs\n",
        "#from face_encoder_model import vgg_face_dag\n",
        "#from face_decoder_model import Generator\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def _save_model(epoch, model, optimizer, steps, fname):\n",
        "    torch.save({\n",
        "        'steps': steps,\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        }, join(output_dir_name, 'epoch_{}_steps_{}.pth'.format(epoch, steps)))\n",
        "\n",
        "\n",
        "def train_epoch(epoch,model, trn_dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "\n",
        "    running_loss = 0\n",
        "    dataset_len = len(trn_dataloader.dataset)\n",
        "\n",
        "    count = 0\n",
        "    numOfElements = 0\n",
        "    saving_steps = 50\n",
        "\n",
        "    for x,landmark, y in trn_dataloader:\n",
        "        \n",
        "        count += 1\n",
        "        x = x.squeeze().to(device)  \n",
        "\n",
        "        # y_img = cv2.cvtColor(np.einsum('abc->bca',y[0].numpy()*255),cv2.COLOR_BGR2RGB)\n",
        "        # cv2_imshow(y_img)\n",
        "        y = y.reshape((-1, 3, 224, 224))\n",
        "\n",
        "        y = y.to(device)\n",
        "        landmark = landmark.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outLandmark, outputs  = model(x)\n",
        "\n",
        "\n",
        "        #print(\"\\n\\nO\", outputs.shape, outputs.dtype) #[-1, 3, 224,244]\n",
        "        #print(\"y\", y.shape, y.dtype) # [-1, 224,224, 3]\n",
        "\n",
        "\n",
        "        loss2 = criterion2(outputs, y)\n",
        "        loss1 = criterion(outLandmark, landmark)\n",
        "\n",
        "        loss = (alpha*loss1 + beta*loss2 )\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        # o_img = cv2.cvtColor(np.einsum('abc->bca',outputs[0].cpu().detach().numpy()*255),cv2.COLOR_BGR2RGB)\n",
        "        #cv2_imshow(o_img)\n",
        "        running_loss += loss.item() * x.size(0)\n",
        "        numOfElements += x.shape[0]\n",
        "        \n",
        "\n",
        "        \n",
        "        # outLandmark = outLandmark.cpu().detach().numpy()\n",
        "        # outLandmark = np.dstack((outLandmark[:,0::2],outLandmark[:,1::2]))\n",
        "        # print(\"land np img np \", outL_.shape, img_.shape)\n",
        "        # img1 = np.einsum('dabc->dbca',outputs.cpu().detach().numpy()*255)\n",
        "        # img1 = (img1).astype(np.uint8)\n",
        "        #print(\"img_t \",img_t.shape, img_t[0])\n",
        "        # src, flag = face_landmark(img1)\n",
        "        # if flag:\n",
        "        #     for r in flag:\n",
        "        #         src[r] = outLandmark[r]\n",
        "        # result = image_warping(img1.astype(np.float32), src.astype(np.float32), outLandmark.astype(np.float32))\n",
        "\n",
        "        # w_img = cv2.cvtColor(result[0], cv2.COLOR_BGR2RGB)\n",
        "        #cv2_imshow(w_img)\n",
        "\n",
        "        #print(\"loss:\", running_loss / (x.shape[0] * count) , \"Steps:\", str(numOfElements) + \"/\" + str(dataset_len) )\n",
        "\n",
        "        #if (numOfElements % (x.shape[0] * saving_steps)) == 0:\n",
        "        #  _save_model(epoch, model, optimizer, numOfElements, output_dir_name)\n",
        "    \n",
        "    trn_loss = running_loss/len(trn_dataloader.dataset)\n",
        "    \n",
        "    return trn_loss\n",
        "\n",
        "\n",
        "def train(model, dataloaders, criterion, optimizer, device, output_dir_name, num_epochs=100):\n",
        "    trn_losses = []\n",
        "\n",
        "    # make dir where trained model saved\n",
        "    os.makedirs(output_dir_name, exist_ok=True)\n",
        "\n",
        "    # start at zero, end at num_epochs (including)\n",
        "    print(\"Start Training\")\n",
        "    for epoch in tqdm(range(global_epoch, num_epochs+1)):\n",
        "        # Training phase\n",
        "        trn_loss = train_epoch(epoch, model, dataloaders, criterion, optimizer, device)\n",
        "        trn_losses += [trn_loss]\n",
        "        print(\"Epoch: {}  || Loss: {}\".format(epoch, trn_loss))\n",
        "    \n",
        "        _save_model(epoch, model, optimizer, 0, output_dir_name)\n",
        "\n",
        "\n",
        "base_dir='/content/drive/My Drive/Speech2Face/'\n",
        "input=\"vox1_dataset/unzippedFaces\"\n",
        "batch_size=128\n",
        "num_epochs=80\n",
        "\n",
        "# Load face-encoder model\n",
        "# face_encoder_path = join(base_dir, \"Pretrained\", \"vgg_face_dag.pth\")\n",
        "# face_encoder_model = vgg_face_dag(face_encoder_path).to(device)\n",
        "# color_mean, color_std = face_encoder_model.meta[\"mean\"], face_encoder_model.meta[\"std\"]\n",
        "# color_mean = [tmp / 255.0 for tmp in color_mean]\n",
        "# color_std = [tmp / 255.0 for tmp in color_std]\n",
        "\n",
        "# Set DataLoader\n",
        "input_path = '/content/content/Face_Feature/'\n",
        "print('Image Pairing')\n",
        "dataset = EmbedImagePairs(input_path)\n",
        "print('Creating dataloader')\n",
        "trn_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
        "output_dir_name = join(base_dir, \"models\", \"face_decoder_warping/new_dataset_v5\")\n",
        "\n",
        "# Train\n",
        "train(model, trn_dataloader, criterion, optimizer, device, output_dir_name, num_epochs=num_epochs)\n",
        "print(\"Finish training\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/81 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Image Pairing\n",
            "Creating dataloader\n",
            "Start Training\n",
            "loss: 52.302608489990234 Steps: 128/12117\n",
            "loss: 51.53023338317871 Steps: 256/12117\n",
            "loss: 51.16382853190104 Steps: 384/12117\n",
            "loss: 51.66936111450195 Steps: 512/12117\n",
            "loss: 52.076507568359375 Steps: 640/12117\n",
            "loss: 52.19790776570638 Steps: 768/12117\n",
            "loss: 52.13435091291155 Steps: 896/12117\n",
            "loss: 52.18768262863159 Steps: 1024/12117\n",
            "loss: 52.28740310668945 Steps: 1152/12117\n",
            "loss: 52.34021987915039 Steps: 1280/12117\n",
            "loss: 52.34658640081232 Steps: 1408/12117\n",
            "loss: 52.50662803649902 Steps: 1536/12117\n",
            "loss: 52.532684326171875 Steps: 1664/12117\n",
            "loss: 52.58227757045201 Steps: 1792/12117\n",
            "loss: 52.535779825846355 Steps: 1920/12117\n",
            "loss: 52.46027421951294 Steps: 2048/12117\n",
            "loss: 52.39029940436868 Steps: 2176/12117\n",
            "loss: 52.385089026557075 Steps: 2304/12117\n",
            "loss: 52.35911700600072 Steps: 2432/12117\n",
            "loss: 52.31344718933106 Steps: 2560/12117\n",
            "loss: 52.22888982863653 Steps: 2688/12117\n",
            "loss: 52.26574568314986 Steps: 2816/12117\n",
            "loss: 52.25911198491635 Steps: 2944/12117\n",
            "loss: 52.220449129740395 Steps: 3072/12117\n",
            "loss: 52.322274169921876 Steps: 3200/12117\n",
            "loss: 52.29467568030724 Steps: 3328/12117\n",
            "loss: 52.32675396954572 Steps: 3456/12117\n",
            "loss: 52.311687333243235 Steps: 3584/12117\n",
            "loss: 52.243493441877696 Steps: 3712/12117\n",
            "loss: 52.27099850972493 Steps: 3840/12117\n",
            "loss: 52.22773767286731 Steps: 3968/12117\n",
            "loss: 52.19342386722565 Steps: 4096/12117\n",
            "loss: 52.24518666122899 Steps: 4224/12117\n",
            "loss: 52.23229834612678 Steps: 4352/12117\n",
            "loss: 52.2389410836356 Steps: 4480/12117\n",
            "loss: 52.210700564914276 Steps: 4608/12117\n",
            "loss: 52.240482124122416 Steps: 4736/12117\n",
            "loss: 52.250362597013776 Steps: 4864/12117\n",
            "loss: 52.25343655317258 Steps: 4992/12117\n",
            "loss: 52.25035753250122 Steps: 5120/12117\n",
            "loss: 52.29135169052496 Steps: 5248/12117\n",
            "loss: 52.30682790847052 Steps: 5376/12117\n",
            "loss: 52.32799627614576 Steps: 5504/12117\n",
            "loss: 52.33272344415838 Steps: 5632/12117\n",
            "loss: 52.332114919026694 Steps: 5760/12117\n",
            "loss: 52.33602366240128 Steps: 5888/12117\n",
            "loss: 52.313295323797995 Steps: 6016/12117\n",
            "loss: 52.307350953420006 Steps: 6144/12117\n",
            "loss: 52.28374901596381 Steps: 6272/12117\n",
            "loss: 52.288993759155275 Steps: 6400/12117\n",
            "loss: 52.281676722507854 Steps: 6528/12117\n",
            "loss: 52.2733830672044 Steps: 6656/12117\n",
            "loss: 52.28641617972896 Steps: 6784/12117\n",
            "loss: 52.30200216505263 Steps: 6912/12117\n",
            "loss: 52.312375571511005 Steps: 7040/12117\n",
            "loss: 52.32941395895822 Steps: 7168/12117\n",
            "loss: 52.33007484569884 Steps: 7296/12117\n",
            "loss: 52.32852304392848 Steps: 7424/12117\n",
            "loss: 52.31941261938063 Steps: 7552/12117\n",
            "loss: 52.310197766621904 Steps: 7680/12117\n",
            "loss: 52.31156921386719 Steps: 7808/12117\n",
            "loss: 52.30280020929152 Steps: 7936/12117\n",
            "loss: 52.30116526285807 Steps: 8064/12117\n",
            "loss: 52.3242307305336 Steps: 8192/12117\n",
            "loss: 52.340707573523886 Steps: 8320/12117\n",
            "loss: 52.342804937651664 Steps: 8448/12117\n",
            "loss: 52.33094673726096 Steps: 8576/12117\n",
            "loss: 52.31468677520752 Steps: 8704/12117\n",
            "loss: 52.30525633217632 Steps: 8832/12117\n",
            "loss: 52.31216158185686 Steps: 8960/12117\n",
            "loss: 52.304182294388895 Steps: 9088/12117\n",
            "loss: 52.302627404530845 Steps: 9216/12117\n",
            "loss: 52.291390562710696 Steps: 9344/12117\n",
            "loss: 52.299219801619245 Steps: 9472/12117\n",
            "loss: 52.287412058512366 Steps: 9600/12117\n",
            "loss: 52.29135096700568 Steps: 9728/12117\n",
            "loss: 52.28096191604416 Steps: 9856/12117\n",
            "loss: 52.25547321026142 Steps: 9984/12117\n",
            "loss: 52.272271168382865 Steps: 10112/12117\n",
            "loss: 52.27005496025085 Steps: 10240/12117\n",
            "loss: 52.27303582650644 Steps: 10368/12117\n",
            "loss: 52.272686423324956 Steps: 10496/12117\n",
            "loss: 52.27080260127424 Steps: 10624/12117\n",
            "loss: 52.276832262674965 Steps: 10752/12117\n",
            "loss: 52.28547040153952 Steps: 10880/12117\n",
            "loss: 52.30360541232797 Steps: 11008/12117\n",
            "loss: 52.32215776114628 Steps: 11136/12117\n",
            "loss: 52.328705224123865 Steps: 11264/12117\n",
            "loss: 52.314298265435724 Steps: 11392/12117\n",
            "loss: 52.31365581088596 Steps: 11520/12117\n",
            "loss: 52.32399938394735 Steps: 11648/12117\n",
            "loss: 52.31984557276187 Steps: 11776/12117\n",
            "loss: 52.29402107320806 Steps: 11904/12117\n",
            "loss: 52.301753510820106 Steps: 12032/12117\n",
            "loss: 78.46419787144144 Steps: 12117/12117\n",
            "Epoch: 0  || Loss: 52.29003860789714\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "  1%|          | 1/81 [08:04<10:46:26, 484.83s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 50.85444641113281 Steps: 128/12117\n",
            "loss: 52.554800033569336 Steps: 256/12117\n",
            "loss: 52.903082529703774 Steps: 384/12117\n",
            "loss: 52.364752769470215 Steps: 512/12117\n",
            "loss: 52.39353256225586 Steps: 640/12117\n",
            "loss: 52.60896301269531 Steps: 768/12117\n",
            "loss: 52.64117431640625 Steps: 896/12117\n",
            "loss: 52.8578200340271 Steps: 1024/12117\n",
            "loss: 52.73579321967231 Steps: 1152/12117\n",
            "loss: 52.58002281188965 Steps: 1280/12117\n",
            "loss: 52.53561921553178 Steps: 1408/12117\n",
            "loss: 52.48404630025228 Steps: 1536/12117\n",
            "loss: 52.50322048480694 Steps: 1664/12117\n",
            "loss: 52.61994334629604 Steps: 1792/12117\n",
            "loss: 52.60984624226888 Steps: 1920/12117\n",
            "loss: 52.60027360916138 Steps: 2048/12117\n",
            "loss: 52.57835590138155 Steps: 2176/12117\n",
            "loss: 52.543329026963974 Steps: 2304/12117\n",
            "loss: 52.5266103242573 Steps: 2432/12117\n",
            "loss: 52.49924240112305 Steps: 2560/12117\n",
            "loss: 52.44857842581613 Steps: 2688/12117\n",
            "loss: 52.49384897405451 Steps: 2816/12117\n",
            "loss: 52.40681789232337 Steps: 2944/12117\n",
            "loss: 52.39076979955038 Steps: 3072/12117\n",
            "loss: 52.4400749206543 Steps: 3200/12117\n",
            "loss: 52.477766036987305 Steps: 3328/12117\n",
            "loss: 52.43300911232277 Steps: 3456/12117\n",
            "loss: 52.34583868299212 Steps: 3584/12117\n",
            "loss: 52.364707420612206 Steps: 3712/12117\n",
            "loss: 52.36281051635742 Steps: 3840/12117\n",
            "loss: 52.36500426261656 Steps: 3968/12117\n",
            "loss: 52.30961310863495 Steps: 4096/12117\n",
            "loss: 52.28671426484079 Steps: 4224/12117\n",
            "loss: 52.27689170837402 Steps: 4352/12117\n",
            "loss: 52.28815340314593 Steps: 4480/12117\n",
            "loss: 52.29879252115885 Steps: 4608/12117\n",
            "loss: 52.314695925325964 Steps: 4736/12117\n",
            "loss: 52.30998902571829 Steps: 4864/12117\n",
            "loss: 52.33464842576247 Steps: 4992/12117\n",
            "loss: 52.35790309906006 Steps: 5120/12117\n",
            "loss: 52.34368431277392 Steps: 5248/12117\n",
            "loss: 52.32275663103376 Steps: 5376/12117\n",
            "loss: 52.30635922454124 Steps: 5504/12117\n",
            "loss: 52.281808679754086 Steps: 5632/12117\n",
            "loss: 52.279607984754776 Steps: 5760/12117\n",
            "loss: 52.2693862085757 Steps: 5888/12117\n",
            "loss: 52.258477637108335 Steps: 6016/12117\n",
            "loss: 52.26661904652914 Steps: 6144/12117\n",
            "loss: 52.27745655604771 Steps: 6272/12117\n",
            "loss: 52.25770248413086 Steps: 6400/12117\n",
            "loss: 52.28714438045726 Steps: 6528/12117\n",
            "loss: 52.30361967820387 Steps: 6656/12117\n",
            "loss: 52.30714366121112 Steps: 6784/12117\n",
            "loss: 52.2912409040663 Steps: 6912/12117\n",
            "loss: 52.31500029130415 Steps: 7040/12117\n",
            "loss: 52.30873319080898 Steps: 7168/12117\n",
            "loss: 52.307149385151114 Steps: 7296/12117\n",
            "loss: 52.286406878767345 Steps: 7424/12117\n",
            "loss: 52.250982834120926 Steps: 7552/12117\n",
            "loss: 52.21039950052897 Steps: 7680/12117\n",
            "loss: 52.218017015300816 Steps: 7808/12117\n",
            "loss: 52.18963850698164 Steps: 7936/12117\n",
            "loss: 52.17022365993924 Steps: 8064/12117\n",
            "loss: 52.172868490219116 Steps: 8192/12117\n",
            "loss: 52.190515840970555 Steps: 8320/12117\n",
            "loss: 52.217481266368516 Steps: 8448/12117\n",
            "loss: 52.217850699353576 Steps: 8576/12117\n",
            "loss: 52.22405893662397 Steps: 8704/12117\n",
            "loss: 52.232537587483726 Steps: 8832/12117\n",
            "loss: 52.23428916931152 Steps: 8960/12117\n",
            "loss: 52.255354814126456 Steps: 9088/12117\n",
            "loss: 52.2615639368693 Steps: 9216/12117\n",
            "loss: 52.25004285002408 Steps: 9344/12117\n",
            "loss: 52.237672187186575 Steps: 9472/12117\n",
            "loss: 52.25152099609375 Steps: 9600/12117\n",
            "loss: 52.24880840903834 Steps: 9728/12117\n",
            "loss: 52.24649642349838 Steps: 9856/12117\n",
            "loss: 52.24567198142027 Steps: 9984/12117\n",
            "loss: 52.24779022796245 Steps: 10112/12117\n",
            "loss: 52.24099531173706 Steps: 10240/12117\n",
            "loss: 52.22182012487341 Steps: 10368/12117\n",
            "loss: 52.23965049371487 Steps: 10496/12117\n",
            "loss: 52.237209044307114 Steps: 10624/12117\n",
            "loss: 52.233123733883815 Steps: 10752/12117\n",
            "loss: 52.23124133839327 Steps: 10880/12117\n",
            "loss: 52.21365999621014 Steps: 11008/12117\n",
            "loss: 52.22989194146518 Steps: 11136/12117\n",
            "loss: 52.2220554785295 Steps: 11264/12117\n",
            "loss: 52.20171210471164 Steps: 11392/12117\n",
            "loss: 52.18880733913846 Steps: 11520/12117\n",
            "loss: 52.184203011649 Steps: 11648/12117\n",
            "loss: 52.17844697703486 Steps: 11776/12117\n",
            "loss: 52.17582567276493 Steps: 11904/12117\n",
            "loss: 52.16587249268877 Steps: 12032/12117\n",
            "loss: 78.26466790308524 Steps: 12117/12117\n",
            "Epoch: 1  || Loss: 52.157068029826966\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "  2%|▏         | 2/81 [16:11<10:38:53, 485.24s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 51.9653205871582 Steps: 128/12117\n",
            "loss: 51.06343460083008 Steps: 256/12117\n",
            "loss: 51.44773737589518 Steps: 384/12117\n",
            "loss: 51.58135795593262 Steps: 512/12117\n",
            "loss: 51.74429626464844 Steps: 640/12117\n",
            "loss: 51.71845245361328 Steps: 768/12117\n",
            "loss: 51.84145518711635 Steps: 896/12117\n",
            "loss: 51.90685510635376 Steps: 1024/12117\n",
            "loss: 51.91380861070421 Steps: 1152/12117\n",
            "loss: 51.934804153442386 Steps: 1280/12117\n",
            "loss: 51.88020220669833 Steps: 1408/12117\n",
            "loss: 51.969194094340004 Steps: 1536/12117\n",
            "loss: 51.93427511361929 Steps: 1664/12117\n",
            "loss: 51.87215232849121 Steps: 1792/12117\n",
            "loss: 51.80691986083984 Steps: 1920/12117\n",
            "loss: 51.872581481933594 Steps: 2048/12117\n",
            "loss: 51.83522863949047 Steps: 2176/12117\n",
            "loss: 51.838842180040146 Steps: 2304/12117\n",
            "loss: 51.917585473311576 Steps: 2432/12117\n",
            "loss: 51.91621704101563 Steps: 2560/12117\n",
            "loss: 51.966436113630024 Steps: 2688/12117\n",
            "loss: 52.045361432162196 Steps: 2816/12117\n",
            "loss: 51.983666627303414 Steps: 2944/12117\n",
            "loss: 52.01308822631836 Steps: 3072/12117\n",
            "loss: 51.9707926940918 Steps: 3200/12117\n",
            "loss: 52.012566346388596 Steps: 3328/12117\n",
            "loss: 52.10860697428385 Steps: 3456/12117\n",
            "loss: 52.08708572387695 Steps: 3584/12117\n",
            "loss: 52.08739997600687 Steps: 3712/12117\n",
            "loss: 52.07725524902344 Steps: 3840/12117\n",
            "loss: 52.09093106177546 Steps: 3968/12117\n",
            "loss: 52.06222581863403 Steps: 4096/12117\n",
            "loss: 52.04789051865087 Steps: 4224/12117\n",
            "loss: 51.9757869944853 Steps: 4352/12117\n",
            "loss: 52.016450718470985 Steps: 4480/12117\n",
            "loss: 52.02445761362711 Steps: 4608/12117\n",
            "loss: 52.008452853640996 Steps: 4736/12117\n",
            "loss: 52.007028579711914 Steps: 4864/12117\n",
            "loss: 51.99930376884265 Steps: 4992/12117\n",
            "loss: 52.03678789138794 Steps: 5120/12117\n",
            "loss: 52.02109127509885 Steps: 5248/12117\n",
            "loss: 52.02316302344913 Steps: 5376/12117\n",
            "loss: 52.044510952261994 Steps: 5504/12117\n",
            "loss: 52.05014566941695 Steps: 5632/12117\n",
            "loss: 52.066950988769534 Steps: 5760/12117\n",
            "loss: 52.03221735746964 Steps: 5888/12117\n",
            "loss: 52.037114001334984 Steps: 6016/12117\n",
            "loss: 52.03454875946045 Steps: 6144/12117\n",
            "loss: 52.04961768948302 Steps: 6272/12117\n",
            "loss: 52.00623428344726 Steps: 6400/12117\n",
            "loss: 52.040525623396334 Steps: 6528/12117\n",
            "loss: 51.99797050769512 Steps: 6656/12117\n",
            "loss: 52.02382854245744 Steps: 6784/12117\n",
            "loss: 52.04321310255263 Steps: 6912/12117\n",
            "loss: 52.043276422674005 Steps: 7040/12117\n",
            "loss: 52.08648647580828 Steps: 7168/12117\n",
            "loss: 52.06078613013552 Steps: 7296/12117\n",
            "loss: 52.06793061618147 Steps: 7424/12117\n",
            "loss: 52.074055105952894 Steps: 7552/12117\n",
            "loss: 52.06337381998698 Steps: 7680/12117\n",
            "loss: 52.06464204631868 Steps: 7808/12117\n",
            "loss: 52.056078080208074 Steps: 7936/12117\n",
            "loss: 52.05135835920061 Steps: 8064/12117\n",
            "loss: 52.07004702091217 Steps: 8192/12117\n",
            "loss: 52.089297485351565 Steps: 8320/12117\n",
            "loss: 52.09545563206528 Steps: 8448/12117\n",
            "loss: 52.08975071693534 Steps: 8576/12117\n",
            "loss: 52.109878427842084 Steps: 8704/12117\n",
            "loss: 52.11697044925413 Steps: 8832/12117\n",
            "loss: 52.1079295022147 Steps: 8960/12117\n",
            "loss: 52.11168831838688 Steps: 9088/12117\n",
            "loss: 52.119359493255615 Steps: 9216/12117\n",
            "loss: 52.12313346340232 Steps: 9344/12117\n",
            "loss: 52.11848269282161 Steps: 9472/12117\n",
            "loss: 52.1165055847168 Steps: 9600/12117\n",
            "loss: 52.121059367531224 Steps: 9728/12117\n",
            "loss: 52.12417290427468 Steps: 9856/12117\n",
            "loss: 52.112831115722656 Steps: 9984/12117\n",
            "loss: 52.115397441236276 Steps: 10112/12117\n",
            "loss: 52.14522395133972 Steps: 10240/12117\n",
            "loss: 52.150189293755425 Steps: 10368/12117\n",
            "loss: 52.121818030752785 Steps: 10496/12117\n",
            "loss: 52.10281312322042 Steps: 10624/12117\n",
            "loss: 52.11414300827753 Steps: 10752/12117\n",
            "loss: 52.10317203297335 Steps: 10880/12117\n",
            "loss: 52.08781131478243 Steps: 11008/12117\n",
            "loss: 52.09754851768757 Steps: 11136/12117\n",
            "loss: 52.11489005522294 Steps: 11264/12117\n",
            "loss: 52.12334446424848 Steps: 11392/12117\n",
            "loss: 52.11016582912869 Steps: 11520/12117\n",
            "loss: 52.12363601516891 Steps: 11648/12117\n",
            "loss: 52.12734317779541 Steps: 11776/12117\n",
            "loss: 52.117688332834554 Steps: 11904/12117\n",
            "loss: 52.114455973848386 Steps: 12032/12117\n",
            "loss: 78.20238265707766 Steps: 12117/12117\n",
            "Epoch: 2  || Loss: 52.11555995344575\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "  4%|▎         | 3/81 [24:20<10:32:39, 486.66s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 50.41754913330078 Steps: 128/12117\n",
            "loss: 50.92031669616699 Steps: 256/12117\n",
            "loss: 51.414103190104164 Steps: 384/12117\n",
            "loss: 51.644012451171875 Steps: 512/12117\n",
            "loss: 51.52774353027344 Steps: 640/12117\n",
            "loss: 51.484768549601235 Steps: 768/12117\n",
            "loss: 51.643475668770925 Steps: 896/12117\n",
            "loss: 51.60706615447998 Steps: 1024/12117\n",
            "loss: 51.467672983805336 Steps: 1152/12117\n",
            "loss: 51.61051139831543 Steps: 1280/12117\n",
            "loss: 51.7142878445712 Steps: 1408/12117\n",
            "loss: 51.74673811594645 Steps: 1536/12117\n",
            "loss: 51.792064079871544 Steps: 1664/12117\n",
            "loss: 51.775753838675364 Steps: 1792/12117\n",
            "loss: 51.82195765177409 Steps: 1920/12117\n",
            "loss: 51.96766114234924 Steps: 2048/12117\n",
            "loss: 51.962854497572955 Steps: 2176/12117\n",
            "loss: 51.99962382846408 Steps: 2304/12117\n",
            "loss: 52.03665261519583 Steps: 2432/12117\n",
            "loss: 52.05871391296387 Steps: 2560/12117\n",
            "loss: 51.98131325131371 Steps: 2688/12117\n",
            "loss: 51.93849684975364 Steps: 2816/12117\n",
            "loss: 51.97233332758365 Steps: 2944/12117\n",
            "loss: 52.025208950042725 Steps: 3072/12117\n",
            "loss: 52.04302947998047 Steps: 3200/12117\n",
            "loss: 51.98709810697115 Steps: 3328/12117\n",
            "loss: 52.019374988697194 Steps: 3456/12117\n",
            "loss: 52.04200717381069 Steps: 3584/12117\n",
            "loss: 52.04070729222791 Steps: 3712/12117\n",
            "loss: 52.030239232381184 Steps: 3840/12117\n",
            "loss: 52.01492395708638 Steps: 3968/12117\n",
            "loss: 52.05627501010895 Steps: 4096/12117\n",
            "loss: 52.05821043072325 Steps: 4224/12117\n",
            "loss: 52.00090139052447 Steps: 4352/12117\n",
            "loss: 51.99368874686105 Steps: 4480/12117\n",
            "loss: 52.02189095815023 Steps: 4608/12117\n",
            "loss: 52.07565441647091 Steps: 4736/12117\n",
            "loss: 52.03695718865646 Steps: 4864/12117\n",
            "loss: 52.04638926188151 Steps: 4992/12117\n",
            "loss: 52.02997245788574 Steps: 5120/12117\n",
            "loss: 52.07032040851872 Steps: 5248/12117\n",
            "loss: 52.07843171982538 Steps: 5376/12117\n",
            "loss: 52.057023336721024 Steps: 5504/12117\n",
            "loss: 52.07169125296853 Steps: 5632/12117\n",
            "loss: 52.08418223063151 Steps: 5760/12117\n",
            "loss: 52.11002789372983 Steps: 5888/12117\n",
            "loss: 52.125062901922995 Steps: 6016/12117\n",
            "loss: 52.14015340805054 Steps: 6144/12117\n",
            "loss: 52.139765136095946 Steps: 6272/12117\n",
            "loss: 52.11924613952637 Steps: 6400/12117\n",
            "loss: 52.11259490368413 Steps: 6528/12117\n",
            "loss: 52.10456833472619 Steps: 6656/12117\n",
            "loss: 52.09802332464254 Steps: 6784/12117\n",
            "loss: 52.13470713297526 Steps: 6912/12117\n",
            "loss: 52.14768128828569 Steps: 7040/12117\n",
            "loss: 52.141576698848176 Steps: 7168/12117\n",
            "loss: 52.117776904189796 Steps: 7296/12117\n",
            "loss: 52.124273497482825 Steps: 7424/12117\n",
            "loss: 52.11388132127665 Steps: 7552/12117\n",
            "loss: 52.113669840494794 Steps: 7680/12117\n",
            "loss: 52.10542403674516 Steps: 7808/12117\n",
            "loss: 52.107068953975556 Steps: 7936/12117\n",
            "loss: 52.10582406180246 Steps: 8064/12117\n",
            "loss: 52.11641079187393 Steps: 8192/12117\n",
            "loss: 52.145196944016675 Steps: 8320/12117\n",
            "loss: 52.13641900727243 Steps: 8448/12117\n",
            "loss: 52.14464181928492 Steps: 8576/12117\n",
            "loss: 52.162076613482306 Steps: 8704/12117\n",
            "loss: 52.13959049832994 Steps: 8832/12117\n",
            "loss: 52.12833033970424 Steps: 8960/12117\n",
            "loss: 52.141447685134246 Steps: 9088/12117\n",
            "loss: 52.13189389970567 Steps: 9216/12117\n",
            "loss: 52.10969083603114 Steps: 9344/12117\n",
            "loss: 52.11197889173353 Steps: 9472/12117\n",
            "loss: 52.11090957641601 Steps: 9600/12117\n",
            "loss: 52.11374558900532 Steps: 9728/12117\n",
            "loss: 52.11679696417474 Steps: 9856/12117\n",
            "loss: 52.10474782112317 Steps: 9984/12117\n",
            "loss: 52.12304035621354 Steps: 10112/12117\n",
            "loss: 52.13052978515625 Steps: 10240/12117\n",
            "loss: 52.1164269623933 Steps: 10368/12117\n",
            "loss: 52.12521120397056 Steps: 10496/12117\n",
            "loss: 52.11985020465161 Steps: 10624/12117\n",
            "loss: 52.11657192593529 Steps: 10752/12117\n",
            "loss: 52.11709491505342 Steps: 10880/12117\n",
            "loss: 52.07813498031261 Steps: 11008/12117\n",
            "loss: 52.06892088089866 Steps: 11136/12117\n",
            "loss: 52.07375972921198 Steps: 11264/12117\n",
            "loss: 52.07708688800255 Steps: 11392/12117\n",
            "loss: 52.06608289082845 Steps: 11520/12117\n",
            "loss: 52.052314087584776 Steps: 11648/12117\n",
            "loss: 52.04681425509246 Steps: 11776/12117\n",
            "loss: 52.05782035089308 Steps: 11904/12117\n",
            "loss: 52.06699160312085 Steps: 12032/12117\n",
            "loss: 78.1372965946729 Steps: 12117/12117\n",
            "Epoch: 3  || Loss: 52.0721853595761\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "  5%|▍         | 4/81 [32:28<10:25:00, 487.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 51.74003219604492 Steps: 128/12117\n",
            "loss: 52.655019760131836 Steps: 256/12117\n",
            "loss: 52.35013326009115 Steps: 384/12117\n",
            "loss: 52.21517753601074 Steps: 512/12117\n",
            "loss: 52.21880340576172 Steps: 640/12117\n",
            "loss: 52.20457394917806 Steps: 768/12117\n",
            "loss: 52.4292003086635 Steps: 896/12117\n",
            "loss: 52.081472396850586 Steps: 1024/12117\n",
            "loss: 52.01037851969401 Steps: 1152/12117\n",
            "loss: 51.89762725830078 Steps: 1280/12117\n",
            "loss: 51.93443991921165 Steps: 1408/12117\n",
            "loss: 51.9833075205485 Steps: 1536/12117\n",
            "loss: 51.94387259850135 Steps: 1664/12117\n",
            "loss: 51.82122366768973 Steps: 1792/12117\n",
            "loss: 51.865797424316405 Steps: 1920/12117\n",
            "loss: 51.88163876533508 Steps: 2048/12117\n",
            "loss: 51.90945277494543 Steps: 2176/12117\n",
            "loss: 51.943055894639755 Steps: 2304/12117\n",
            "loss: 51.9407699986508 Steps: 2432/12117\n",
            "loss: 51.985520935058595 Steps: 2560/12117\n",
            "loss: 51.889709654308504 Steps: 2688/12117\n",
            "loss: 51.957651658491656 Steps: 2816/12117\n",
            "loss: 52.00002554188604 Steps: 2944/12117\n",
            "loss: 52.04191144307455 Steps: 3072/12117\n",
            "loss: 52.06549835205078 Steps: 3200/12117\n",
            "loss: 51.99148046053373 Steps: 3328/12117\n",
            "loss: 51.984170277913414 Steps: 3456/12117\n",
            "loss: 51.95517035893032 Steps: 3584/12117\n",
            "loss: 52.00943913953058 Steps: 3712/12117\n",
            "loss: 52.067908986409506 Steps: 3840/12117\n",
            "loss: 52.02593206590222 Steps: 3968/12117\n",
            "loss: 52.05546820163727 Steps: 4096/12117\n",
            "loss: 52.06489169958866 Steps: 4224/12117\n",
            "loss: 52.04581159703872 Steps: 4352/12117\n",
            "loss: 52.10574035644531 Steps: 4480/12117\n",
            "loss: 52.1065412097507 Steps: 4608/12117\n",
            "loss: 52.0410606796677 Steps: 4736/12117\n",
            "loss: 52.02983043068334 Steps: 4864/12117\n",
            "loss: 52.04283102964744 Steps: 4992/12117\n",
            "loss: 52.044750118255614 Steps: 5120/12117\n",
            "loss: 52.08083938970798 Steps: 5248/12117\n",
            "loss: 52.09164174397787 Steps: 5376/12117\n",
            "loss: 52.10553865654524 Steps: 5504/12117\n",
            "loss: 52.059166648171164 Steps: 5632/12117\n",
            "loss: 52.10703735351562 Steps: 5760/12117\n",
            "loss: 52.12375674040421 Steps: 5888/12117\n",
            "loss: 52.1138521559695 Steps: 6016/12117\n",
            "loss: 52.10683878262838 Steps: 6144/12117\n",
            "loss: 52.10019107740752 Steps: 6272/12117\n",
            "loss: 52.105829086303714 Steps: 6400/12117\n",
            "loss: 52.136175791422524 Steps: 6528/12117\n",
            "loss: 52.17078913175143 Steps: 6656/12117\n",
            "loss: 52.16143388568231 Steps: 6784/12117\n",
            "loss: 52.18047410470468 Steps: 6912/12117\n",
            "loss: 52.1675800670277 Steps: 7040/12117\n",
            "loss: 52.16243750708444 Steps: 7168/12117\n",
            "loss: 52.116917727286356 Steps: 7296/12117\n",
            "loss: 52.12691418877964 Steps: 7424/12117\n",
            "loss: 52.11080855030124 Steps: 7552/12117\n",
            "loss: 52.11229337056478 Steps: 7680/12117\n",
            "loss: 52.114757600377814 Steps: 7808/12117\n",
            "loss: 52.07828158717002 Steps: 7936/12117\n",
            "loss: 52.102106487940226 Steps: 8064/12117\n",
            "loss: 52.113873183727264 Steps: 8192/12117\n",
            "loss: 52.13009197528546 Steps: 8320/12117\n",
            "loss: 52.141774726636484 Steps: 8448/12117\n",
            "loss: 52.11407453622391 Steps: 8576/12117\n",
            "loss: 52.1038319643806 Steps: 8704/12117\n",
            "loss: 52.07036336263021 Steps: 8832/12117\n",
            "loss: 52.09440373011998 Steps: 8960/12117\n",
            "loss: 52.096049778898 Steps: 9088/12117\n",
            "loss: 52.07737053765191 Steps: 9216/12117\n",
            "loss: 52.07171944396136 Steps: 9344/12117\n",
            "loss: 52.03868505117055 Steps: 9472/12117\n",
            "loss: 52.01935317993164 Steps: 9600/12117\n",
            "loss: 51.9902148497732 Steps: 9728/12117\n",
            "loss: 51.975381578717915 Steps: 9856/12117\n",
            "loss: 51.98205023545485 Steps: 9984/12117\n",
            "loss: 51.98567016215264 Steps: 10112/12117\n",
            "loss: 51.98584723472595 Steps: 10240/12117\n",
            "loss: 51.985784507092134 Steps: 10368/12117\n",
            "loss: 51.99416481576315 Steps: 10496/12117\n",
            "loss: 51.99494267659015 Steps: 10624/12117\n",
            "loss: 51.98065321786063 Steps: 10752/12117\n",
            "loss: 51.98214506261489 Steps: 10880/12117\n",
            "loss: 51.98382155839787 Steps: 11008/12117\n",
            "loss: 51.99396418428969 Steps: 11136/12117\n",
            "loss: 51.995783372358844 Steps: 11264/12117\n",
            "loss: 52.01163195492177 Steps: 11392/12117\n",
            "loss: 52.01717211405436 Steps: 11520/12117\n",
            "loss: 52.007212837973796 Steps: 11648/12117\n",
            "loss: 52.009382621101715 Steps: 11776/12117\n",
            "loss: 52.01610881026073 Steps: 11904/12117\n",
            "loss: 52.01736750501267 Steps: 12032/12117\n",
            "loss: 78.05468418463833 Steps: 12117/12117\n",
            "Epoch: 4  || Loss: 52.017130873232205\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "  6%|▌         | 5/81 [40:32<10:15:36, 486.00s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 50.836219787597656 Steps: 128/12117\n",
            "loss: 52.1861572265625 Steps: 256/12117\n",
            "loss: 51.914849599202476 Steps: 384/12117\n",
            "loss: 52.08772277832031 Steps: 512/12117\n",
            "loss: 52.170336151123045 Steps: 640/12117\n",
            "loss: 52.12913386027018 Steps: 768/12117\n",
            "loss: 52.07915987287249 Steps: 896/12117\n",
            "loss: 51.98917865753174 Steps: 1024/12117\n",
            "loss: 51.91928566826714 Steps: 1152/12117\n",
            "loss: 51.957702255249025 Steps: 1280/12117\n",
            "loss: 51.82716993852095 Steps: 1408/12117\n",
            "loss: 51.859965006510414 Steps: 1536/12117\n",
            "loss: 51.85684409508338 Steps: 1664/12117\n",
            "loss: 51.7920297895159 Steps: 1792/12117\n",
            "loss: 51.75747248331706 Steps: 1920/12117\n",
            "loss: 51.729602336883545 Steps: 2048/12117\n",
            "loss: 51.799836551441864 Steps: 2176/12117\n",
            "loss: 51.78532579210069 Steps: 2304/12117\n",
            "loss: 51.82591769569799 Steps: 2432/12117\n",
            "loss: 51.78895263671875 Steps: 2560/12117\n",
            "loss: 51.79537945701962 Steps: 2688/12117\n",
            "loss: 51.89160641756925 Steps: 2816/12117\n",
            "loss: 51.892423215119734 Steps: 2944/12117\n",
            "loss: 51.936063289642334 Steps: 3072/12117\n",
            "loss: 51.9786410522461 Steps: 3200/12117\n",
            "loss: 52.005963398860054 Steps: 3328/12117\n",
            "loss: 51.994745325159144 Steps: 3456/12117\n",
            "loss: 52.02483994620187 Steps: 3584/12117\n",
            "loss: 51.98255065391804 Steps: 3712/12117\n",
            "loss: 52.016835657755536 Steps: 3840/12117\n",
            "loss: 51.992232291929184 Steps: 3968/12117\n",
            "loss: 51.9641752243042 Steps: 4096/12117\n",
            "loss: 51.92969316424745 Steps: 4224/12117\n",
            "loss: 51.9561076444738 Steps: 4352/12117\n",
            "loss: 51.976409258161276 Steps: 4480/12117\n",
            "loss: 51.96547794342041 Steps: 4608/12117\n",
            "loss: 51.943382469383444 Steps: 4736/12117\n",
            "loss: 51.94002914428711 Steps: 4864/12117\n",
            "loss: 51.92042844723432 Steps: 4992/12117\n",
            "loss: 51.947483444213866 Steps: 5120/12117\n",
            "loss: 51.97914421267626 Steps: 5248/12117\n",
            "loss: 51.991955257597425 Steps: 5376/12117\n",
            "loss: 52.0048028812852 Steps: 5504/12117\n",
            "loss: 51.9816178408536 Steps: 5632/12117\n",
            "loss: 51.98841264512804 Steps: 5760/12117\n",
            "loss: 51.95861575914466 Steps: 5888/12117\n",
            "loss: 51.916333218838304 Steps: 6016/12117\n",
            "loss: 51.93985255559286 Steps: 6144/12117\n",
            "loss: 51.92124456288863 Steps: 6272/12117\n",
            "loss: 51.918793182373044 Steps: 6400/12117\n",
            "loss: 51.92040716433058 Steps: 6528/12117\n",
            "loss: 51.908914639399605 Steps: 6656/12117\n",
            "loss: 51.90492097386774 Steps: 6784/12117\n",
            "loss: 51.91977091188784 Steps: 6912/12117\n",
            "loss: 51.91907771717418 Steps: 7040/12117\n",
            "loss: 51.92903103147234 Steps: 7168/12117\n",
            "loss: 51.929128077992225 Steps: 7296/12117\n",
            "loss: 51.934544333096206 Steps: 7424/12117\n",
            "loss: 51.952933683233745 Steps: 7552/12117\n",
            "loss: 51.940175183614095 Steps: 7680/12117\n",
            "loss: 51.93423980963035 Steps: 7808/12117\n",
            "loss: 51.934861767676566 Steps: 7936/12117\n",
            "loss: 51.92429939148918 Steps: 8064/12117\n",
            "loss: 51.918802320957184 Steps: 8192/12117\n",
            "loss: 51.90613508958083 Steps: 8320/12117\n",
            "loss: 51.88861101323908 Steps: 8448/12117\n",
            "loss: 51.880351906392114 Steps: 8576/12117\n",
            "loss: 51.877105208004224 Steps: 8704/12117\n",
            "loss: 51.87224390886832 Steps: 8832/12117\n",
            "loss: 51.870117568969725 Steps: 8960/12117\n",
            "loss: 51.90324906899895 Steps: 9088/12117\n",
            "loss: 51.886249171362984 Steps: 9216/12117\n",
            "loss: 51.88729299257879 Steps: 9344/12117\n",
            "loss: 51.88587539260452 Steps: 9472/12117\n",
            "loss: 51.88651178995768 Steps: 9600/12117\n",
            "loss: 51.90740028180574 Steps: 9728/12117\n",
            "loss: 51.90991111854454 Steps: 9856/12117\n",
            "loss: 51.92382729359162 Steps: 9984/12117\n",
            "loss: 51.92433828040014 Steps: 10112/12117\n",
            "loss: 51.920686101913454 Steps: 10240/12117\n",
            "loss: 51.93007984867803 Steps: 10368/12117\n",
            "loss: 51.93348363550698 Steps: 10496/12117\n",
            "loss: 51.945350233330785 Steps: 10624/12117\n",
            "loss: 51.95611367906843 Steps: 10752/12117\n",
            "loss: 51.94676693187041 Steps: 10880/12117\n",
            "loss: 51.93825659641 Steps: 11008/12117\n",
            "loss: 51.940613187592604 Steps: 11136/12117\n",
            "loss: 51.949392795562744 Steps: 11264/12117\n",
            "loss: 51.93671935863709 Steps: 11392/12117\n",
            "loss: 51.94303355746799 Steps: 11520/12117\n",
            "loss: 51.93960902455089 Steps: 11648/12117\n",
            "loss: 51.936346219933554 Steps: 11776/12117\n",
            "loss: 51.94045831823862 Steps: 11904/12117\n",
            "loss: 51.92710283969311 Steps: 12032/12117\n",
            "loss: 77.93697315227875 Steps: 12117/12117\n",
            "Epoch: 5  || Loss: 51.93868599526705\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "  7%|▋         | 6/81 [48:28<10:03:43, 482.98s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 52.76036834716797 Steps: 128/12117\n",
            "loss: 52.71342086791992 Steps: 256/12117\n",
            "loss: 52.69907887776693 Steps: 384/12117\n",
            "loss: 52.514933586120605 Steps: 512/12117\n",
            "loss: 52.36627731323242 Steps: 640/12117\n",
            "loss: 52.31885655721029 Steps: 768/12117\n",
            "loss: 52.2585563659668 Steps: 896/12117\n",
            "loss: 52.161943435668945 Steps: 1024/12117\n",
            "loss: 52.1830088297526 Steps: 1152/12117\n",
            "loss: 52.110459899902345 Steps: 1280/12117\n",
            "loss: 52.034855929288 Steps: 1408/12117\n",
            "loss: 51.99946562449137 Steps: 1536/12117\n",
            "loss: 52.0326546889085 Steps: 1664/12117\n",
            "loss: 51.93216242109026 Steps: 1792/12117\n",
            "loss: 51.978477223714194 Steps: 1920/12117\n",
            "loss: 52.019325971603394 Steps: 2048/12117\n",
            "loss: 51.89380802827723 Steps: 2176/12117\n",
            "loss: 51.88898849487305 Steps: 2304/12117\n",
            "loss: 51.901084096808184 Steps: 2432/12117\n",
            "loss: 51.8877513885498 Steps: 2560/12117\n",
            "loss: 51.918668292817614 Steps: 2688/12117\n",
            "loss: 51.9062425440008 Steps: 2816/12117\n",
            "loss: 51.868159584377125 Steps: 2944/12117\n",
            "loss: 51.86474212010702 Steps: 3072/12117\n",
            "loss: 51.89370590209961 Steps: 3200/12117\n",
            "loss: 51.93697547912598 Steps: 3328/12117\n",
            "loss: 51.95411484329789 Steps: 3456/12117\n",
            "loss: 51.94994803837368 Steps: 3584/12117\n",
            "loss: 51.908312961972996 Steps: 3712/12117\n",
            "loss: 51.919432703653975 Steps: 3840/12117\n",
            "loss: 51.965215375346524 Steps: 3968/12117\n",
            "loss: 51.95113265514374 Steps: 4096/12117\n",
            "loss: 51.94750965002811 Steps: 4224/12117\n",
            "loss: 51.9239865471335 Steps: 4352/12117\n",
            "loss: 51.91517835344587 Steps: 4480/12117\n",
            "loss: 51.89468320210775 Steps: 4608/12117\n",
            "loss: 51.918803034601986 Steps: 4736/12117\n",
            "loss: 51.91512348777369 Steps: 4864/12117\n",
            "loss: 51.91106962546324 Steps: 4992/12117\n",
            "loss: 51.88817663192749 Steps: 5120/12117\n",
            "loss: 51.9043281369093 Steps: 5248/12117\n",
            "loss: 51.92126074291411 Steps: 5376/12117\n",
            "loss: 51.89837211786315 Steps: 5504/12117\n",
            "loss: 51.90731698816473 Steps: 5632/12117\n",
            "loss: 51.9197398715549 Steps: 5760/12117\n",
            "loss: 51.893460646919586 Steps: 5888/12117\n",
            "loss: 51.88484605829766 Steps: 6016/12117\n",
            "loss: 51.87665605545044 Steps: 6144/12117\n",
            "loss: 51.91907088610591 Steps: 6272/12117\n",
            "loss: 51.921371688842775 Steps: 6400/12117\n",
            "loss: 51.91870251823874 Steps: 6528/12117\n",
            "loss: 51.95011285635141 Steps: 6656/12117\n",
            "loss: 51.93724858985757 Steps: 6784/12117\n",
            "loss: 51.934285481770836 Steps: 6912/12117\n",
            "loss: 51.90210918079723 Steps: 7040/12117\n",
            "loss: 51.90332242420742 Steps: 7168/12117\n",
            "loss: 51.88790090460526 Steps: 7296/12117\n",
            "loss: 51.874961918797986 Steps: 7424/12117\n",
            "loss: 51.88189302864721 Steps: 7552/12117\n",
            "loss: 51.912163798014326 Steps: 7680/12117\n",
            "loss: 51.89607132458296 Steps: 7808/12117\n",
            "loss: 51.87876732118668 Steps: 7936/12117\n",
            "loss: 51.87309446788969 Steps: 8064/12117\n",
            "loss: 51.88469451665878 Steps: 8192/12117\n",
            "loss: 51.864889408991885 Steps: 8320/12117\n",
            "loss: 51.87705583283395 Steps: 8448/12117\n",
            "loss: 51.85042634650843 Steps: 8576/12117\n",
            "loss: 51.85235449847053 Steps: 8704/12117\n",
            "loss: 51.86675074480582 Steps: 8832/12117\n",
            "loss: 51.869048309326175 Steps: 8960/12117\n",
            "loss: 51.86181033497125 Steps: 9088/12117\n",
            "loss: 51.8644191953871 Steps: 9216/12117\n",
            "loss: 51.85999961748515 Steps: 9344/12117\n",
            "loss: 51.850635580114414 Steps: 9472/12117\n",
            "loss: 51.86236078898112 Steps: 9600/12117\n",
            "loss: 51.866551098070644 Steps: 9728/12117\n",
            "loss: 51.85782316133574 Steps: 9856/12117\n",
            "loss: 51.8390882443159 Steps: 9984/12117\n",
            "loss: 51.86975295634209 Steps: 10112/12117\n",
            "loss: 51.85788869857788 Steps: 10240/12117\n",
            "loss: 51.852151187849636 Steps: 10368/12117\n",
            "loss: 51.85766168920005 Steps: 10496/12117\n",
            "loss: 51.8673176133489 Steps: 10624/12117\n",
            "loss: 51.87774830772763 Steps: 10752/12117\n",
            "loss: 51.854681351605585 Steps: 10880/12117\n",
            "loss: 51.86359046226324 Steps: 11008/12117\n",
            "loss: 51.84973271687826 Steps: 11136/12117\n",
            "loss: 51.842412428422406 Steps: 11264/12117\n",
            "loss: 51.8321182165253 Steps: 11392/12117\n",
            "loss: 51.848547829522026 Steps: 11520/12117\n",
            "loss: 51.84602133782355 Steps: 11648/12117\n",
            "loss: 51.8440754931906 Steps: 11776/12117\n",
            "loss: 51.84322869905861 Steps: 11904/12117\n",
            "loss: 51.864641676557824 Steps: 12032/12117\n",
            "loss: 77.81713759950809 Steps: 12117/12117\n",
            "Epoch: 6  || Loss: 51.85882529636278\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "  9%|▊         | 7/81 [56:29<9:55:06, 482.52s/it] \u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 53.824195861816406 Steps: 128/12117\n",
            "loss: 52.62174606323242 Steps: 256/12117\n",
            "loss: 52.434775034586586 Steps: 384/12117\n",
            "loss: 52.035672187805176 Steps: 512/12117\n",
            "loss: 51.70462646484375 Steps: 640/12117\n",
            "loss: 51.80950482686361 Steps: 768/12117\n",
            "loss: 51.936985560825896 Steps: 896/12117\n",
            "loss: 51.97276258468628 Steps: 1024/12117\n",
            "loss: 51.986342112223305 Steps: 1152/12117\n",
            "loss: 51.997283172607425 Steps: 1280/12117\n",
            "loss: 51.95726117220792 Steps: 1408/12117\n",
            "loss: 51.923475901285805 Steps: 1536/12117\n",
            "loss: 51.84786840585562 Steps: 1664/12117\n",
            "loss: 51.924382073538645 Steps: 1792/12117\n",
            "loss: 52.03919041951497 Steps: 1920/12117\n",
            "loss: 51.965879917144775 Steps: 2048/12117\n",
            "loss: 51.90487693337833 Steps: 2176/12117\n",
            "loss: 51.91742706298828 Steps: 2304/12117\n",
            "loss: 51.98566818237305 Steps: 2432/12117\n",
            "loss: 52.014881706237794 Steps: 2560/12117\n",
            "loss: 52.0424550374349 Steps: 2688/12117\n",
            "loss: 51.94029478593306 Steps: 2816/12117\n",
            "loss: 51.94475240292756 Steps: 2944/12117\n",
            "loss: 51.99548705418905 Steps: 3072/12117\n",
            "loss: 51.93814468383789 Steps: 3200/12117\n",
            "loss: 52.010947740994965 Steps: 3328/12117\n",
            "loss: 52.00103915179217 Steps: 3456/12117\n",
            "loss: 51.921570369175505 Steps: 3584/12117\n",
            "loss: 51.941859146644326 Steps: 3712/12117\n",
            "loss: 51.92773679097493 Steps: 3840/12117\n",
            "loss: 51.98207732169859 Steps: 3968/12117\n",
            "loss: 51.94412612915039 Steps: 4096/12117\n",
            "loss: 51.94177904996005 Steps: 4224/12117\n",
            "loss: 51.95201671824736 Steps: 4352/12117\n",
            "loss: 51.901394326346264 Steps: 4480/12117\n",
            "loss: 51.95505290561252 Steps: 4608/12117\n",
            "loss: 51.89422957961624 Steps: 4736/12117\n",
            "loss: 51.89604448017321 Steps: 4864/12117\n",
            "loss: 51.92410923884465 Steps: 4992/12117\n",
            "loss: 51.9225793838501 Steps: 5120/12117\n",
            "loss: 51.87382916706364 Steps: 5248/12117\n",
            "loss: 51.84481875101725 Steps: 5376/12117\n",
            "loss: 51.8788763534191 Steps: 5504/12117\n",
            "loss: 51.86634974046187 Steps: 5632/12117\n",
            "loss: 51.865321180555554 Steps: 5760/12117\n",
            "loss: 51.87477518164593 Steps: 5888/12117\n",
            "loss: 51.82567150034803 Steps: 6016/12117\n",
            "loss: 51.78069043159485 Steps: 6144/12117\n",
            "loss: 51.76410379215162 Steps: 6272/12117\n",
            "loss: 51.75262710571289 Steps: 6400/12117\n",
            "loss: 51.72636017144895 Steps: 6528/12117\n",
            "loss: 51.7723743732159 Steps: 6656/12117\n",
            "loss: 51.74657476173257 Steps: 6784/12117\n",
            "loss: 51.764339164451314 Steps: 6912/12117\n",
            "loss: 51.751930167458276 Steps: 7040/12117\n",
            "loss: 51.74472638538906 Steps: 7168/12117\n",
            "loss: 51.75085576375326 Steps: 7296/12117\n",
            "loss: 51.76351330198091 Steps: 7424/12117\n",
            "loss: 51.770034919350834 Steps: 7552/12117\n",
            "loss: 51.779957135518394 Steps: 7680/12117\n",
            "loss: 51.77060311739562 Steps: 7808/12117\n",
            "loss: 51.769844116703155 Steps: 7936/12117\n",
            "loss: 51.77333753071134 Steps: 8064/12117\n",
            "loss: 51.78114491701126 Steps: 8192/12117\n",
            "loss: 51.7739498431866 Steps: 8320/12117\n",
            "loss: 51.79591167334354 Steps: 8448/12117\n",
            "loss: 51.77448289785812 Steps: 8576/12117\n",
            "loss: 51.754797879387354 Steps: 8704/12117\n",
            "loss: 51.75655702231587 Steps: 8832/12117\n",
            "loss: 51.753245162963864 Steps: 8960/12117\n",
            "loss: 51.7533300695285 Steps: 9088/12117\n",
            "loss: 51.74790700276693 Steps: 9216/12117\n",
            "loss: 51.75633255422932 Steps: 9344/12117\n",
            "loss: 51.775285669275235 Steps: 9472/12117\n",
            "loss: 51.778584238688154 Steps: 9600/12117\n",
            "loss: 51.77134132385254 Steps: 9728/12117\n",
            "loss: 51.784157740605345 Steps: 9856/12117\n",
            "loss: 51.781555420313126 Steps: 9984/12117\n",
            "loss: 51.78412666803674 Steps: 10112/12117\n",
            "loss: 51.781056022644044 Steps: 10240/12117\n",
            "loss: 51.75575562465338 Steps: 10368/12117\n",
            "loss: 51.78012396649616 Steps: 10496/12117\n",
            "loss: 51.79057298223656 Steps: 10624/12117\n",
            "loss: 51.806426184517996 Steps: 10752/12117\n",
            "loss: 51.814599205465875 Steps: 10880/12117\n",
            "loss: 51.81535867203114 Steps: 11008/12117\n",
            "loss: 51.803480696404115 Steps: 11136/12117\n",
            "loss: 51.801443490115076 Steps: 11264/12117\n",
            "loss: 51.79991351352649 Steps: 11392/12117\n",
            "loss: 51.80789438883463 Steps: 11520/12117\n",
            "loss: 51.79148126958491 Steps: 11648/12117\n",
            "loss: 51.78122848013173 Steps: 11776/12117\n",
            "loss: 51.779588760868194 Steps: 11904/12117\n",
            "loss: 51.777690359886655 Steps: 12032/12117\n",
            "loss: 77.68921354925669 Steps: 12117/12117\n",
            "Epoch: 7  || Loss: 51.77357426840371\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            " 10%|▉         | 8/81 [1:04:32<9:47:11, 482.63s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 51.504249572753906 Steps: 128/12117\n",
            "loss: 51.16329765319824 Steps: 256/12117\n",
            "loss: 51.28325653076172 Steps: 384/12117\n",
            "loss: 51.637776374816895 Steps: 512/12117\n",
            "loss: 51.8333122253418 Steps: 640/12117\n",
            "loss: 51.84883054097494 Steps: 768/12117\n",
            "loss: 52.051818302699495 Steps: 896/12117\n",
            "loss: 52.066895961761475 Steps: 1024/12117\n",
            "loss: 52.279791514078774 Steps: 1152/12117\n",
            "loss: 52.05746307373047 Steps: 1280/12117\n",
            "loss: 51.965669111772016 Steps: 1408/12117\n",
            "loss: 51.98754851023356 Steps: 1536/12117\n",
            "loss: 52.0013914841872 Steps: 1664/12117\n",
            "loss: 51.9475337437221 Steps: 1792/12117\n",
            "loss: 51.9049430847168 Steps: 1920/12117\n",
            "loss: 51.86249828338623 Steps: 2048/12117\n",
            "loss: 51.91653801413143 Steps: 2176/12117\n",
            "loss: 51.86420716179742 Steps: 2304/12117\n",
            "loss: 51.826753315172695 Steps: 2432/12117\n",
            "loss: 51.813466453552245 Steps: 2560/12117\n",
            "loss: 51.758041018531436 Steps: 2688/12117\n",
            "loss: 51.756370544433594 Steps: 2816/12117\n",
            "loss: 51.74163055419922 Steps: 2944/12117\n",
            "loss: 51.68400605519613 Steps: 3072/12117\n",
            "loss: 51.701315002441405 Steps: 3200/12117\n",
            "loss: 51.63909912109375 Steps: 3328/12117\n",
            "loss: 51.69429764924226 Steps: 3456/12117\n",
            "loss: 51.65003408704485 Steps: 3584/12117\n",
            "loss: 51.65729483242693 Steps: 3712/12117\n",
            "loss: 51.67423718770345 Steps: 3840/12117\n",
            "loss: 51.64760700348885 Steps: 3968/12117\n",
            "loss: 51.65214657783508 Steps: 4096/12117\n",
            "loss: 51.61763786547112 Steps: 4224/12117\n",
            "loss: 51.60731685862822 Steps: 4352/12117\n",
            "loss: 51.594681767054965 Steps: 4480/12117\n",
            "loss: 51.629514906141495 Steps: 4608/12117\n",
            "loss: 51.66257899516338 Steps: 4736/12117\n",
            "loss: 51.70958097357499 Steps: 4864/12117\n",
            "loss: 51.724527701353416 Steps: 4992/12117\n",
            "loss: 51.70006151199341 Steps: 5120/12117\n",
            "loss: 51.693621844780154 Steps: 5248/12117\n",
            "loss: 51.734937486194426 Steps: 5376/12117\n",
            "loss: 51.71233013064362 Steps: 5504/12117\n",
            "loss: 51.72375609657981 Steps: 5632/12117\n",
            "loss: 51.72296719021267 Steps: 5760/12117\n",
            "loss: 51.68582758696183 Steps: 5888/12117\n",
            "loss: 51.670652511272024 Steps: 6016/12117\n",
            "loss: 51.68112587928772 Steps: 6144/12117\n",
            "loss: 51.68695286342076 Steps: 6272/12117\n",
            "loss: 51.68924217224121 Steps: 6400/12117\n",
            "loss: 51.71502797743853 Steps: 6528/12117\n",
            "loss: 51.74873065948486 Steps: 6656/12117\n",
            "loss: 51.79520999260669 Steps: 6784/12117\n",
            "loss: 51.79913061636466 Steps: 6912/12117\n",
            "loss: 51.829491285844284 Steps: 7040/12117\n",
            "loss: 51.80119902747018 Steps: 7168/12117\n",
            "loss: 51.80142961468613 Steps: 7296/12117\n",
            "loss: 51.79080390930176 Steps: 7424/12117\n",
            "loss: 51.78521870758574 Steps: 7552/12117\n",
            "loss: 51.77661959330241 Steps: 7680/12117\n",
            "loss: 51.79314791569944 Steps: 7808/12117\n",
            "loss: 51.768676880867254 Steps: 7936/12117\n",
            "loss: 51.7714725676037 Steps: 8064/12117\n",
            "loss: 51.76490104198456 Steps: 8192/12117\n",
            "loss: 51.77031731238732 Steps: 8320/12117\n",
            "loss: 51.787881504405625 Steps: 8448/12117\n",
            "loss: 51.780806584144706 Steps: 8576/12117\n",
            "loss: 51.78778283736285 Steps: 8704/12117\n",
            "loss: 51.784569892330445 Steps: 8832/12117\n",
            "loss: 51.77914908272879 Steps: 8960/12117\n",
            "loss: 51.79374501402949 Steps: 9088/12117\n",
            "loss: 51.80627123514811 Steps: 9216/12117\n",
            "loss: 51.784954175557175 Steps: 9344/12117\n",
            "loss: 51.77160680616224 Steps: 9472/12117\n",
            "loss: 51.76434117635091 Steps: 9600/12117\n",
            "loss: 51.76157931277626 Steps: 9728/12117\n",
            "loss: 51.75882938929966 Steps: 9856/12117\n",
            "loss: 51.76318124624399 Steps: 9984/12117\n",
            "loss: 51.73438098762609 Steps: 10112/12117\n",
            "loss: 51.73714327812195 Steps: 10240/12117\n",
            "loss: 51.73976262410482 Steps: 10368/12117\n",
            "loss: 51.728879370340486 Steps: 10496/12117\n",
            "loss: 51.70908530361681 Steps: 10624/12117\n",
            "loss: 51.70098082224528 Steps: 10752/12117\n",
            "loss: 51.69437264835133 Steps: 10880/12117\n",
            "loss: 51.67877552121185 Steps: 11008/12117\n",
            "loss: 51.69392127552252 Steps: 11136/12117\n",
            "loss: 51.69108741933649 Steps: 11264/12117\n",
            "loss: 51.692826131756384 Steps: 11392/12117\n",
            "loss: 51.68492245144314 Steps: 11520/12117\n",
            "loss: 51.681768773676275 Steps: 11648/12117\n",
            "loss: 51.6962311371513 Steps: 11776/12117\n",
            "loss: 51.6930065770303 Steps: 11904/12117\n",
            "loss: 51.69318909340716 Steps: 12032/12117\n",
            "loss: 77.55262759182106 Steps: 12117/12117\n",
            "Epoch: 8  || Loss: 51.68255078022242\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            " 11%|█         | 9/81 [1:12:36<9:39:43, 483.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 49.60694122314453 Steps: 128/12117\n",
            "loss: 51.082162857055664 Steps: 256/12117\n",
            "loss: 51.10134251912435 Steps: 384/12117\n",
            "loss: 51.37177753448486 Steps: 512/12117\n",
            "loss: 51.65912780761719 Steps: 640/12117\n",
            "loss: 51.46009063720703 Steps: 768/12117\n",
            "loss: 51.38517052786691 Steps: 896/12117\n",
            "loss: 51.53377389907837 Steps: 1024/12117\n",
            "loss: 51.59445317586263 Steps: 1152/12117\n",
            "loss: 51.480889892578126 Steps: 1280/12117\n",
            "loss: 51.29318063909357 Steps: 1408/12117\n",
            "loss: 51.36756579081217 Steps: 1536/12117\n",
            "loss: 51.393392709585335 Steps: 1664/12117\n",
            "loss: 51.365038190569194 Steps: 1792/12117\n",
            "loss: 51.39948705037435 Steps: 1920/12117\n",
            "loss: 51.43812417984009 Steps: 2048/12117\n",
            "loss: 51.48330060173483 Steps: 2176/12117\n",
            "loss: 51.53184403313531 Steps: 2304/12117\n",
            "loss: 51.57328475149054 Steps: 2432/12117\n",
            "loss: 51.57926712036133 Steps: 2560/12117\n",
            "loss: 51.59624190557571 Steps: 2688/12117\n",
            "loss: 51.565453095869586 Steps: 2816/12117\n",
            "loss: 51.57006172511888 Steps: 2944/12117\n",
            "loss: 51.651940981547035 Steps: 3072/12117\n",
            "loss: 51.72444381713867 Steps: 3200/12117\n",
            "loss: 51.74489666865422 Steps: 3328/12117\n",
            "loss: 51.77415649979203 Steps: 3456/12117\n",
            "loss: 51.72688375200544 Steps: 3584/12117\n",
            "loss: 51.715196806809 Steps: 3712/12117\n",
            "loss: 51.6877991994222 Steps: 3840/12117\n",
            "loss: 51.65789573423324 Steps: 3968/12117\n",
            "loss: 51.66583013534546 Steps: 4096/12117\n",
            "loss: 51.62745735862038 Steps: 4224/12117\n",
            "loss: 51.653166378245636 Steps: 4352/12117\n",
            "loss: 51.63206852504185 Steps: 4480/12117\n",
            "loss: 51.63793585035536 Steps: 4608/12117\n",
            "loss: 51.65915411871833 Steps: 4736/12117\n",
            "loss: 51.679204539248815 Steps: 4864/12117\n",
            "loss: 51.68566571749174 Steps: 4992/12117\n",
            "loss: 51.67127542495727 Steps: 5120/12117\n",
            "loss: 51.687582434677495 Steps: 5248/12117\n",
            "loss: 51.69103186471121 Steps: 5376/12117\n",
            "loss: 51.700087879979336 Steps: 5504/12117\n",
            "loss: 51.70552349090576 Steps: 5632/12117\n",
            "loss: 51.68035447862413 Steps: 5760/12117\n",
            "loss: 51.671857833862305 Steps: 5888/12117\n",
            "loss: 51.6379471636833 Steps: 6016/12117\n",
            "loss: 51.61213151613871 Steps: 6144/12117\n",
            "loss: 51.62848896882972 Steps: 6272/12117\n",
            "loss: 51.62039276123047 Steps: 6400/12117\n",
            "loss: 51.64142937753715 Steps: 6528/12117\n",
            "loss: 51.65257138472337 Steps: 6656/12117\n",
            "loss: 51.655384567548644 Steps: 6784/12117\n",
            "loss: 51.66541127805357 Steps: 6912/12117\n",
            "loss: 51.65901080044833 Steps: 7040/12117\n",
            "loss: 51.629907267434255 Steps: 7168/12117\n",
            "loss: 51.661254615114444 Steps: 7296/12117\n",
            "loss: 51.6666312382139 Steps: 7424/12117\n",
            "loss: 51.657361887269104 Steps: 7552/12117\n",
            "loss: 51.66752077738444 Steps: 7680/12117\n",
            "loss: 51.67927419943888 Steps: 7808/12117\n",
            "loss: 51.67810612340127 Steps: 7936/12117\n",
            "loss: 51.69269791860429 Steps: 8064/12117\n",
            "loss: 51.67856800556183 Steps: 8192/12117\n",
            "loss: 51.66763986440805 Steps: 8320/12117\n",
            "loss: 51.65319991834236 Steps: 8448/12117\n",
            "loss: 51.67366335285244 Steps: 8576/12117\n",
            "loss: 51.66937631719253 Steps: 8704/12117\n",
            "loss: 51.668458468672156 Steps: 8832/12117\n",
            "loss: 51.67037108285086 Steps: 8960/12117\n",
            "loss: 51.660113267495596 Steps: 9088/12117\n",
            "loss: 51.644385708702934 Steps: 9216/12117\n",
            "loss: 51.651451528888856 Steps: 9344/12117\n",
            "loss: 51.66483791454418 Steps: 9472/12117\n",
            "loss: 51.67666463216146 Steps: 9600/12117\n",
            "loss: 51.676197252775495 Steps: 9728/12117\n",
            "loss: 51.65948456603211 Steps: 9856/12117\n",
            "loss: 51.671706028473686 Steps: 9984/12117\n",
            "loss: 51.65695446352415 Steps: 10112/12117\n",
            "loss: 51.64431729316711 Steps: 10240/12117\n",
            "loss: 51.62619644918559 Steps: 10368/12117\n",
            "loss: 51.61245369329685 Steps: 10496/12117\n",
            "loss: 51.604924397296216 Steps: 10624/12117\n",
            "loss: 51.61159896850586 Steps: 10752/12117\n",
            "loss: 51.6035078160903 Steps: 10880/12117\n",
            "loss: 51.59682220636412 Steps: 11008/12117\n",
            "loss: 51.60555227871599 Steps: 11136/12117\n",
            "loss: 51.58570198579268 Steps: 11264/12117\n",
            "loss: 51.60166806853219 Steps: 11392/12117\n",
            "loss: 51.628726408216686 Steps: 11520/12117\n",
            "loss: 51.6224618429666 Steps: 11648/12117\n",
            "loss: 51.633033171944 Steps: 11776/12117\n",
            "loss: 51.6194225434334 Steps: 11904/12117\n",
            "loss: 51.636047566190676 Steps: 12032/12117\n",
            "loss: 77.48121669285068 Steps: 12117/12117\n",
            "Epoch: 9  || Loss: 51.63496119458358\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            " 12%|█▏        | 10/81 [1:20:38<9:31:01, 482.56s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 51.880126953125 Steps: 128/12117\n",
            "loss: 51.24163818359375 Steps: 256/12117\n",
            "loss: 51.51727040608724 Steps: 384/12117\n",
            "loss: 51.63966369628906 Steps: 512/12117\n",
            "loss: 51.81215591430664 Steps: 640/12117\n",
            "loss: 51.70450528462728 Steps: 768/12117\n",
            "loss: 51.62888717651367 Steps: 896/12117\n",
            "loss: 51.702274322509766 Steps: 1024/12117\n",
            "loss: 51.781005435519745 Steps: 1152/12117\n",
            "loss: 51.86202964782715 Steps: 1280/12117\n",
            "loss: 51.78478934548118 Steps: 1408/12117\n",
            "loss: 51.72606182098389 Steps: 1536/12117\n",
            "loss: 51.75817108154297 Steps: 1664/12117\n",
            "loss: 51.82077734810965 Steps: 1792/12117\n",
            "loss: 51.76961873372396 Steps: 1920/12117\n",
            "loss: 51.785719871520996 Steps: 2048/12117\n",
            "loss: 51.638214111328125 Steps: 2176/12117\n",
            "loss: 51.67182328965929 Steps: 2304/12117\n",
            "loss: 51.682625218441615 Steps: 2432/12117\n",
            "loss: 51.62543773651123 Steps: 2560/12117\n",
            "loss: 51.639442443847656 Steps: 2688/12117\n",
            "loss: 51.56723629344594 Steps: 2816/12117\n",
            "loss: 51.58198132722274 Steps: 2944/12117\n",
            "loss: 51.572380701700844 Steps: 3072/12117\n",
            "loss: 51.53146026611328 Steps: 3200/12117\n",
            "loss: 51.600283696101265 Steps: 3328/12117\n",
            "loss: 51.5546954119647 Steps: 3456/12117\n",
            "loss: 51.52457332611084 Steps: 3584/12117\n",
            "loss: 51.53443316755624 Steps: 3712/12117\n",
            "loss: 51.53511695861816 Steps: 3840/12117\n",
            "loss: 51.54365650300057 Steps: 3968/12117\n",
            "loss: 51.542171597480774 Steps: 4096/12117\n",
            "loss: 51.524498679421164 Steps: 4224/12117\n",
            "loss: 51.53006261937759 Steps: 4352/12117\n",
            "loss: 51.54573527744838 Steps: 4480/12117\n",
            "loss: 51.55054569244385 Steps: 4608/12117\n",
            "loss: 51.56649481283652 Steps: 4736/12117\n",
            "loss: 51.54116771095678 Steps: 4864/12117\n",
            "loss: 51.52914487398588 Steps: 4992/12117\n",
            "loss: 51.517097282409665 Steps: 5120/12117\n",
            "loss: 51.5537023776915 Steps: 5248/12117\n",
            "loss: 51.531835465204146 Steps: 5376/12117\n",
            "loss: 51.58369028845499 Steps: 5504/12117\n",
            "loss: 51.579008709300645 Steps: 5632/12117\n",
            "loss: 51.57824223836263 Steps: 5760/12117\n",
            "loss: 51.59281714066215 Steps: 5888/12117\n",
            "loss: 51.61850194728121 Steps: 6016/12117\n",
            "loss: 51.58728647232056 Steps: 6144/12117\n",
            "loss: 51.62084766310088 Steps: 6272/12117\n",
            "loss: 51.616355438232425 Steps: 6400/12117\n",
            "loss: 51.639448951272406 Steps: 6528/12117\n",
            "loss: 51.65152828509991 Steps: 6656/12117\n",
            "loss: 51.64235989552624 Steps: 6784/12117\n",
            "loss: 51.63563565854673 Steps: 6912/12117\n",
            "loss: 51.63856041648171 Steps: 7040/12117\n",
            "loss: 51.677194118499756 Steps: 7168/12117\n",
            "loss: 51.658780215079325 Steps: 7296/12117\n",
            "loss: 51.652858602589575 Steps: 7424/12117\n",
            "loss: 51.644365666276315 Steps: 7552/12117\n",
            "loss: 51.64010842641195 Steps: 7680/12117\n",
            "loss: 51.64920181524558 Steps: 7808/12117\n",
            "loss: 51.622322820848034 Steps: 7936/12117\n",
            "loss: 51.610155862475196 Steps: 8064/12117\n",
            "loss: 51.61164039373398 Steps: 8192/12117\n",
            "loss: 51.62024870652419 Steps: 8320/12117\n",
            "loss: 51.63684203407981 Steps: 8448/12117\n",
            "loss: 51.62659801653962 Steps: 8576/12117\n",
            "loss: 51.623728303348315 Steps: 8704/12117\n",
            "loss: 51.62749110788539 Steps: 8832/12117\n",
            "loss: 51.60506308419364 Steps: 8960/12117\n",
            "loss: 51.59889097616706 Steps: 9088/12117\n",
            "loss: 51.60058148701986 Steps: 9216/12117\n",
            "loss: 51.60117836521096 Steps: 9344/12117\n",
            "loss: 51.62070320747994 Steps: 9472/12117\n",
            "loss: 51.635270029703776 Steps: 9600/12117\n",
            "loss: 51.643609850030195 Steps: 9728/12117\n",
            "loss: 51.63864190237863 Steps: 9856/12117\n",
            "loss: 51.596472324469154 Steps: 9984/12117\n",
            "loss: 51.59239626534377 Steps: 10112/12117\n",
            "loss: 51.56927461624146 Steps: 10240/12117\n",
            "loss: 51.56240679893965 Steps: 10368/12117\n",
            "loss: 51.55703195711462 Steps: 10496/12117\n",
            "loss: 51.54014670130718 Steps: 10624/12117\n",
            "loss: 51.524839037940616 Steps: 10752/12117\n",
            "loss: 51.524763982436234 Steps: 10880/12117\n",
            "loss: 51.523241264875544 Steps: 11008/12117\n",
            "loss: 51.52203930383441 Steps: 11136/12117\n",
            "loss: 51.52681116624312 Steps: 11264/12117\n",
            "loss: 51.53563630179073 Steps: 11392/12117\n",
            "loss: 51.53793813917372 Steps: 11520/12117\n",
            "loss: 51.540942978072955 Steps: 11648/12117\n",
            "loss: 51.54739972819453 Steps: 11776/12117\n",
            "loss: 51.53106632027575 Steps: 11904/12117\n",
            "loss: 51.516201181614655 Steps: 12032/12117\n",
            "loss: 77.31407062211642 Steps: 12117/12117\n",
            "Epoch: 10  || Loss: 51.52357186379385\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            " 14%|█▎        | 11/81 [1:28:24<9:17:26, 477.81s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 53.482574462890625 Steps: 128/12117\n",
            "loss: 52.91913032531738 Steps: 256/12117\n",
            "loss: 52.11103820800781 Steps: 384/12117\n",
            "loss: 51.81941318511963 Steps: 512/12117\n",
            "loss: 51.85611419677734 Steps: 640/12117\n",
            "loss: 51.8262144724528 Steps: 768/12117\n",
            "loss: 51.81482424054827 Steps: 896/12117\n",
            "loss: 51.744750022888184 Steps: 1024/12117\n",
            "loss: 51.59907658894857 Steps: 1152/12117\n",
            "loss: 51.60177001953125 Steps: 1280/12117\n",
            "loss: 51.60176398537376 Steps: 1408/12117\n",
            "loss: 51.60576597849528 Steps: 1536/12117\n",
            "loss: 51.664350949800934 Steps: 1664/12117\n",
            "loss: 51.50325965881348 Steps: 1792/12117\n",
            "loss: 51.52643559773763 Steps: 1920/12117\n",
            "loss: 51.57867097854614 Steps: 2048/12117\n",
            "loss: 51.58016765818876 Steps: 2176/12117\n",
            "loss: 51.58206261528863 Steps: 2304/12117\n",
            "loss: 51.546297575298105 Steps: 2432/12117\n",
            "loss: 51.47096424102783 Steps: 2560/12117\n",
            "loss: 51.43819336664109 Steps: 2688/12117\n",
            "loss: 51.438737869262695 Steps: 2816/12117\n",
            "loss: 51.45315236630647 Steps: 2944/12117\n",
            "loss: 51.4114883740743 Steps: 3072/12117\n",
            "loss: 51.3508235168457 Steps: 3200/12117\n",
            "loss: 51.39819673391489 Steps: 3328/12117\n",
            "loss: 51.42194988109447 Steps: 3456/12117\n",
            "loss: 51.42323684692383 Steps: 3584/12117\n",
            "loss: 51.443522880817284 Steps: 3712/12117\n",
            "loss: 51.378489303588864 Steps: 3840/12117\n",
            "loss: 51.3013073090584 Steps: 3968/12117\n",
            "loss: 51.25468039512634 Steps: 4096/12117\n",
            "loss: 51.2295080242735 Steps: 4224/12117\n",
            "loss: 51.29257583618164 Steps: 4352/12117\n",
            "loss: 51.335142081124445 Steps: 4480/12117\n",
            "loss: 51.32616986168755 Steps: 4608/12117\n",
            "loss: 51.31413341212917 Steps: 4736/12117\n",
            "loss: 51.36379352368807 Steps: 4864/12117\n",
            "loss: 51.37459916334886 Steps: 4992/12117\n",
            "loss: 51.37685546875 Steps: 5120/12117\n",
            "loss: 51.38468858672351 Steps: 5248/12117\n",
            "loss: 51.39413361322312 Steps: 5376/12117\n",
            "loss: 51.38482018404229 Steps: 5504/12117\n",
            "loss: 51.407515872608535 Steps: 5632/12117\n",
            "loss: 51.366143290201826 Steps: 5760/12117\n",
            "loss: 51.35112538545028 Steps: 5888/12117\n",
            "loss: 51.34165767913169 Steps: 6016/12117\n",
            "loss: 51.37767203648885 Steps: 6144/12117\n",
            "loss: 51.3931908899424 Steps: 6272/12117\n",
            "loss: 51.39122955322266 Steps: 6400/12117\n",
            "loss: 51.38438991471833 Steps: 6528/12117\n",
            "loss: 51.41488772172194 Steps: 6656/12117\n",
            "loss: 51.42302279202443 Steps: 6784/12117\n",
            "loss: 51.40851508246528 Steps: 6912/12117\n",
            "loss: 51.41669686057351 Steps: 7040/12117\n",
            "loss: 51.4064942087446 Steps: 7168/12117\n",
            "loss: 51.41094562463593 Steps: 7296/12117\n",
            "loss: 51.41174493986985 Steps: 7424/12117\n",
            "loss: 51.42410879620051 Steps: 7552/12117\n",
            "loss: 51.44093335469564 Steps: 7680/12117\n",
            "loss: 51.45348401929511 Steps: 7808/12117\n",
            "loss: 51.450138768842145 Steps: 7936/12117\n",
            "loss: 51.43560076516772 Steps: 8064/12117\n",
            "loss: 51.46639770269394 Steps: 8192/12117\n",
            "loss: 51.48025031456581 Steps: 8320/12117\n",
            "loss: 51.46469526579886 Steps: 8448/12117\n",
            "loss: 51.47591855632725 Steps: 8576/12117\n",
            "loss: 51.46603550630457 Steps: 8704/12117\n",
            "loss: 51.44553966798644 Steps: 8832/12117\n",
            "loss: 51.45103536333357 Steps: 8960/12117\n",
            "loss: 51.444051178408344 Steps: 9088/12117\n",
            "loss: 51.465333779652916 Steps: 9216/12117\n",
            "loss: 51.45525083149949 Steps: 9344/12117\n",
            "loss: 51.454126667332005 Steps: 9472/12117\n",
            "loss: 51.46873006184896 Steps: 9600/12117\n",
            "loss: 51.46444616819683 Steps: 9728/12117\n",
            "loss: 51.451348094197066 Steps: 9856/12117\n",
            "loss: 51.47694387191381 Steps: 9984/12117\n",
            "loss: 51.47417937652974 Steps: 10112/12117\n",
            "loss: 51.46939678192139 Steps: 10240/12117\n",
            "loss: 51.477174546983505 Steps: 10368/12117\n",
            "loss: 51.476390373416066 Steps: 10496/12117\n",
            "loss: 51.47088273749294 Steps: 10624/12117\n",
            "loss: 51.472060203552246 Steps: 10752/12117\n",
            "loss: 51.46998133939855 Steps: 10880/12117\n",
            "loss: 51.46044482741245 Steps: 11008/12117\n",
            "loss: 51.45326631918721 Steps: 11136/12117\n",
            "loss: 51.471895391290836 Steps: 11264/12117\n",
            "loss: 51.47371056374539 Steps: 11392/12117\n",
            "loss: 51.474085023668074 Steps: 11520/12117\n",
            "loss: 51.47249997317136 Steps: 11648/12117\n",
            "loss: 51.459228349768594 Steps: 11776/12117\n",
            "loss: 51.45205380839686 Steps: 11904/12117\n",
            "loss: 51.45427261514867 Steps: 12032/12117\n",
            "loss: 77.2116363208877 Steps: 12117/12117\n",
            "Epoch: 11  || Loss: 51.45530769094398\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            " 15%|█▍        | 12/81 [1:36:21<9:08:56, 477.34s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 52.94159698486328 Steps: 128/12117\n",
            "loss: 51.397531509399414 Steps: 256/12117\n",
            "loss: 50.93917465209961 Steps: 384/12117\n",
            "loss: 50.9385986328125 Steps: 512/12117\n",
            "loss: 51.301964569091794 Steps: 640/12117\n",
            "loss: 51.20512135823568 Steps: 768/12117\n",
            "loss: 51.17122214181082 Steps: 896/12117\n",
            "loss: 51.29200744628906 Steps: 1024/12117\n",
            "loss: 51.3210334777832 Steps: 1152/12117\n",
            "loss: 51.44883575439453 Steps: 1280/12117\n",
            "loss: 51.38273967396129 Steps: 1408/12117\n",
            "loss: 51.34407424926758 Steps: 1536/12117\n",
            "loss: 51.37424791776217 Steps: 1664/12117\n",
            "loss: 51.37706320626395 Steps: 1792/12117\n",
            "loss: 51.46426823933919 Steps: 1920/12117\n",
            "loss: 51.48240399360657 Steps: 2048/12117\n",
            "loss: 51.436473173253674 Steps: 2176/12117\n",
            "loss: 51.42639266120063 Steps: 2304/12117\n",
            "loss: 51.5122214869449 Steps: 2432/12117\n",
            "loss: 51.47469539642334 Steps: 2560/12117\n",
            "loss: 51.53508140927269 Steps: 2688/12117\n",
            "loss: 51.52968840165572 Steps: 2816/12117\n",
            "loss: 51.48080295065175 Steps: 2944/12117\n",
            "loss: 51.404853661855064 Steps: 3072/12117\n",
            "loss: 51.42764587402344 Steps: 3200/12117\n",
            "loss: 51.46841107881986 Steps: 3328/12117\n",
            "loss: 51.476953153257014 Steps: 3456/12117\n",
            "loss: 51.497888156345915 Steps: 3584/12117\n",
            "loss: 51.49554377588733 Steps: 3712/12117\n",
            "loss: 51.45090497334798 Steps: 3840/12117\n",
            "loss: 51.43239581200384 Steps: 3968/12117\n",
            "loss: 51.42657816410065 Steps: 4096/12117\n",
            "loss: 51.38816636981386 Steps: 4224/12117\n",
            "loss: 51.44009433073156 Steps: 4352/12117\n",
            "loss: 51.46888242449079 Steps: 4480/12117\n",
            "loss: 51.44333034091525 Steps: 4608/12117\n",
            "loss: 51.44016997878616 Steps: 4736/12117\n",
            "loss: 51.43734620746813 Steps: 4864/12117\n",
            "loss: 51.44365535638271 Steps: 4992/12117\n",
            "loss: 51.46447601318359 Steps: 5120/12117\n",
            "loss: 51.40637951362424 Steps: 5248/12117\n",
            "loss: 51.43016733442034 Steps: 5376/12117\n",
            "loss: 51.42859223831532 Steps: 5504/12117\n",
            "loss: 51.44591097398238 Steps: 5632/12117\n",
            "loss: 51.41632571750217 Steps: 5760/12117\n",
            "loss: 51.386047197424844 Steps: 5888/12117\n",
            "loss: 51.37440474489902 Steps: 6016/12117\n",
            "loss: 51.37368369102478 Steps: 6144/12117\n",
            "loss: 51.403797227509166 Steps: 6272/12117\n",
            "loss: 51.36731513977051 Steps: 6400/12117\n",
            "loss: 51.375829509660306 Steps: 6528/12117\n",
            "loss: 51.359375146719124 Steps: 6656/12117\n",
            "loss: 51.42094119089954 Steps: 6784/12117\n",
            "loss: 51.40750821431478 Steps: 6912/12117\n",
            "loss: 51.39059004350142 Steps: 7040/12117\n",
            "loss: 51.38232728413173 Steps: 7168/12117\n",
            "loss: 51.384802299633364 Steps: 7296/12117\n",
            "loss: 51.37206248579354 Steps: 7424/12117\n",
            "loss: 51.3346326795675 Steps: 7552/12117\n",
            "loss: 51.34741268157959 Steps: 7680/12117\n",
            "loss: 51.34974207643603 Steps: 7808/12117\n",
            "loss: 51.35465886515956 Steps: 7936/12117\n",
            "loss: 51.36715056404235 Steps: 8064/12117\n",
            "loss: 51.36747992038727 Steps: 8192/12117\n",
            "loss: 51.35806702833909 Steps: 8320/12117\n",
            "loss: 51.36479499123313 Steps: 8448/12117\n",
            "loss: 51.339286861135 Steps: 8576/12117\n",
            "loss: 51.36468315124512 Steps: 8704/12117\n",
            "loss: 51.37111332105554 Steps: 8832/12117\n",
            "loss: 51.39160216195243 Steps: 8960/12117\n",
            "loss: 51.38261741315815 Steps: 9088/12117\n",
            "loss: 51.372888247172035 Steps: 9216/12117\n",
            "loss: 51.38443808359643 Steps: 9344/12117\n",
            "loss: 51.35771179199219 Steps: 9472/12117\n",
            "loss: 51.364348907470706 Steps: 9600/12117\n",
            "loss: 51.39085548802426 Steps: 9728/12117\n",
            "loss: 51.37843124587815 Steps: 9856/12117\n",
            "loss: 51.37994203812037 Steps: 9984/12117\n",
            "loss: 51.37378325643419 Steps: 10112/12117\n",
            "loss: 51.37469372749329 Steps: 10240/12117\n",
            "loss: 51.35927567658601 Steps: 10368/12117\n",
            "loss: 51.3662931860947 Steps: 10496/12117\n",
            "loss: 51.37837600708008 Steps: 10624/12117\n",
            "loss: 51.36808254605248 Steps: 10752/12117\n",
            "loss: 51.34862316355986 Steps: 10880/12117\n",
            "loss: 51.33556574444438 Steps: 11008/12117\n",
            "loss: 51.322853526849855 Steps: 11136/12117\n",
            "loss: 51.32920451597734 Steps: 11264/12117\n",
            "loss: 51.35024535789918 Steps: 11392/12117\n",
            "loss: 51.38003289964464 Steps: 11520/12117\n",
            "loss: 51.392996777544965 Steps: 11648/12117\n",
            "loss: 51.36293709796408 Steps: 11776/12117\n",
            "loss: 51.3702387655935 Steps: 11904/12117\n",
            "loss: 51.35412147197317 Steps: 12032/12117\n",
            "loss: 77.06369108028825 Steps: 12117/12117\n",
            "Epoch: 12  || Loss: 51.356714159720035\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            " 16%|█▌        | 13/81 [1:44:24<9:02:54, 479.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 49.48907470703125 Steps: 128/12117\n",
            "loss: 50.06732940673828 Steps: 256/12117\n",
            "loss: 50.5758908589681 Steps: 384/12117\n",
            "loss: 51.00842761993408 Steps: 512/12117\n",
            "loss: 50.97120208740235 Steps: 640/12117\n",
            "loss: 51.18656222025553 Steps: 768/12117\n",
            "loss: 51.01301629202707 Steps: 896/12117\n",
            "loss: 51.223631858825684 Steps: 1024/12117\n",
            "loss: 51.157474517822266 Steps: 1152/12117\n",
            "loss: 51.161278533935544 Steps: 1280/12117\n",
            "loss: 51.1371622952548 Steps: 1408/12117\n",
            "loss: 51.159108797709145 Steps: 1536/12117\n",
            "loss: 51.269300607534554 Steps: 1664/12117\n",
            "loss: 51.375799724033904 Steps: 1792/12117\n",
            "loss: 51.32982864379883 Steps: 1920/12117\n",
            "loss: 51.34132480621338 Steps: 2048/12117\n",
            "loss: 51.47266500136431 Steps: 2176/12117\n",
            "loss: 51.439492755466034 Steps: 2304/12117\n",
            "loss: 51.350340391460215 Steps: 2432/12117\n",
            "loss: 51.28059101104736 Steps: 2560/12117\n",
            "loss: 51.21463267008463 Steps: 2688/12117\n",
            "loss: 51.147265520962804 Steps: 2816/12117\n",
            "loss: 51.10651812346085 Steps: 2944/12117\n",
            "loss: 51.087403297424316 Steps: 3072/12117\n",
            "loss: 51.099835052490235 Steps: 3200/12117\n",
            "loss: 51.111537786630485 Steps: 3328/12117\n",
            "loss: 51.11029575489186 Steps: 3456/12117\n",
            "loss: 51.148086956569124 Steps: 3584/12117\n",
            "loss: 51.129138157285496 Steps: 3712/12117\n",
            "loss: 51.16976000467936 Steps: 3840/12117\n",
            "loss: 51.23920391451928 Steps: 3968/12117\n",
            "loss: 51.23467707633972 Steps: 4096/12117\n",
            "loss: 51.240565328887016 Steps: 4224/12117\n",
            "loss: 51.235030454747815 Steps: 4352/12117\n",
            "loss: 51.230892181396484 Steps: 4480/12117\n",
            "loss: 51.258114390903046 Steps: 4608/12117\n",
            "loss: 51.26940051929371 Steps: 4736/12117\n",
            "loss: 51.25715546858938 Steps: 4864/12117\n",
            "loss: 51.2859008984688 Steps: 4992/12117\n",
            "loss: 51.28832292556763 Steps: 5120/12117\n",
            "loss: 51.27106587479754 Steps: 5248/12117\n",
            "loss: 51.2462584177653 Steps: 5376/12117\n",
            "loss: 51.23824532087459 Steps: 5504/12117\n",
            "loss: 51.237951798872515 Steps: 5632/12117\n",
            "loss: 51.25409283108181 Steps: 5760/12117\n",
            "loss: 51.260404172150984 Steps: 5888/12117\n",
            "loss: 51.255067784735495 Steps: 6016/12117\n",
            "loss: 51.23750710487366 Steps: 6144/12117\n",
            "loss: 51.23711885724749 Steps: 6272/12117\n",
            "loss: 51.216783905029295 Steps: 6400/12117\n",
            "loss: 51.23110213934206 Steps: 6528/12117\n",
            "loss: 51.2546948653001 Steps: 6656/12117\n",
            "loss: 51.26350294868901 Steps: 6784/12117\n",
            "loss: 51.233568050243235 Steps: 6912/12117\n",
            "loss: 51.204091574928974 Steps: 7040/12117\n",
            "loss: 51.21752200807844 Steps: 7168/12117\n",
            "loss: 51.240182173879525 Steps: 7296/12117\n",
            "loss: 51.220336190585435 Steps: 7424/12117\n",
            "loss: 51.22105595216913 Steps: 7552/12117\n",
            "loss: 51.22382615407308 Steps: 7680/12117\n",
            "loss: 51.24664487995085 Steps: 7808/12117\n",
            "loss: 51.246590214390906 Steps: 7936/12117\n",
            "loss: 51.22038820054796 Steps: 8064/12117\n",
            "loss: 51.23876267671585 Steps: 8192/12117\n",
            "loss: 51.257028022179234 Steps: 8320/12117\n",
            "loss: 51.260904774521336 Steps: 8448/12117\n",
            "loss: 51.2602133110388 Steps: 8576/12117\n",
            "loss: 51.25282169790829 Steps: 8704/12117\n",
            "loss: 51.24242578036543 Steps: 8832/12117\n",
            "loss: 51.2432801927839 Steps: 8960/12117\n",
            "loss: 51.24563437448421 Steps: 9088/12117\n",
            "loss: 51.25647751490275 Steps: 9216/12117\n",
            "loss: 51.28285180705868 Steps: 9344/12117\n",
            "loss: 51.271516903026686 Steps: 9472/12117\n",
            "loss: 51.28548182169596 Steps: 9600/12117\n",
            "loss: 51.29108474129125 Steps: 9728/12117\n",
            "loss: 51.299751876236556 Steps: 9856/12117\n",
            "loss: 51.2949536152375 Steps: 9984/12117\n",
            "loss: 51.29184003419514 Steps: 10112/12117\n",
            "loss: 51.29422240257263 Steps: 10240/12117\n",
            "loss: 51.314042927306375 Steps: 10368/12117\n",
            "loss: 51.31932407472192 Steps: 10496/12117\n",
            "loss: 51.29037489374 Steps: 10624/12117\n",
            "loss: 51.274431637355256 Steps: 10752/12117\n",
            "loss: 51.269529454848346 Steps: 10880/12117\n",
            "loss: 51.28313437173533 Steps: 11008/12117\n",
            "loss: 51.26727821087015 Steps: 11136/12117\n",
            "loss: 51.262189561670475 Steps: 11264/12117\n",
            "loss: 51.26690103766624 Steps: 11392/12117\n",
            "loss: 51.260298029581705 Steps: 11520/12117\n",
            "loss: 51.28256129170512 Steps: 11648/12117\n",
            "loss: 51.27292326222295 Steps: 11776/12117\n",
            "loss: 51.26693897862588 Steps: 11904/12117\n",
            "loss: 51.28085931818536 Steps: 12032/12117\n",
            "loss: 76.9450043896182 Steps: 12117/12117\n",
            "Epoch: 13  || Loss: 51.27761908444062\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            " 17%|█▋        | 14/81 [1:52:33<8:58:17, 482.06s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 51.236515045166016 Steps: 128/12117\n",
            "loss: 50.46865463256836 Steps: 256/12117\n",
            "loss: 50.716253916422524 Steps: 384/12117\n",
            "loss: 51.097652435302734 Steps: 512/12117\n",
            "loss: 50.801031494140624 Steps: 640/12117\n",
            "loss: 50.809130350748696 Steps: 768/12117\n",
            "loss: 50.881469181605745 Steps: 896/12117\n",
            "loss: 51.0892539024353 Steps: 1024/12117\n",
            "loss: 51.18438466389974 Steps: 1152/12117\n",
            "loss: 51.35923461914062 Steps: 1280/12117\n",
            "loss: 51.236234144731 Steps: 1408/12117\n",
            "loss: 51.211082458496094 Steps: 1536/12117\n",
            "loss: 51.02873200636644 Steps: 1664/12117\n",
            "loss: 51.052604402814595 Steps: 1792/12117\n",
            "loss: 51.08842493693034 Steps: 1920/12117\n",
            "loss: 51.116533517837524 Steps: 2048/12117\n",
            "loss: 51.26248662612017 Steps: 2176/12117\n",
            "loss: 51.21562470330132 Steps: 2304/12117\n",
            "loss: 51.20338500173468 Steps: 2432/12117\n",
            "loss: 51.13900585174561 Steps: 2560/12117\n",
            "loss: 51.16482816423689 Steps: 2688/12117\n",
            "loss: 51.15562577681108 Steps: 2816/12117\n",
            "loss: 51.1437814132027 Steps: 2944/12117\n",
            "loss: 51.154091358184814 Steps: 3072/12117\n",
            "loss: 51.17693420410156 Steps: 3200/12117\n",
            "loss: 51.2317628126878 Steps: 3328/12117\n",
            "loss: 51.241462848804616 Steps: 3456/12117\n",
            "loss: 51.26011712210519 Steps: 3584/12117\n",
            "loss: 51.26208627635035 Steps: 3712/12117\n",
            "loss: 51.273820114135745 Steps: 3840/12117\n",
            "loss: 51.28663561421056 Steps: 3968/12117\n",
            "loss: 51.28624665737152 Steps: 4096/12117\n",
            "loss: 51.318158814401336 Steps: 4224/12117\n",
            "loss: 51.327448901008154 Steps: 4352/12117\n",
            "loss: 51.30752966744559 Steps: 4480/12117\n",
            "loss: 51.26380846235487 Steps: 4608/12117\n",
            "loss: 51.29493878338788 Steps: 4736/12117\n",
            "loss: 51.31575494063528 Steps: 4864/12117\n",
            "loss: 51.360521169809196 Steps: 4992/12117\n",
            "loss: 51.345801734924315 Steps: 5120/12117\n",
            "loss: 51.36488723754883 Steps: 5248/12117\n",
            "loss: 51.36844353448777 Steps: 5376/12117\n",
            "loss: 51.352122284645255 Steps: 5504/12117\n",
            "loss: 51.3533485585993 Steps: 5632/12117\n",
            "loss: 51.36183073255751 Steps: 5760/12117\n",
            "loss: 51.32766682168712 Steps: 5888/12117\n",
            "loss: 51.32347472170566 Steps: 6016/12117\n",
            "loss: 51.32947301864624 Steps: 6144/12117\n",
            "loss: 51.328235003412985 Steps: 6272/12117\n",
            "loss: 51.29498359680176 Steps: 6400/12117\n",
            "loss: 51.26157992493872 Steps: 6528/12117\n",
            "loss: 51.264192214378944 Steps: 6656/12117\n",
            "loss: 51.264601437550674 Steps: 6784/12117\n",
            "loss: 51.270362359506116 Steps: 6912/12117\n",
            "loss: 51.268077989058064 Steps: 7040/12117\n",
            "loss: 51.26960665839059 Steps: 7168/12117\n",
            "loss: 51.29334399574682 Steps: 7296/12117\n",
            "loss: 51.285404205322266 Steps: 7424/12117\n",
            "loss: 51.28943853863215 Steps: 7552/12117\n",
            "loss: 51.2630386988322 Steps: 7680/12117\n",
            "loss: 51.27506912731734 Steps: 7808/12117\n",
            "loss: 51.27611375624134 Steps: 7936/12117\n",
            "loss: 51.22182864234561 Steps: 8064/12117\n",
            "loss: 51.21545058488846 Steps: 8192/12117\n",
            "loss: 51.22404978825496 Steps: 8320/12117\n",
            "loss: 51.25392052621552 Steps: 8448/12117\n",
            "loss: 51.24268528952528 Steps: 8576/12117\n",
            "loss: 51.24018472783706 Steps: 8704/12117\n",
            "loss: 51.25353064053301 Steps: 8832/12117\n",
            "loss: 51.23825868879046 Steps: 8960/12117\n",
            "loss: 51.25139951034331 Steps: 9088/12117\n",
            "loss: 51.240202850765655 Steps: 9216/12117\n",
            "loss: 51.21907451054821 Steps: 9344/12117\n",
            "loss: 51.23267096442145 Steps: 9472/12117\n",
            "loss: 51.22720169067383 Steps: 9600/12117\n",
            "loss: 51.240018593637565 Steps: 9728/12117\n",
            "loss: 51.23393199970196 Steps: 9856/12117\n",
            "loss: 51.23005558894231 Steps: 9984/12117\n",
            "loss: 51.23630040808569 Steps: 10112/12117\n",
            "loss: 51.224573373794556 Steps: 10240/12117\n",
            "loss: 51.22917198840483 Steps: 10368/12117\n",
            "loss: 51.22851906753168 Steps: 10496/12117\n",
            "loss: 51.23066596525261 Steps: 10624/12117\n",
            "loss: 51.21374479929606 Steps: 10752/12117\n",
            "loss: 51.18619232177734 Steps: 10880/12117\n",
            "loss: 51.196065015571065 Steps: 11008/12117\n",
            "loss: 51.203862332749644 Steps: 11136/12117\n",
            "loss: 51.19538866389882 Steps: 11264/12117\n",
            "loss: 51.198840794938334 Steps: 11392/12117\n",
            "loss: 51.190574900309244 Steps: 11520/12117\n",
            "loss: 51.188059628664796 Steps: 11648/12117\n",
            "loss: 51.18596839904785 Steps: 11776/12117\n",
            "loss: 51.18459205217259 Steps: 11904/12117\n",
            "loss: 51.189036267869014 Steps: 12032/12117\n",
            "loss: 76.81887635269402 Steps: 12117/12117\n",
            "Epoch: 14  || Loss: 51.19356495403187\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            " 19%|█▊        | 15/81 [2:00:44<8:53:22, 484.89s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 50.807498931884766 Steps: 128/12117\n",
            "loss: 51.076576232910156 Steps: 256/12117\n",
            "loss: 51.44413757324219 Steps: 384/12117\n",
            "loss: 51.16911697387695 Steps: 512/12117\n",
            "loss: 51.37997283935547 Steps: 640/12117\n",
            "loss: 51.28215344746908 Steps: 768/12117\n",
            "loss: 51.50105285644531 Steps: 896/12117\n",
            "loss: 51.62269067764282 Steps: 1024/12117\n",
            "loss: 51.492561764187286 Steps: 1152/12117\n",
            "loss: 51.357971572875975 Steps: 1280/12117\n",
            "loss: 51.21285976063121 Steps: 1408/12117\n",
            "loss: 51.12177340189616 Steps: 1536/12117\n",
            "loss: 51.12456424419697 Steps: 1664/12117\n",
            "loss: 51.23102978297642 Steps: 1792/12117\n",
            "loss: 51.28785171508789 Steps: 1920/12117\n",
            "loss: 51.2020628452301 Steps: 2048/12117\n",
            "loss: 51.25760964786305 Steps: 2176/12117\n",
            "loss: 51.31275282965766 Steps: 2304/12117\n",
            "loss: 51.36655566566869 Steps: 2432/12117\n",
            "loss: 51.33939037322998 Steps: 2560/12117\n",
            "loss: 51.31331979660761 Steps: 2688/12117\n",
            "loss: 51.39011764526367 Steps: 2816/12117\n",
            "loss: 51.349760801895805 Steps: 2944/12117\n",
            "loss: 51.3008074760437 Steps: 3072/12117\n",
            "loss: 51.29380325317383 Steps: 3200/12117\n",
            "loss: 51.30660438537598 Steps: 3328/12117\n",
            "loss: 51.343969839590564 Steps: 3456/12117\n",
            "loss: 51.32277243477957 Steps: 3584/12117\n",
            "loss: 51.372859691751415 Steps: 3712/12117\n",
            "loss: 51.39964167277018 Steps: 3840/12117\n",
            "loss: 51.40883378059633 Steps: 3968/12117\n",
            "loss: 51.374815344810486 Steps: 4096/12117\n",
            "loss: 51.35396945837772 Steps: 4224/12117\n",
            "loss: 51.38597454744227 Steps: 4352/12117\n",
            "loss: 51.39449103219169 Steps: 4480/12117\n",
            "loss: 51.37023629082574 Steps: 4608/12117\n",
            "loss: 51.38098464141021 Steps: 4736/12117\n",
            "loss: 51.331173143888776 Steps: 4864/12117\n",
            "loss: 51.2989265246269 Steps: 4992/12117\n",
            "loss: 51.32595424652099 Steps: 5120/12117\n",
            "loss: 51.27957274274128 Steps: 5248/12117\n",
            "loss: 51.26859174455915 Steps: 5376/12117\n",
            "loss: 51.30583989342978 Steps: 5504/12117\n",
            "loss: 51.34086886319247 Steps: 5632/12117\n",
            "loss: 51.33628514607747 Steps: 5760/12117\n",
            "loss: 51.35842953557553 Steps: 5888/12117\n",
            "loss: 51.348330233959445 Steps: 6016/12117\n",
            "loss: 51.332964976628624 Steps: 6144/12117\n",
            "loss: 51.310587591054485 Steps: 6272/12117\n",
            "loss: 51.32264236450195 Steps: 6400/12117\n",
            "loss: 51.35541093115713 Steps: 6528/12117\n",
            "loss: 51.31612454927885 Steps: 6656/12117\n",
            "loss: 51.293389086453416 Steps: 6784/12117\n",
            "loss: 51.29779865123607 Steps: 6912/12117\n",
            "loss: 51.31712320501154 Steps: 7040/12117\n",
            "loss: 51.305316175733296 Steps: 7168/12117\n",
            "loss: 51.33123926530804 Steps: 7296/12117\n",
            "loss: 51.34305243656553 Steps: 7424/12117\n",
            "loss: 51.32196600962494 Steps: 7552/12117\n",
            "loss: 51.314484469095866 Steps: 7680/12117\n",
            "loss: 51.28093913344086 Steps: 7808/12117\n",
            "loss: 51.280785775953724 Steps: 7936/12117\n",
            "loss: 51.25697853451683 Steps: 8064/12117\n",
            "loss: 51.2427037358284 Steps: 8192/12117\n",
            "loss: 51.25822519155649 Steps: 8320/12117\n",
            "loss: 51.27758286216042 Steps: 8448/12117\n",
            "loss: 51.281381692459334 Steps: 8576/12117\n",
            "loss: 51.2779764287612 Steps: 8704/12117\n",
            "loss: 51.2589420373889 Steps: 8832/12117\n",
            "loss: 51.26061068943569 Steps: 8960/12117\n",
            "loss: 51.28133902751224 Steps: 9088/12117\n",
            "loss: 51.265476067860924 Steps: 9216/12117\n",
            "loss: 51.24603736563905 Steps: 9344/12117\n",
            "loss: 51.223600284473314 Steps: 9472/12117\n",
            "loss: 51.22488321940104 Steps: 9600/12117\n",
            "loss: 51.226679400393834 Steps: 9728/12117\n",
            "loss: 51.21615526273653 Steps: 9856/12117\n",
            "loss: 51.213394262851814 Steps: 9984/12117\n",
            "loss: 51.20025171207476 Steps: 10112/12117\n",
            "loss: 51.18700213432312 Steps: 10240/12117\n",
            "loss: 51.194934138545285 Steps: 10368/12117\n",
            "loss: 51.18633507519233 Steps: 10496/12117\n",
            "loss: 51.163174870502516 Steps: 10624/12117\n",
            "loss: 51.17174652644566 Steps: 10752/12117\n",
            "loss: 51.154830977495976 Steps: 10880/12117\n",
            "loss: 51.14725055251011 Steps: 11008/12117\n",
            "loss: 51.1325674824331 Steps: 11136/12117\n",
            "loss: 51.13238542730158 Steps: 11264/12117\n",
            "loss: 51.11919891700316 Steps: 11392/12117\n",
            "loss: 51.121501710679794 Steps: 11520/12117\n",
            "loss: 51.10147472004314 Steps: 11648/12117\n",
            "loss: 51.08555420585301 Steps: 11776/12117\n",
            "loss: 51.09009962184455 Steps: 11904/12117\n",
            "loss: 51.1076857384215 Steps: 12032/12117\n",
            "loss: 76.6962008374099 Steps: 12117/12117\n",
            "Epoch: 15  || Loss: 51.111811649920355\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            " 20%|█▉        | 16/81 [2:08:50<8:45:36, 485.18s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 49.320106506347656 Steps: 128/12117\n",
            "loss: 51.17797088623047 Steps: 256/12117\n",
            "loss: 51.19491958618164 Steps: 384/12117\n",
            "loss: 50.9934606552124 Steps: 512/12117\n",
            "loss: 50.878759765625 Steps: 640/12117\n",
            "loss: 50.909592946370445 Steps: 768/12117\n",
            "loss: 51.05213219778879 Steps: 896/12117\n",
            "loss: 50.914466857910156 Steps: 1024/12117\n",
            "loss: 50.89390309651693 Steps: 1152/12117\n",
            "loss: 50.94290771484375 Steps: 1280/12117\n",
            "loss: 50.99032141945579 Steps: 1408/12117\n",
            "loss: 50.97001043955485 Steps: 1536/12117\n",
            "loss: 51.08604665902945 Steps: 1664/12117\n",
            "loss: 51.11029243469238 Steps: 1792/12117\n",
            "loss: 51.21573206583659 Steps: 1920/12117\n",
            "loss: 51.175309896469116 Steps: 2048/12117\n",
            "loss: 51.21634741390453 Steps: 2176/12117\n",
            "loss: 51.29517279730903 Steps: 2304/12117\n",
            "loss: 51.211541627582754 Steps: 2432/12117\n",
            "loss: 51.2145492553711 Steps: 2560/12117\n",
            "loss: 51.24094409034366 Steps: 2688/12117\n",
            "loss: 51.24805103648793 Steps: 2816/12117\n",
            "loss: 51.309389860733695 Steps: 2944/12117\n",
            "loss: 51.279879887898765 Steps: 3072/12117\n",
            "loss: 51.279852600097655 Steps: 3200/12117\n",
            "loss: 51.260993223923904 Steps: 3328/12117\n",
            "loss: 51.28743150499132 Steps: 3456/12117\n",
            "loss: 51.24877793448312 Steps: 3584/12117\n",
            "loss: 51.19564740411167 Steps: 3712/12117\n",
            "loss: 51.18440767923991 Steps: 3840/12117\n",
            "loss: 51.16236311389554 Steps: 3968/12117\n",
            "loss: 51.14725887775421 Steps: 4096/12117\n",
            "loss: 51.12631757331617 Steps: 4224/12117\n",
            "loss: 51.086337706621954 Steps: 4352/12117\n",
            "loss: 51.06134458269392 Steps: 4480/12117\n",
            "loss: 51.09726280636258 Steps: 4608/12117\n",
            "loss: 51.08994365382839 Steps: 4736/12117\n",
            "loss: 51.1538432271857 Steps: 4864/12117\n",
            "loss: 51.15251120542869 Steps: 4992/12117\n",
            "loss: 51.15308380126953 Steps: 5120/12117\n",
            "loss: 51.14369406351229 Steps: 5248/12117\n",
            "loss: 51.123461314610076 Steps: 5376/12117\n",
            "loss: 51.12938823256382 Steps: 5504/12117\n",
            "loss: 51.10882013494318 Steps: 5632/12117\n",
            "loss: 51.10870327419705 Steps: 5760/12117\n",
            "loss: 51.125592853712 Steps: 5888/12117\n",
            "loss: 51.073235937889585 Steps: 6016/12117\n",
            "loss: 51.0633536974589 Steps: 6144/12117\n",
            "loss: 51.06811725850008 Steps: 6272/12117\n",
            "loss: 51.07164375305176 Steps: 6400/12117\n",
            "loss: 51.08293256572649 Steps: 6528/12117\n",
            "loss: 51.05888256659875 Steps: 6656/12117\n",
            "loss: 51.06109604745541 Steps: 6784/12117\n",
            "loss: 51.046852818241824 Steps: 6912/12117\n",
            "loss: 51.03571146184748 Steps: 7040/12117\n",
            "loss: 51.0329715864999 Steps: 7168/12117\n",
            "loss: 51.034155460826135 Steps: 7296/12117\n",
            "loss: 51.040605545043945 Steps: 7424/12117\n",
            "loss: 51.043471772791975 Steps: 7552/12117\n",
            "loss: 51.052004178365074 Steps: 7680/12117\n",
            "loss: 51.01564863861584 Steps: 7808/12117\n",
            "loss: 51.02540496087843 Steps: 7936/12117\n",
            "loss: 51.04703715490916 Steps: 8064/12117\n",
            "loss: 51.03139454126358 Steps: 8192/12117\n",
            "loss: 51.035912499061 Steps: 8320/12117\n",
            "loss: 51.0446254267837 Steps: 8448/12117\n",
            "loss: 51.0289303793836 Steps: 8576/12117\n",
            "loss: 51.0365624147303 Steps: 8704/12117\n",
            "loss: 51.02151035916978 Steps: 8832/12117\n",
            "loss: 51.01507824489048 Steps: 8960/12117\n",
            "loss: 51.033428675691844 Steps: 9088/12117\n",
            "loss: 51.04935148027208 Steps: 9216/12117\n",
            "loss: 51.064506426249466 Steps: 9344/12117\n",
            "loss: 51.06577491760254 Steps: 9472/12117\n",
            "loss: 51.06134023030599 Steps: 9600/12117\n",
            "loss: 51.043730434618496 Steps: 9728/12117\n",
            "loss: 51.01226023884563 Steps: 9856/12117\n",
            "loss: 51.011218193249825 Steps: 9984/12117\n",
            "loss: 51.009184535545636 Steps: 10112/12117\n",
            "loss: 51.00958938598633 Steps: 10240/12117\n",
            "loss: 51.00069366266698 Steps: 10368/12117\n",
            "loss: 51.01564449217261 Steps: 10496/12117\n",
            "loss: 51.00543433499624 Steps: 10624/12117\n",
            "loss: 51.034129778544106 Steps: 10752/12117\n",
            "loss: 51.04377396527459 Steps: 10880/12117\n",
            "loss: 51.04048906370651 Steps: 11008/12117\n",
            "loss: 51.03515129527826 Steps: 11136/12117\n",
            "loss: 51.022322091189295 Steps: 11264/12117\n",
            "loss: 51.03928606697683 Steps: 11392/12117\n",
            "loss: 51.0520028008355 Steps: 11520/12117\n",
            "loss: 51.03689118270036 Steps: 11648/12117\n",
            "loss: 51.03369306481403 Steps: 11776/12117\n",
            "loss: 51.02649680517053 Steps: 11904/12117\n",
            "loss: 51.02542008744909 Steps: 12032/12117\n",
            "loss: 76.57539469857703 Steps: 12117/12117\n",
            "Epoch: 16  || Loss: 51.03130413394483\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            " 21%|██        | 17/81 [2:16:58<8:38:25, 486.03s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 51.79530334472656 Steps: 128/12117\n",
            "loss: 51.67733573913574 Steps: 256/12117\n",
            "loss: 51.27700169881185 Steps: 384/12117\n",
            "loss: 50.941043853759766 Steps: 512/12117\n",
            "loss: 50.96573257446289 Steps: 640/12117\n",
            "loss: 50.99277305603027 Steps: 768/12117\n",
            "loss: 50.86367743355887 Steps: 896/12117\n",
            "loss: 50.58551597595215 Steps: 1024/12117\n",
            "loss: 50.44063483344184 Steps: 1152/12117\n",
            "loss: 50.65226058959961 Steps: 1280/12117\n",
            "loss: 50.74982799183238 Steps: 1408/12117\n",
            "loss: 50.71852493286133 Steps: 1536/12117\n",
            "loss: 50.752069619985726 Steps: 1664/12117\n",
            "loss: 50.76689747401646 Steps: 1792/12117\n",
            "loss: 50.747352600097656 Steps: 1920/12117\n",
            "loss: 50.71440768241882 Steps: 2048/12117\n",
            "loss: 50.81497820685892 Steps: 2176/12117\n",
            "loss: 50.82307370503744 Steps: 2304/12117\n",
            "loss: 50.79994763826069 Steps: 2432/12117\n",
            "loss: 50.744791412353514 Steps: 2560/12117\n",
            "loss: 50.78739020937965 Steps: 2688/12117\n",
            "loss: 50.82052352211692 Steps: 2816/12117\n",
            "loss: 50.800353838049844 Steps: 2944/12117\n",
            "loss: 50.78750133514404 Steps: 3072/12117\n",
            "loss: 50.77028457641602 Steps: 3200/12117\n",
            "loss: 50.76454059894268 Steps: 3328/12117\n",
            "loss: 50.76175181070963 Steps: 3456/12117\n",
            "loss: 50.761512075151714 Steps: 3584/12117\n",
            "loss: 50.78497643306338 Steps: 3712/12117\n",
            "loss: 50.79661610921224 Steps: 3840/12117\n",
            "loss: 50.82800908242503 Steps: 3968/12117\n",
            "loss: 50.84042298793793 Steps: 4096/12117\n",
            "loss: 50.85113409793738 Steps: 4224/12117\n",
            "loss: 50.87264173171099 Steps: 4352/12117\n",
            "loss: 50.885713304792134 Steps: 4480/12117\n",
            "loss: 50.870011223687065 Steps: 4608/12117\n",
            "loss: 50.83616287643845 Steps: 4736/12117\n",
            "loss: 50.842679375096374 Steps: 4864/12117\n",
            "loss: 50.901716672457184 Steps: 4992/12117\n",
            "loss: 50.87718181610107 Steps: 5120/12117\n",
            "loss: 50.89582499062143 Steps: 5248/12117\n",
            "loss: 50.89073935009184 Steps: 5376/12117\n",
            "loss: 50.90017327596975 Steps: 5504/12117\n",
            "loss: 50.89296037500555 Steps: 5632/12117\n",
            "loss: 50.90832782321506 Steps: 5760/12117\n",
            "loss: 50.896363631538726 Steps: 5888/12117\n",
            "loss: 50.91773183295067 Steps: 6016/12117\n",
            "loss: 50.88668441772461 Steps: 6144/12117\n",
            "loss: 50.86177787002252 Steps: 6272/12117\n",
            "loss: 50.879561004638674 Steps: 6400/12117\n",
            "loss: 50.84620786180683 Steps: 6528/12117\n",
            "loss: 50.8511722271259 Steps: 6656/12117\n",
            "loss: 50.8618013633872 Steps: 6784/12117\n",
            "loss: 50.84413026880335 Steps: 6912/12117\n",
            "loss: 50.84049918434837 Steps: 7040/12117\n",
            "loss: 50.85603339331491 Steps: 7168/12117\n",
            "loss: 50.858845827872294 Steps: 7296/12117\n",
            "loss: 50.865672144396555 Steps: 7424/12117\n",
            "loss: 50.8889471151061 Steps: 7552/12117\n",
            "loss: 50.90258598327637 Steps: 7680/12117\n",
            "loss: 50.896347546186604 Steps: 7808/12117\n",
            "loss: 50.89200998121692 Steps: 7936/12117\n",
            "loss: 50.895131005181206 Steps: 8064/12117\n",
            "loss: 50.89750546216965 Steps: 8192/12117\n",
            "loss: 50.90162611741286 Steps: 8320/12117\n",
            "loss: 50.90107559435295 Steps: 8448/12117\n",
            "loss: 50.91097151343502 Steps: 8576/12117\n",
            "loss: 50.91281453300925 Steps: 8704/12117\n",
            "loss: 50.91750169836956 Steps: 8832/12117\n",
            "loss: 50.877169363839286 Steps: 8960/12117\n",
            "loss: 50.889986494897116 Steps: 9088/12117\n",
            "loss: 50.88934453328451 Steps: 9216/12117\n",
            "loss: 50.87881835519451 Steps: 9344/12117\n",
            "loss: 50.889888299478066 Steps: 9472/12117\n",
            "loss: 50.88403793334961 Steps: 9600/12117\n",
            "loss: 50.889692406905326 Steps: 9728/12117\n",
            "loss: 50.87942767452884 Steps: 9856/12117\n",
            "loss: 50.872031872089096 Steps: 9984/12117\n",
            "loss: 50.87926937054984 Steps: 10112/12117\n",
            "loss: 50.91525092124939 Steps: 10240/12117\n",
            "loss: 50.91717797738534 Steps: 10368/12117\n",
            "loss: 50.91761421575779 Steps: 10496/12117\n",
            "loss: 50.905656240072595 Steps: 10624/12117\n",
            "loss: 50.9056137175787 Steps: 10752/12117\n",
            "loss: 50.91696992761948 Steps: 10880/12117\n",
            "loss: 50.90996626920478 Steps: 11008/12117\n",
            "loss: 50.90957764373429 Steps: 11136/12117\n",
            "loss: 50.912643389268354 Steps: 11264/12117\n",
            "loss: 50.93696311350619 Steps: 11392/12117\n",
            "loss: 50.92573521931966 Steps: 11520/12117\n",
            "loss: 50.92812972016387 Steps: 11648/12117\n",
            "loss: 50.92872093034827 Steps: 11776/12117\n",
            "loss: 50.93636116930234 Steps: 11904/12117\n",
            "loss: 50.9409777052859 Steps: 12032/12117\n",
            "loss: 76.44721223955185 Steps: 12117/12117\n",
            "Epoch: 17  || Loss: 50.945880897448305\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            " 22%|██▏       | 18/81 [2:25:11<8:32:25, 488.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 50.94651412963867 Steps: 128/12117\n",
            "loss: 51.10672950744629 Steps: 256/12117\n",
            "loss: 50.73159917195638 Steps: 384/12117\n",
            "loss: 50.79840564727783 Steps: 512/12117\n",
            "loss: 50.7678726196289 Steps: 640/12117\n",
            "loss: 50.877752939860024 Steps: 768/12117\n",
            "loss: 50.82166726248605 Steps: 896/12117\n",
            "loss: 50.608357429504395 Steps: 1024/12117\n",
            "loss: 50.76224051581489 Steps: 1152/12117\n",
            "loss: 50.78561592102051 Steps: 1280/12117\n",
            "loss: 50.891949740323156 Steps: 1408/12117\n",
            "loss: 50.835991541544594 Steps: 1536/12117\n",
            "loss: 51.01392716627855 Steps: 1664/12117\n",
            "loss: 51.04008892604283 Steps: 1792/12117\n",
            "loss: 51.15933787027995 Steps: 1920/12117\n",
            "loss: 51.17017912864685 Steps: 2048/12117\n",
            "loss: 51.09968230303596 Steps: 2176/12117\n",
            "loss: 51.0552912818061 Steps: 2304/12117\n",
            "loss: 51.042545318603516 Steps: 2432/12117\n",
            "loss: 50.9742431640625 Steps: 2560/12117\n",
            "loss: 50.970554533458895 Steps: 2688/12117\n",
            "loss: 50.91261014071378 Steps: 2816/12117\n",
            "loss: 50.948710565981656 Steps: 2944/12117\n",
            "loss: 50.97022835413615 Steps: 3072/12117\n",
            "loss: 50.892451477050784 Steps: 3200/12117\n",
            "loss: 50.85704333965595 Steps: 3328/12117\n",
            "loss: 50.87294543230975 Steps: 3456/12117\n",
            "loss: 50.84467165810721 Steps: 3584/12117\n",
            "loss: 50.850539634967674 Steps: 3712/12117\n",
            "loss: 50.84592653910319 Steps: 3840/12117\n",
            "loss: 50.8332270960654 Steps: 3968/12117\n",
            "loss: 50.897525668144226 Steps: 4096/12117\n",
            "loss: 50.86746319857511 Steps: 4224/12117\n",
            "loss: 50.85429988187902 Steps: 4352/12117\n",
            "loss: 50.84148243495396 Steps: 4480/12117\n",
            "loss: 50.84378348456489 Steps: 4608/12117\n",
            "loss: 50.812182761527396 Steps: 4736/12117\n",
            "loss: 50.81413891440943 Steps: 4864/12117\n",
            "loss: 50.82976267887996 Steps: 4992/12117\n",
            "loss: 50.829077529907224 Steps: 5120/12117\n",
            "loss: 50.798798630877236 Steps: 5248/12117\n",
            "loss: 50.82790501912435 Steps: 5376/12117\n",
            "loss: 50.83349618246389 Steps: 5504/12117\n",
            "loss: 50.87893954190341 Steps: 5632/12117\n",
            "loss: 50.86229019165039 Steps: 5760/12117\n",
            "loss: 50.88078606646994 Steps: 5888/12117\n",
            "loss: 50.83959287278196 Steps: 6016/12117\n",
            "loss: 50.8888521194458 Steps: 6144/12117\n",
            "loss: 50.86890831772162 Steps: 6272/12117\n",
            "loss: 50.869527130126954 Steps: 6400/12117\n",
            "loss: 50.86469859702915 Steps: 6528/12117\n",
            "loss: 50.875335473280686 Steps: 6656/12117\n",
            "loss: 50.84655092347343 Steps: 6784/12117\n",
            "loss: 50.8467244748716 Steps: 6912/12117\n",
            "loss: 50.832766862349075 Steps: 7040/12117\n",
            "loss: 50.861685684749055 Steps: 7168/12117\n",
            "loss: 50.82219762969435 Steps: 7296/12117\n",
            "loss: 50.84638122032429 Steps: 7424/12117\n",
            "loss: 50.83570299310199 Steps: 7552/12117\n",
            "loss: 50.85212421417236 Steps: 7680/12117\n",
            "loss: 50.858034978147415 Steps: 7808/12117\n",
            "loss: 50.839532236899096 Steps: 7936/12117\n",
            "loss: 50.842225453210254 Steps: 8064/12117\n",
            "loss: 50.84359157085419 Steps: 8192/12117\n",
            "loss: 50.85570238553561 Steps: 8320/12117\n",
            "loss: 50.86726142420913 Steps: 8448/12117\n",
            "loss: 50.873497407827806 Steps: 8576/12117\n",
            "loss: 50.85372111376594 Steps: 8704/12117\n",
            "loss: 50.848502781080164 Steps: 8832/12117\n",
            "loss: 50.82776652744838 Steps: 8960/12117\n",
            "loss: 50.83849920353419 Steps: 9088/12117\n",
            "loss: 50.85782522625394 Steps: 9216/12117\n",
            "loss: 50.850054283664655 Steps: 9344/12117\n",
            "loss: 50.84444339855297 Steps: 9472/12117\n",
            "loss: 50.8264778137207 Steps: 9600/12117\n",
            "loss: 50.821785726045306 Steps: 9728/12117\n",
            "loss: 50.836574455360314 Steps: 9856/12117\n",
            "loss: 50.842508951822914 Steps: 9984/12117\n",
            "loss: 50.85539409782313 Steps: 10112/12117\n",
            "loss: 50.86710839271545 Steps: 10240/12117\n",
            "loss: 50.869860613787615 Steps: 10368/12117\n",
            "loss: 50.866553236798545 Steps: 10496/12117\n",
            "loss: 50.87510754688677 Steps: 10624/12117\n",
            "loss: 50.87229719616118 Steps: 10752/12117\n",
            "loss: 50.86446147245519 Steps: 10880/12117\n",
            "loss: 50.86462517671807 Steps: 11008/12117\n",
            "loss: 50.87263953548738 Steps: 11136/12117\n",
            "loss: 50.857269677248865 Steps: 11264/12117\n",
            "loss: 50.85971176490355 Steps: 11392/12117\n",
            "loss: 50.85730912950304 Steps: 11520/12117\n",
            "loss: 50.86106168306791 Steps: 11648/12117\n",
            "loss: 50.84522881715194 Steps: 11776/12117\n",
            "loss: 50.85235115789598 Steps: 11904/12117\n",
            "loss: 50.839046478271484 Steps: 12032/12117\n",
            "loss: 76.28313649038782 Steps: 12117/12117\n",
            "Epoch: 18  || Loss: 50.83653768753665\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            " 23%|██▎       | 19/81 [2:33:22<8:25:20, 489.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 50.75212478637695 Steps: 128/12117\n",
            "loss: 51.06007194519043 Steps: 256/12117\n",
            "loss: 51.12804412841797 Steps: 384/12117\n",
            "loss: 50.86433696746826 Steps: 512/12117\n",
            "loss: 50.9188117980957 Steps: 640/12117\n",
            "loss: 50.84785143534342 Steps: 768/12117\n",
            "loss: 50.93328366960798 Steps: 896/12117\n",
            "loss: 51.02438402175903 Steps: 1024/12117\n",
            "loss: 50.799268086751304 Steps: 1152/12117\n",
            "loss: 50.727152252197264 Steps: 1280/12117\n",
            "loss: 50.94334238225763 Steps: 1408/12117\n",
            "loss: 50.84043916066488 Steps: 1536/12117\n",
            "loss: 50.88623516376202 Steps: 1664/12117\n",
            "loss: 50.84831428527832 Steps: 1792/12117\n",
            "loss: 50.720559946695964 Steps: 1920/12117\n",
            "loss: 50.6377329826355 Steps: 2048/12117\n",
            "loss: 50.6232728397145 Steps: 2176/12117\n",
            "loss: 50.52705319722494 Steps: 2304/12117\n",
            "loss: 50.62524574681332 Steps: 2432/12117\n",
            "loss: 50.626973152160645 Steps: 2560/12117\n",
            "loss: 50.61955788021996 Steps: 2688/12117\n",
            "loss: 50.70017381147905 Steps: 2816/12117\n",
            "loss: 50.68013896112857 Steps: 2944/12117\n",
            "loss: 50.623900731404625 Steps: 3072/12117\n",
            "loss: 50.607052917480466 Steps: 3200/12117\n",
            "loss: 50.70940751295824 Steps: 3328/12117\n",
            "loss: 50.69563462999132 Steps: 3456/12117\n",
            "loss: 50.694537435259136 Steps: 3584/12117\n",
            "loss: 50.66833969642376 Steps: 3712/12117\n",
            "loss: 50.6977793375651 Steps: 3840/12117\n",
            "loss: 50.67254146452873 Steps: 3968/12117\n",
            "loss: 50.68705487251282 Steps: 4096/12117\n",
            "loss: 50.644862897468336 Steps: 4224/12117\n",
            "loss: 50.645335477941174 Steps: 4352/12117\n",
            "loss: 50.680502319335936 Steps: 4480/12117\n",
            "loss: 50.65402804480659 Steps: 4608/12117\n",
            "loss: 50.68074015024546 Steps: 4736/12117\n",
            "loss: 50.709198801141035 Steps: 4864/12117\n",
            "loss: 50.67855942554963 Steps: 4992/12117\n",
            "loss: 50.685634803771975 Steps: 5120/12117\n",
            "loss: 50.67623063994617 Steps: 5248/12117\n",
            "loss: 50.65439787365141 Steps: 5376/12117\n",
            "loss: 50.65259001975836 Steps: 5504/12117\n",
            "loss: 50.685387264598496 Steps: 5632/12117\n",
            "loss: 50.69278581407335 Steps: 5760/12117\n",
            "loss: 50.68499415853749 Steps: 5888/12117\n",
            "loss: 50.70105012934258 Steps: 6016/12117\n",
            "loss: 50.672277530034386 Steps: 6144/12117\n",
            "loss: 50.644865308489116 Steps: 6272/12117\n",
            "loss: 50.61967971801758 Steps: 6400/12117\n",
            "loss: 50.61994395536535 Steps: 6528/12117\n",
            "loss: 50.63210810147799 Steps: 6656/12117\n",
            "loss: 50.686881443239606 Steps: 6784/12117\n",
            "loss: 50.7026170094808 Steps: 6912/12117\n",
            "loss: 50.71981499411843 Steps: 7040/12117\n",
            "loss: 50.744087151118684 Steps: 7168/12117\n",
            "loss: 50.74291510331003 Steps: 7296/12117\n",
            "loss: 50.763079281510976 Steps: 7424/12117\n",
            "loss: 50.72277069091797 Steps: 7552/12117\n",
            "loss: 50.74123922983805 Steps: 7680/12117\n",
            "loss: 50.75970415209161 Steps: 7808/12117\n",
            "loss: 50.74459666590537 Steps: 7936/12117\n",
            "loss: 50.74620080372644 Steps: 8064/12117\n",
            "loss: 50.74262583255768 Steps: 8192/12117\n",
            "loss: 50.73782923771785 Steps: 8320/12117\n",
            "loss: 50.73994682774399 Steps: 8448/12117\n",
            "loss: 50.74727966536337 Steps: 8576/12117\n",
            "loss: 50.76852501139921 Steps: 8704/12117\n",
            "loss: 50.7926781695822 Steps: 8832/12117\n",
            "loss: 50.78565695626395 Steps: 8960/12117\n",
            "loss: 50.79697208673182 Steps: 9088/12117\n",
            "loss: 50.818017429775665 Steps: 9216/12117\n",
            "loss: 50.834043163142795 Steps: 9344/12117\n",
            "loss: 50.85089420627903 Steps: 9472/12117\n",
            "loss: 50.8303412882487 Steps: 9600/12117\n",
            "loss: 50.82579667944657 Steps: 9728/12117\n",
            "loss: 50.795721524721614 Steps: 9856/12117\n",
            "loss: 50.81176728468675 Steps: 9984/12117\n",
            "loss: 50.797836158849016 Steps: 10112/12117\n",
            "loss: 50.78690395355225 Steps: 10240/12117\n",
            "loss: 50.79861002792547 Steps: 10368/12117\n",
            "loss: 50.79777973454173 Steps: 10496/12117\n",
            "loss: 50.779353865657946 Steps: 10624/12117\n",
            "loss: 50.76484271458217 Steps: 10752/12117\n",
            "loss: 50.75577163696289 Steps: 10880/12117\n",
            "loss: 50.76674873884334 Steps: 11008/12117\n",
            "loss: 50.76409982264727 Steps: 11136/12117\n",
            "loss: 50.78136968612671 Steps: 11264/12117\n",
            "loss: 50.77042667517501 Steps: 11392/12117\n",
            "loss: 50.75484920077854 Steps: 11520/12117\n",
            "loss: 50.758832742879676 Steps: 11648/12117\n",
            "loss: 50.76063454669455 Steps: 11776/12117\n",
            "loss: 50.77324028425319 Steps: 11904/12117\n",
            "loss: 50.7637047463275 Steps: 12032/12117\n",
            "loss: 76.1764303007805 Steps: 12117/12117\n",
            "Epoch: 19  || Loss: 50.76542664676096\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            " 25%|██▍       | 20/81 [2:41:40<8:19:43, 491.53s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 50.67332458496094 Steps: 128/12117\n",
            "loss: 51.092010498046875 Steps: 256/12117\n",
            "loss: 50.27026240030924 Steps: 384/12117\n",
            "loss: 50.50647163391113 Steps: 512/12117\n",
            "loss: 50.772840881347655 Steps: 640/12117\n",
            "loss: 50.79893557230631 Steps: 768/12117\n",
            "loss: 50.8530022757394 Steps: 896/12117\n",
            "loss: 50.66093969345093 Steps: 1024/12117\n",
            "loss: 50.92834175957574 Steps: 1152/12117\n",
            "loss: 50.7043960571289 Steps: 1280/12117\n",
            "loss: 50.73510534113104 Steps: 1408/12117\n",
            "loss: 50.71712176005045 Steps: 1536/12117\n",
            "loss: 50.64023560744066 Steps: 1664/12117\n",
            "loss: 50.54964556012835 Steps: 1792/12117\n",
            "loss: 50.58746465047201 Steps: 1920/12117\n",
            "loss: 50.429829120635986 Steps: 2048/12117\n",
            "loss: 50.43726034725414 Steps: 2176/12117\n",
            "loss: 50.53628412882487 Steps: 2304/12117\n",
            "loss: 50.61427086278012 Steps: 2432/12117\n",
            "loss: 50.62695446014404 Steps: 2560/12117\n",
            "loss: 50.56183424450102 Steps: 2688/12117\n",
            "loss: 50.52776180614125 Steps: 2816/12117\n",
            "loss: 50.50541803111201 Steps: 2944/12117\n",
            "loss: 50.501168409983315 Steps: 3072/12117\n",
            "loss: 50.49362457275391 Steps: 3200/12117\n",
            "loss: 50.52847099304199 Steps: 3328/12117\n",
            "loss: 50.56424430564598 Steps: 3456/12117\n",
            "loss: 50.54977525983538 Steps: 3584/12117\n",
            "loss: 50.619669420965785 Steps: 3712/12117\n",
            "loss: 50.62978960673014 Steps: 3840/12117\n",
            "loss: 50.58567908502394 Steps: 3968/12117\n",
            "loss: 50.5946809053421 Steps: 4096/12117\n",
            "loss: 50.564847425981 Steps: 4224/12117\n",
            "loss: 50.57940101623535 Steps: 4352/12117\n",
            "loss: 50.590330723353794 Steps: 4480/12117\n",
            "loss: 50.579453468322754 Steps: 4608/12117\n",
            "loss: 50.5604133605957 Steps: 4736/12117\n",
            "loss: 50.51044293453819 Steps: 4864/12117\n",
            "loss: 50.47036733382787 Steps: 4992/12117\n",
            "loss: 50.524112701416016 Steps: 5120/12117\n",
            "loss: 50.54868428300067 Steps: 5248/12117\n",
            "loss: 50.56486656552269 Steps: 5376/12117\n",
            "loss: 50.56581337507381 Steps: 5504/12117\n",
            "loss: 50.518047159368344 Steps: 5632/12117\n",
            "loss: 50.52190060085721 Steps: 5760/12117\n",
            "loss: 50.533737099688985 Steps: 5888/12117\n",
            "loss: 50.58005280190326 Steps: 6016/12117\n",
            "loss: 50.598493019739784 Steps: 6144/12117\n",
            "loss: 50.62813754957549 Steps: 6272/12117\n",
            "loss: 50.66729179382324 Steps: 6400/12117\n",
            "loss: 50.69442449831495 Steps: 6528/12117\n",
            "loss: 50.70443395467905 Steps: 6656/12117\n",
            "loss: 50.71081312647406 Steps: 6784/12117\n",
            "loss: 50.72058423360189 Steps: 6912/12117\n",
            "loss: 50.718001764470884 Steps: 7040/12117\n",
            "loss: 50.72798524584089 Steps: 7168/12117\n",
            "loss: 50.729231583444694 Steps: 7296/12117\n",
            "loss: 50.72565762750034 Steps: 7424/12117\n",
            "loss: 50.73398700002897 Steps: 7552/12117\n",
            "loss: 50.73522707621257 Steps: 7680/12117\n",
            "loss: 50.737982452892865 Steps: 7808/12117\n",
            "loss: 50.727257021011845 Steps: 7936/12117\n",
            "loss: 50.73862917461093 Steps: 8064/12117\n",
            "loss: 50.753244519233704 Steps: 8192/12117\n",
            "loss: 50.74802656907302 Steps: 8320/12117\n",
            "loss: 50.74564731482303 Steps: 8448/12117\n",
            "loss: 50.74226015005539 Steps: 8576/12117\n",
            "loss: 50.72284417993882 Steps: 8704/12117\n",
            "loss: 50.74145032357478 Steps: 8832/12117\n",
            "loss: 50.7334655216762 Steps: 8960/12117\n",
            "loss: 50.768234843939126 Steps: 9088/12117\n",
            "loss: 50.78844669130113 Steps: 9216/12117\n",
            "loss: 50.78069190456443 Steps: 9344/12117\n",
            "loss: 50.76012441274282 Steps: 9472/12117\n",
            "loss: 50.75907404581706 Steps: 9600/12117\n",
            "loss: 50.74429873416298 Steps: 9728/12117\n",
            "loss: 50.76117760794504 Steps: 9856/12117\n",
            "loss: 50.74582867744641 Steps: 9984/12117\n",
            "loss: 50.7616715733009 Steps: 10112/12117\n",
            "loss: 50.758310604095456 Steps: 10240/12117\n",
            "loss: 50.77666779506354 Steps: 10368/12117\n",
            "loss: 50.7778092361078 Steps: 10496/12117\n",
            "loss: 50.757465822150905 Steps: 10624/12117\n",
            "loss: 50.75306615375337 Steps: 10752/12117\n",
            "loss: 50.74792668959674 Steps: 10880/12117\n",
            "loss: 50.759052631466886 Steps: 11008/12117\n",
            "loss: 50.74740622509485 Steps: 11136/12117\n",
            "loss: 50.73409188877452 Steps: 11264/12117\n",
            "loss: 50.72477306408828 Steps: 11392/12117\n",
            "loss: 50.693915685017906 Steps: 11520/12117\n",
            "loss: 50.67856199662764 Steps: 11648/12117\n",
            "loss: 50.68275157265041 Steps: 11776/12117\n",
            "loss: 50.69414766373173 Steps: 11904/12117\n",
            "loss: 50.69047692481508 Steps: 12032/12117\n",
            "loss: 76.05724211701667 Steps: 12117/12117\n",
            "Epoch: 20  || Loss: 50.68599736691505\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            " 26%|██▌       | 21/81 [2:49:55<8:12:44, 492.74s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 51.83264923095703 Steps: 128/12117\n",
            "loss: 52.06180763244629 Steps: 256/12117\n",
            "loss: 51.85071690877279 Steps: 384/12117\n",
            "loss: 51.4815559387207 Steps: 512/12117\n",
            "loss: 51.01756973266602 Steps: 640/12117\n",
            "loss: 51.09013303120931 Steps: 768/12117\n",
            "loss: 51.10670525687082 Steps: 896/12117\n",
            "loss: 50.95214366912842 Steps: 1024/12117\n",
            "loss: 50.8849720425076 Steps: 1152/12117\n",
            "loss: 50.93569755554199 Steps: 1280/12117\n",
            "loss: 50.797209306196734 Steps: 1408/12117\n",
            "loss: 50.64405790964762 Steps: 1536/12117\n",
            "loss: 50.69579285841722 Steps: 1664/12117\n",
            "loss: 50.67983872549875 Steps: 1792/12117\n",
            "loss: 50.67269287109375 Steps: 1920/12117\n",
            "loss: 50.75067210197449 Steps: 2048/12117\n",
            "loss: 50.79190085915958 Steps: 2176/12117\n",
            "loss: 50.73314921061198 Steps: 2304/12117\n",
            "loss: 50.76395295795641 Steps: 2432/12117\n",
            "loss: 50.68224468231201 Steps: 2560/12117\n",
            "loss: 50.67275219871884 Steps: 2688/12117\n",
            "loss: 50.63835404135964 Steps: 2816/12117\n",
            "loss: 50.64331469328507 Steps: 2944/12117\n",
            "loss: 50.56277354558309 Steps: 3072/12117\n",
            "loss: 50.552384796142576 Steps: 3200/12117\n",
            "loss: 50.53570776719313 Steps: 3328/12117\n",
            "loss: 50.54772709034108 Steps: 3456/12117\n",
            "loss: 50.55690710885184 Steps: 3584/12117\n",
            "loss: 50.523740702661975 Steps: 3712/12117\n",
            "loss: 50.55907758076986 Steps: 3840/12117\n",
            "loss: 50.50121147401871 Steps: 3968/12117\n",
            "loss: 50.49114203453064 Steps: 4096/12117\n",
            "loss: 50.46682866414388 Steps: 4224/12117\n",
            "loss: 50.44103218527401 Steps: 4352/12117\n",
            "loss: 50.43941421508789 Steps: 4480/12117\n",
            "loss: 50.391850577460396 Steps: 4608/12117\n",
            "loss: 50.37630545126425 Steps: 4736/12117\n",
            "loss: 50.38531805339613 Steps: 4864/12117\n",
            "loss: 50.366434439634666 Steps: 4992/12117\n",
            "loss: 50.30482711791992 Steps: 5120/12117\n",
            "loss: 50.322066144245426 Steps: 5248/12117\n",
            "loss: 50.37082908267067 Steps: 5376/12117\n",
            "loss: 50.42267032002294 Steps: 5504/12117\n",
            "loss: 50.42510240728205 Steps: 5632/12117\n",
            "loss: 50.42279680040148 Steps: 5760/12117\n",
            "loss: 50.502072541610055 Steps: 5888/12117\n",
            "loss: 50.5155143737793 Steps: 6016/12117\n",
            "loss: 50.53861951828003 Steps: 6144/12117\n",
            "loss: 50.503816254284914 Steps: 6272/12117\n",
            "loss: 50.50973136901855 Steps: 6400/12117\n",
            "loss: 50.51376320334042 Steps: 6528/12117\n",
            "loss: 50.52695743854229 Steps: 6656/12117\n",
            "loss: 50.53071414299731 Steps: 6784/12117\n",
            "loss: 50.544071197509766 Steps: 6912/12117\n",
            "loss: 50.549637256969106 Steps: 7040/12117\n",
            "loss: 50.5421016556876 Steps: 7168/12117\n",
            "loss: 50.531914493493865 Steps: 7296/12117\n",
            "loss: 50.51954723226613 Steps: 7424/12117\n",
            "loss: 50.509281481726696 Steps: 7552/12117\n",
            "loss: 50.51287320454915 Steps: 7680/12117\n",
            "loss: 50.52926016635582 Steps: 7808/12117\n",
            "loss: 50.53026488519484 Steps: 7936/12117\n",
            "loss: 50.54273326813229 Steps: 8064/12117\n",
            "loss: 50.5505211353302 Steps: 8192/12117\n",
            "loss: 50.56684916569636 Steps: 8320/12117\n",
            "loss: 50.56683326489998 Steps: 8448/12117\n",
            "loss: 50.55923866158101 Steps: 8576/12117\n",
            "loss: 50.559382663053626 Steps: 8704/12117\n",
            "loss: 50.550942130710766 Steps: 8832/12117\n",
            "loss: 50.534014075143 Steps: 8960/12117\n",
            "loss: 50.52090706623776 Steps: 9088/12117\n",
            "loss: 50.54779349433051 Steps: 9216/12117\n",
            "loss: 50.56783096104452 Steps: 9344/12117\n",
            "loss: 50.5603924313107 Steps: 9472/12117\n",
            "loss: 50.581932576497394 Steps: 9600/12117\n",
            "loss: 50.58729884498998 Steps: 9728/12117\n",
            "loss: 50.58497243113332 Steps: 9856/12117\n",
            "loss: 50.56869169381949 Steps: 9984/12117\n",
            "loss: 50.58527272864233 Steps: 10112/12117\n",
            "loss: 50.58200969696045 Steps: 10240/12117\n",
            "loss: 50.598755683427974 Steps: 10368/12117\n",
            "loss: 50.59479890218595 Steps: 10496/12117\n",
            "loss: 50.59806148115411 Steps: 10624/12117\n",
            "loss: 50.56698335920061 Steps: 10752/12117\n",
            "loss: 50.57917341344497 Steps: 10880/12117\n",
            "loss: 50.57391463878543 Steps: 11008/12117\n",
            "loss: 50.5717764668081 Steps: 11136/12117\n",
            "loss: 50.562045487490565 Steps: 11264/12117\n",
            "loss: 50.55487416299541 Steps: 11392/12117\n",
            "loss: 50.56519643995497 Steps: 11520/12117\n",
            "loss: 50.56409965766655 Steps: 11648/12117\n",
            "loss: 50.60126354383386 Steps: 11776/12117\n",
            "loss: 50.602706088814685 Steps: 11904/12117\n",
            "loss: 50.60888188950559 Steps: 12032/12117\n",
            "loss: 75.9380886940026 Steps: 12117/12117\n",
            "Epoch: 21  || Loss: 50.606591252296035\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            " 27%|██▋       | 22/81 [2:58:05<8:03:38, 491.84s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 50.76228332519531 Steps: 128/12117\n",
            "loss: 50.81217384338379 Steps: 256/12117\n",
            "loss: 50.880706787109375 Steps: 384/12117\n",
            "loss: 50.55723762512207 Steps: 512/12117\n",
            "loss: 50.521253204345705 Steps: 640/12117\n",
            "loss: 50.64275550842285 Steps: 768/12117\n",
            "loss: 50.82424817766462 Steps: 896/12117\n",
            "loss: 50.89686155319214 Steps: 1024/12117\n",
            "loss: 50.92974387274848 Steps: 1152/12117\n",
            "loss: 50.97236442565918 Steps: 1280/12117\n",
            "loss: 50.821161790327594 Steps: 1408/12117\n",
            "loss: 50.90755240122477 Steps: 1536/12117\n",
            "loss: 50.82445878248949 Steps: 1664/12117\n",
            "loss: 50.78790882655552 Steps: 1792/12117\n",
            "loss: 50.726043446858725 Steps: 1920/12117\n",
            "loss: 50.67954730987549 Steps: 2048/12117\n",
            "loss: 50.66781683529125 Steps: 2176/12117\n",
            "loss: 50.687736087375214 Steps: 2304/12117\n",
            "loss: 50.651829970510384 Steps: 2432/12117\n",
            "loss: 50.67723560333252 Steps: 2560/12117\n",
            "loss: 50.715486617315385 Steps: 2688/12117\n",
            "loss: 50.67346174066717 Steps: 2816/12117\n",
            "loss: 50.69917512976605 Steps: 2944/12117\n",
            "loss: 50.65532477696737 Steps: 3072/12117\n",
            "loss: 50.673963470458986 Steps: 3200/12117\n",
            "loss: 50.66321020859938 Steps: 3328/12117\n",
            "loss: 50.638531296341505 Steps: 3456/12117\n",
            "loss: 50.676899092538015 Steps: 3584/12117\n",
            "loss: 50.611095691549366 Steps: 3712/12117\n",
            "loss: 50.60362459818522 Steps: 3840/12117\n",
            "loss: 50.59835803124212 Steps: 3968/12117\n",
            "loss: 50.55340874195099 Steps: 4096/12117\n",
            "loss: 50.54473148692738 Steps: 4224/12117\n",
            "loss: 50.53713966818417 Steps: 4352/12117\n",
            "loss: 50.48270111083984 Steps: 4480/12117\n",
            "loss: 50.47296227349175 Steps: 4608/12117\n",
            "loss: 50.426900090398014 Steps: 4736/12117\n",
            "loss: 50.41638936494526 Steps: 4864/12117\n",
            "loss: 50.42504335061098 Steps: 4992/12117\n",
            "loss: 50.41027097702026 Steps: 5120/12117\n",
            "loss: 50.3647716801341 Steps: 5248/12117\n",
            "loss: 50.33516802106585 Steps: 5376/12117\n",
            "loss: 50.32918912310933 Steps: 5504/12117\n",
            "loss: 50.33134937286377 Steps: 5632/12117\n",
            "loss: 50.33366054958768 Steps: 5760/12117\n",
            "loss: 50.32719910663107 Steps: 5888/12117\n",
            "loss: 50.32272923246343 Steps: 6016/12117\n",
            "loss: 50.298713525136314 Steps: 6144/12117\n",
            "loss: 50.257733169867066 Steps: 6272/12117\n",
            "loss: 50.25472091674805 Steps: 6400/12117\n",
            "loss: 50.2731533424527 Steps: 6528/12117\n",
            "loss: 50.29541015625 Steps: 6656/12117\n",
            "loss: 50.307599985374594 Steps: 6784/12117\n",
            "loss: 50.310069684629084 Steps: 6912/12117\n",
            "loss: 50.343638749556106 Steps: 7040/12117\n",
            "loss: 50.332609449114116 Steps: 7168/12117\n",
            "loss: 50.365238792017884 Steps: 7296/12117\n",
            "loss: 50.368172744224815 Steps: 7424/12117\n",
            "loss: 50.34066468578274 Steps: 7552/12117\n",
            "loss: 50.36907850901286 Steps: 7680/12117\n",
            "loss: 50.375206431404486 Steps: 7808/12117\n",
            "loss: 50.42102235363376 Steps: 7936/12117\n",
            "loss: 50.43984064980159 Steps: 8064/12117\n",
            "loss: 50.429664611816406 Steps: 8192/12117\n",
            "loss: 50.43870110144982 Steps: 8320/12117\n",
            "loss: 50.461206320560336 Steps: 8448/12117\n",
            "loss: 50.47111015889182 Steps: 8576/12117\n",
            "loss: 50.469253315645105 Steps: 8704/12117\n",
            "loss: 50.47689382580743 Steps: 8832/12117\n",
            "loss: 50.48247773306711 Steps: 8960/12117\n",
            "loss: 50.52772763749243 Steps: 9088/12117\n",
            "loss: 50.52638260523478 Steps: 9216/12117\n",
            "loss: 50.5040329711078 Steps: 9344/12117\n",
            "loss: 50.51574253391575 Steps: 9472/12117\n",
            "loss: 50.510917053222656 Steps: 9600/12117\n",
            "loss: 50.497513821250514 Steps: 9728/12117\n",
            "loss: 50.50069922905464 Steps: 9856/12117\n",
            "loss: 50.493754998231545 Steps: 9984/12117\n",
            "loss: 50.49720870392232 Steps: 10112/12117\n",
            "loss: 50.484835910797116 Steps: 10240/12117\n",
            "loss: 50.503122447449485 Steps: 10368/12117\n",
            "loss: 50.49526321597216 Steps: 10496/12117\n",
            "loss: 50.49346386093691 Steps: 10624/12117\n",
            "loss: 50.47994277590797 Steps: 10752/12117\n",
            "loss: 50.487067906996785 Steps: 10880/12117\n",
            "loss: 50.494494371635966 Steps: 11008/12117\n",
            "loss: 50.48834859913793 Steps: 11136/12117\n",
            "loss: 50.50634362480857 Steps: 11264/12117\n",
            "loss: 50.516562986909676 Steps: 11392/12117\n",
            "loss: 50.50900018480089 Steps: 11520/12117\n",
            "loss: 50.516649015657194 Steps: 11648/12117\n",
            "loss: 50.50123363992442 Steps: 11776/12117\n",
            "loss: 50.498519077095935 Steps: 11904/12117\n",
            "loss: 50.51711581615692 Steps: 12032/12117\n",
            "loss: 75.81745836901591 Steps: 12117/12117\n",
            "Epoch: 22  || Loss: 50.52620090202224\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            " 28%|██▊       | 23/81 [3:06:23<7:57:21, 493.82s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 50.49177551269531 Steps: 128/12117\n",
            "loss: 50.57805824279785 Steps: 256/12117\n",
            "loss: 50.625579833984375 Steps: 384/12117\n",
            "loss: 50.29724407196045 Steps: 512/12117\n",
            "loss: 50.41710586547852 Steps: 640/12117\n",
            "loss: 50.6277764638265 Steps: 768/12117\n",
            "loss: 50.77816663469587 Steps: 896/12117\n",
            "loss: 50.67786264419556 Steps: 1024/12117\n",
            "loss: 50.873175303141274 Steps: 1152/12117\n",
            "loss: 50.74178619384766 Steps: 1280/12117\n",
            "loss: 50.678712671453304 Steps: 1408/12117\n",
            "loss: 50.52528794606527 Steps: 1536/12117\n",
            "loss: 50.39001699594351 Steps: 1664/12117\n",
            "loss: 50.491981506347656 Steps: 1792/12117\n",
            "loss: 50.62306442260742 Steps: 1920/12117\n",
            "loss: 50.589832067489624 Steps: 2048/12117\n",
            "loss: 50.60925786635455 Steps: 2176/12117\n",
            "loss: 50.52901819017198 Steps: 2304/12117\n",
            "loss: 50.530257777163854 Steps: 2432/12117\n",
            "loss: 50.53110980987549 Steps: 2560/12117\n",
            "loss: 50.52181153070359 Steps: 2688/12117\n",
            "loss: 50.52729346535423 Steps: 2816/12117\n",
            "loss: 50.50749538255774 Steps: 2944/12117\n",
            "loss: 50.62270466486613 Steps: 3072/12117\n",
            "loss: 50.622782440185546 Steps: 3200/12117\n",
            "loss: 50.60660890432504 Steps: 3328/12117\n",
            "loss: 50.57214214183666 Steps: 3456/12117\n",
            "loss: 50.532813617161345 Steps: 3584/12117\n",
            "loss: 50.52523501165982 Steps: 3712/12117\n",
            "loss: 50.46553344726563 Steps: 3840/12117\n",
            "loss: 50.4618889593309 Steps: 3968/12117\n",
            "loss: 50.52995431423187 Steps: 4096/12117\n",
            "loss: 50.54114902380741 Steps: 4224/12117\n",
            "loss: 50.476557338938996 Steps: 4352/12117\n",
            "loss: 50.47234616960798 Steps: 4480/12117\n",
            "loss: 50.52754603491889 Steps: 4608/12117\n",
            "loss: 50.5207780374063 Steps: 4736/12117\n",
            "loss: 50.5005525287829 Steps: 4864/12117\n",
            "loss: 50.485404381385216 Steps: 4992/12117\n",
            "loss: 50.46692447662353 Steps: 5120/12117\n",
            "loss: 50.47882833713439 Steps: 5248/12117\n",
            "loss: 50.4901499067034 Steps: 5376/12117\n",
            "loss: 50.47050316389217 Steps: 5504/12117\n",
            "loss: 50.48805791681463 Steps: 5632/12117\n",
            "loss: 50.491470675998265 Steps: 5760/12117\n",
            "loss: 50.47001672827679 Steps: 5888/12117\n",
            "loss: 50.46534566676363 Steps: 6016/12117\n",
            "loss: 50.43984603881836 Steps: 6144/12117\n",
            "loss: 50.45021617655851 Steps: 6272/12117\n",
            "loss: 50.45001739501953 Steps: 6400/12117\n",
            "loss: 50.46410123039694 Steps: 6528/12117\n",
            "loss: 50.48017091017503 Steps: 6656/12117\n",
            "loss: 50.49710500465249 Steps: 6784/12117\n",
            "loss: 50.476936199046946 Steps: 6912/12117\n",
            "loss: 50.460138841108844 Steps: 7040/12117\n",
            "loss: 50.41905689239502 Steps: 7168/12117\n",
            "loss: 50.439369536282726 Steps: 7296/12117\n",
            "loss: 50.44744452114763 Steps: 7424/12117\n",
            "loss: 50.45130862219859 Steps: 7552/12117\n",
            "loss: 50.44028765360515 Steps: 7680/12117\n",
            "loss: 50.44457926515673 Steps: 7808/12117\n",
            "loss: 50.42751964446037 Steps: 7936/12117\n",
            "loss: 50.41622876364087 Steps: 8064/12117\n",
            "loss: 50.40704357624054 Steps: 8192/12117\n",
            "loss: 50.424278083214396 Steps: 8320/12117\n",
            "loss: 50.44886161341812 Steps: 8448/12117\n",
            "loss: 50.46680399197251 Steps: 8576/12117\n",
            "loss: 50.48391201916863 Steps: 8704/12117\n",
            "loss: 50.48538644763007 Steps: 8832/12117\n",
            "loss: 50.48732991899763 Steps: 8960/12117\n",
            "loss: 50.4837095233756 Steps: 9088/12117\n",
            "loss: 50.48985428280301 Steps: 9216/12117\n",
            "loss: 50.494511538988924 Steps: 9344/12117\n",
            "loss: 50.49480927957071 Steps: 9472/12117\n",
            "loss: 50.50883076985677 Steps: 9600/12117\n",
            "loss: 50.49594216597708 Steps: 9728/12117\n",
            "loss: 50.51424620987533 Steps: 9856/12117\n",
            "loss: 50.48764204367613 Steps: 9984/12117\n",
            "loss: 50.477988955340805 Steps: 10112/12117\n",
            "loss: 50.480909156799314 Steps: 10240/12117\n",
            "loss: 50.464870876736114 Steps: 10368/12117\n",
            "loss: 50.45651524241378 Steps: 10496/12117\n",
            "loss: 50.465942750494165 Steps: 10624/12117\n",
            "loss: 50.44897787911551 Steps: 10752/12117\n",
            "loss: 50.455919826731964 Steps: 10880/12117\n",
            "loss: 50.462877229202626 Steps: 11008/12117\n",
            "loss: 50.451269215550916 Steps: 11136/12117\n",
            "loss: 50.45681667327881 Steps: 11264/12117\n",
            "loss: 50.459245574608275 Steps: 11392/12117\n",
            "loss: 50.46192525227865 Steps: 11520/12117\n",
            "loss: 50.446806855254124 Steps: 11648/12117\n",
            "loss: 50.45627187645954 Steps: 11776/12117\n",
            "loss: 50.44793717579175 Steps: 11904/12117\n",
            "loss: 50.45753608865941 Steps: 12032/12117\n",
            "loss: 75.72704795199651 Steps: 12117/12117\n",
            "Epoch: 23  || Loss: 50.465949675032746\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            " 30%|██▉       | 24/81 [3:14:41<7:50:18, 495.06s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 50.311424255371094 Steps: 128/12117\n",
            "loss: 50.10918045043945 Steps: 256/12117\n",
            "loss: 50.619738260904946 Steps: 384/12117\n",
            "loss: 49.9802770614624 Steps: 512/12117\n",
            "loss: 49.98209915161133 Steps: 640/12117\n",
            "loss: 50.10354359944662 Steps: 768/12117\n",
            "loss: 50.25454330444336 Steps: 896/12117\n",
            "loss: 50.0743842124939 Steps: 1024/12117\n",
            "loss: 50.08534706963433 Steps: 1152/12117\n",
            "loss: 50.05666007995605 Steps: 1280/12117\n",
            "loss: 50.109288995916195 Steps: 1408/12117\n",
            "loss: 50.132594426472984 Steps: 1536/12117\n",
            "loss: 50.10041427612305 Steps: 1664/12117\n",
            "loss: 50.17416327340262 Steps: 1792/12117\n",
            "loss: 50.10207824707031 Steps: 1920/12117\n",
            "loss: 50.1498281955719 Steps: 2048/12117\n",
            "loss: 50.17040813670439 Steps: 2176/12117\n",
            "loss: 50.171927558051216 Steps: 2304/12117\n",
            "loss: 50.18052452488949 Steps: 2432/12117\n",
            "loss: 50.14501533508301 Steps: 2560/12117\n",
            "loss: 50.113675617036364 Steps: 2688/12117\n",
            "loss: 50.18281451138583 Steps: 2816/12117\n",
            "loss: 50.1752752221149 Steps: 2944/12117\n",
            "loss: 50.14017232259115 Steps: 3072/12117\n",
            "loss: 50.10496109008789 Steps: 3200/12117\n",
            "loss: 50.174305549034706 Steps: 3328/12117\n",
            "loss: 50.204864219382955 Steps: 3456/12117\n",
            "loss: 50.27500329698835 Steps: 3584/12117\n",
            "loss: 50.345048312483165 Steps: 3712/12117\n",
            "loss: 50.399180221557614 Steps: 3840/12117\n",
            "loss: 50.47675544984879 Steps: 3968/12117\n",
            "loss: 50.43777024745941 Steps: 4096/12117\n",
            "loss: 50.43406261097301 Steps: 4224/12117\n",
            "loss: 50.41350207609289 Steps: 4352/12117\n",
            "loss: 50.39657004220145 Steps: 4480/12117\n",
            "loss: 50.35254277123345 Steps: 4608/12117\n",
            "loss: 50.33647062971785 Steps: 4736/12117\n",
            "loss: 50.31456154271176 Steps: 4864/12117\n",
            "loss: 50.280398442195015 Steps: 4992/12117\n",
            "loss: 50.301204681396484 Steps: 5120/12117\n",
            "loss: 50.25440709183856 Steps: 5248/12117\n",
            "loss: 50.248721713111514 Steps: 5376/12117\n",
            "loss: 50.26115115853243 Steps: 5504/12117\n",
            "loss: 50.303075617009945 Steps: 5632/12117\n",
            "loss: 50.32474822998047 Steps: 5760/12117\n",
            "loss: 50.32238330011783 Steps: 5888/12117\n",
            "loss: 50.31672262638173 Steps: 6016/12117\n",
            "loss: 50.32642690340678 Steps: 6144/12117\n",
            "loss: 50.32492189991231 Steps: 6272/12117\n",
            "loss: 50.36611198425293 Steps: 6400/12117\n",
            "loss: 50.394956775740084 Steps: 6528/12117\n",
            "loss: 50.40145521897536 Steps: 6656/12117\n",
            "loss: 50.39111047420862 Steps: 6784/12117\n",
            "loss: 50.385665540341975 Steps: 6912/12117\n",
            "loss: 50.391734452681106 Steps: 7040/12117\n",
            "loss: 50.41105154582432 Steps: 7168/12117\n",
            "loss: 50.39681752522787 Steps: 7296/12117\n",
            "loss: 50.39993023050243 Steps: 7424/12117\n",
            "loss: 50.41169227988033 Steps: 7552/12117\n",
            "loss: 50.42591177622477 Steps: 7680/12117\n",
            "loss: 50.418676657754865 Steps: 7808/12117\n",
            "loss: 50.40298449608587 Steps: 7936/12117\n",
            "loss: 50.40030216035389 Steps: 8064/12117\n",
            "loss: 50.39007794857025 Steps: 8192/12117\n",
            "loss: 50.371661024827226 Steps: 8320/12117\n",
            "loss: 50.38108097423207 Steps: 8448/12117\n",
            "loss: 50.34188501158757 Steps: 8576/12117\n",
            "loss: 50.3271162369672 Steps: 8704/12117\n",
            "loss: 50.34571821793266 Steps: 8832/12117\n",
            "loss: 50.36164430890764 Steps: 8960/12117\n",
            "loss: 50.36716837278554 Steps: 9088/12117\n",
            "loss: 50.36655781004164 Steps: 9216/12117\n",
            "loss: 50.37553792457058 Steps: 9344/12117\n",
            "loss: 50.38379359889675 Steps: 9472/12117\n",
            "loss: 50.38970219930013 Steps: 9600/12117\n",
            "loss: 50.37955048209742 Steps: 9728/12117\n",
            "loss: 50.37663873449549 Steps: 9856/12117\n",
            "loss: 50.37348649440668 Steps: 9984/12117\n",
            "loss: 50.38214922554885 Steps: 10112/12117\n",
            "loss: 50.382736110687254 Steps: 10240/12117\n",
            "loss: 50.39301931122203 Steps: 10368/12117\n",
            "loss: 50.38756579887576 Steps: 10496/12117\n",
            "loss: 50.3932108591838 Steps: 10624/12117\n",
            "loss: 50.38362984430222 Steps: 10752/12117\n",
            "loss: 50.3906727959128 Steps: 10880/12117\n",
            "loss: 50.39423840544944 Steps: 11008/12117\n",
            "loss: 50.39481257296156 Steps: 11136/12117\n",
            "loss: 50.39717245101929 Steps: 11264/12117\n",
            "loss: 50.386789129021466 Steps: 11392/12117\n",
            "loss: 50.39225985209147 Steps: 11520/12117\n",
            "loss: 50.387907929472874 Steps: 11648/12117\n",
            "loss: 50.400997120401136 Steps: 11776/12117\n",
            "loss: 50.404784664031 Steps: 11904/12117\n",
            "loss: 50.389199967079975 Steps: 12032/12117\n",
            "loss: 75.60516368287267 Steps: 12117/12117\n",
            "Epoch: 24  || Loss: 50.38472367245991\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            " 31%|███       | 25/81 [3:22:47<7:39:20, 492.16s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 51.02717971801758 Steps: 128/12117\n",
            "loss: 49.38643455505371 Steps: 256/12117\n",
            "loss: 49.95933532714844 Steps: 384/12117\n",
            "loss: 49.87332248687744 Steps: 512/12117\n",
            "loss: 50.30117340087891 Steps: 640/12117\n",
            "loss: 50.302014668782554 Steps: 768/12117\n",
            "loss: 50.48989159720285 Steps: 896/12117\n",
            "loss: 50.63111209869385 Steps: 1024/12117\n",
            "loss: 50.58591673109267 Steps: 1152/12117\n",
            "loss: 50.6162826538086 Steps: 1280/12117\n",
            "loss: 50.57778271761808 Steps: 1408/12117\n",
            "loss: 50.604200998942055 Steps: 1536/12117\n",
            "loss: 50.55129770132211 Steps: 1664/12117\n",
            "loss: 50.596014567783904 Steps: 1792/12117\n",
            "loss: 50.62811126708984 Steps: 1920/12117\n",
            "loss: 50.595205545425415 Steps: 2048/12117\n",
            "loss: 50.57821924546186 Steps: 2176/12117\n",
            "loss: 50.48884285820855 Steps: 2304/12117\n",
            "loss: 50.571966673198496 Steps: 2432/12117\n",
            "loss: 50.58326263427735 Steps: 2560/12117\n",
            "loss: 50.625264485677086 Steps: 2688/12117\n",
            "loss: 50.57972821322355 Steps: 2816/12117\n",
            "loss: 50.5190544128418 Steps: 2944/12117\n",
            "loss: 50.47764269510905 Steps: 3072/12117\n",
            "loss: 50.39218734741211 Steps: 3200/12117\n",
            "loss: 50.38155746459961 Steps: 3328/12117\n",
            "loss: 50.35410351223416 Steps: 3456/12117\n",
            "loss: 50.3374547958374 Steps: 3584/12117\n",
            "loss: 50.352847132189524 Steps: 3712/12117\n",
            "loss: 50.330905278523765 Steps: 3840/12117\n",
            "loss: 50.347782135009766 Steps: 3968/12117\n",
            "loss: 50.36031794548035 Steps: 4096/12117\n",
            "loss: 50.362494266394414 Steps: 4224/12117\n",
            "loss: 50.37025934107163 Steps: 4352/12117\n",
            "loss: 50.2985705784389 Steps: 4480/12117\n",
            "loss: 50.309471660190155 Steps: 4608/12117\n",
            "loss: 50.34864879298855 Steps: 4736/12117\n",
            "loss: 50.36673154328999 Steps: 4864/12117\n",
            "loss: 50.37149771665916 Steps: 4992/12117\n",
            "loss: 50.31986246109009 Steps: 5120/12117\n",
            "loss: 50.32717262826315 Steps: 5248/12117\n",
            "loss: 50.33686483474005 Steps: 5376/12117\n",
            "loss: 50.35033523204715 Steps: 5504/12117\n",
            "loss: 50.354936599731445 Steps: 5632/12117\n",
            "loss: 50.40025388929579 Steps: 5760/12117\n",
            "loss: 50.40059670158055 Steps: 5888/12117\n",
            "loss: 50.38173772933635 Steps: 6016/12117\n",
            "loss: 50.37821817398071 Steps: 6144/12117\n",
            "loss: 50.37735569233797 Steps: 6272/12117\n",
            "loss: 50.35880493164063 Steps: 6400/12117\n",
            "loss: 50.35716464472752 Steps: 6528/12117\n",
            "loss: 50.36463693472055 Steps: 6656/12117\n",
            "loss: 50.374921834693765 Steps: 6784/12117\n",
            "loss: 50.37804864954065 Steps: 6912/12117\n",
            "loss: 50.37019736550071 Steps: 7040/12117\n",
            "loss: 50.371627875736785 Steps: 7168/12117\n",
            "loss: 50.37914905213473 Steps: 7296/12117\n",
            "loss: 50.374880823595774 Steps: 7424/12117\n",
            "loss: 50.36675812026202 Steps: 7552/12117\n",
            "loss: 50.36709067026774 Steps: 7680/12117\n",
            "loss: 50.37237530067319 Steps: 7808/12117\n",
            "loss: 50.38950452496928 Steps: 7936/12117\n",
            "loss: 50.3829591539171 Steps: 8064/12117\n",
            "loss: 50.396754920482635 Steps: 8192/12117\n",
            "loss: 50.37138237586388 Steps: 8320/12117\n",
            "loss: 50.372569633252695 Steps: 8448/12117\n",
            "loss: 50.38682334102801 Steps: 8576/12117\n",
            "loss: 50.37392442366656 Steps: 8704/12117\n",
            "loss: 50.33646901448568 Steps: 8832/12117\n",
            "loss: 50.31923043387277 Steps: 8960/12117\n",
            "loss: 50.297344637588715 Steps: 9088/12117\n",
            "loss: 50.27885993321737 Steps: 9216/12117\n",
            "loss: 50.287442351040774 Steps: 9344/12117\n",
            "loss: 50.271671346715976 Steps: 9472/12117\n",
            "loss: 50.26144190470377 Steps: 9600/12117\n",
            "loss: 50.26528614445736 Steps: 9728/12117\n",
            "loss: 50.26058553720449 Steps: 9856/12117\n",
            "loss: 50.28631777641101 Steps: 9984/12117\n",
            "loss: 50.284318175496935 Steps: 10112/12117\n",
            "loss: 50.25468482971191 Steps: 10240/12117\n",
            "loss: 50.250199494538485 Steps: 10368/12117\n",
            "loss: 50.24015808105469 Steps: 10496/12117\n",
            "loss: 50.24571531364717 Steps: 10624/12117\n",
            "loss: 50.25250866299584 Steps: 10752/12117\n",
            "loss: 50.258822586957145 Steps: 10880/12117\n",
            "loss: 50.276139547658524 Steps: 11008/12117\n",
            "loss: 50.27853932873956 Steps: 11136/12117\n",
            "loss: 50.28471647609364 Steps: 11264/12117\n",
            "loss: 50.28635119320302 Steps: 11392/12117\n",
            "loss: 50.29552205403646 Steps: 11520/12117\n",
            "loss: 50.285282512287516 Steps: 11648/12117\n",
            "loss: 50.29244298520295 Steps: 11776/12117\n",
            "loss: 50.28335427725187 Steps: 11904/12117\n",
            "loss: 50.28772983145207 Steps: 12032/12117\n",
            "loss: 75.48057610715506 Steps: 12117/12117\n",
            "Epoch: 25  || Loss: 50.3016961347922\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            " 32%|███▏      | 26/81 [3:30:48<7:28:12, 488.96s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 51.57698440551758 Steps: 128/12117\n",
            "loss: 51.147216796875 Steps: 256/12117\n",
            "loss: 50.885878245035805 Steps: 384/12117\n",
            "loss: 50.88784885406494 Steps: 512/12117\n",
            "loss: 50.80407180786133 Steps: 640/12117\n",
            "loss: 50.64446830749512 Steps: 768/12117\n",
            "loss: 50.698296138218474 Steps: 896/12117\n",
            "loss: 50.660314083099365 Steps: 1024/12117\n",
            "loss: 50.42471610175239 Steps: 1152/12117\n",
            "loss: 50.454922103881835 Steps: 1280/12117\n",
            "loss: 50.44344156438654 Steps: 1408/12117\n",
            "loss: 50.45196374257406 Steps: 1536/12117\n",
            "loss: 50.35742539625902 Steps: 1664/12117\n",
            "loss: 50.390184947422576 Steps: 1792/12117\n",
            "loss: 50.30838241577148 Steps: 1920/12117\n",
            "loss: 50.267900228500366 Steps: 2048/12117\n",
            "loss: 50.31824852438534 Steps: 2176/12117\n",
            "loss: 50.23146438598633 Steps: 2304/12117\n",
            "loss: 50.19767982081363 Steps: 2432/12117\n",
            "loss: 50.15151958465576 Steps: 2560/12117\n",
            "loss: 50.093052092052645 Steps: 2688/12117\n",
            "loss: 50.18797406283292 Steps: 2816/12117\n",
            "loss: 50.20161238960598 Steps: 2944/12117\n",
            "loss: 50.17702563603719 Steps: 3072/12117\n",
            "loss: 50.23272933959961 Steps: 3200/12117\n",
            "loss: 50.245169126070465 Steps: 3328/12117\n",
            "loss: 50.22962471290871 Steps: 3456/12117\n",
            "loss: 50.26046575818743 Steps: 3584/12117\n",
            "loss: 50.30071692631162 Steps: 3712/12117\n",
            "loss: 50.258676783243814 Steps: 3840/12117\n",
            "loss: 50.22805699994487 Steps: 3968/12117\n",
            "loss: 50.22647261619568 Steps: 4096/12117\n",
            "loss: 50.20826004490708 Steps: 4224/12117\n",
            "loss: 50.22415262110093 Steps: 4352/12117\n",
            "loss: 50.22558877127511 Steps: 4480/12117\n",
            "loss: 50.2369384765625 Steps: 4608/12117\n",
            "loss: 50.25356034974794 Steps: 4736/12117\n",
            "loss: 50.242822345934414 Steps: 4864/12117\n",
            "loss: 50.24858015011518 Steps: 4992/12117\n",
            "loss: 50.25755338668823 Steps: 5120/12117\n",
            "loss: 50.24145098430355 Steps: 5248/12117\n",
            "loss: 50.244410832722984 Steps: 5376/12117\n",
            "loss: 50.268045026202536 Steps: 5504/12117\n",
            "loss: 50.262419093738906 Steps: 5632/12117\n",
            "loss: 50.23069381713867 Steps: 5760/12117\n",
            "loss: 50.210622538690984 Steps: 5888/12117\n",
            "loss: 50.20607165072827 Steps: 6016/12117\n",
            "loss: 50.2134313583374 Steps: 6144/12117\n",
            "loss: 50.17412940823302 Steps: 6272/12117\n",
            "loss: 50.15632781982422 Steps: 6400/12117\n",
            "loss: 50.181468290441174 Steps: 6528/12117\n",
            "loss: 50.16535054720365 Steps: 6656/12117\n",
            "loss: 50.14767153757923 Steps: 6784/12117\n",
            "loss: 50.153197394477 Steps: 6912/12117\n",
            "loss: 50.169269769841975 Steps: 7040/12117\n",
            "loss: 50.15466444832938 Steps: 7168/12117\n",
            "loss: 50.15313814397444 Steps: 7296/12117\n",
            "loss: 50.17107128274852 Steps: 7424/12117\n",
            "loss: 50.18126200013241 Steps: 7552/12117\n",
            "loss: 50.170778846740724 Steps: 7680/12117\n",
            "loss: 50.18555763119557 Steps: 7808/12117\n",
            "loss: 50.21118582448652 Steps: 7936/12117\n",
            "loss: 50.168155912369016 Steps: 8064/12117\n",
            "loss: 50.17143052816391 Steps: 8192/12117\n",
            "loss: 50.16418269230769 Steps: 8320/12117\n",
            "loss: 50.191530632250235 Steps: 8448/12117\n",
            "loss: 50.206344035134386 Steps: 8576/12117\n",
            "loss: 50.22649736965404 Steps: 8704/12117\n",
            "loss: 50.22631023241126 Steps: 8832/12117\n",
            "loss: 50.22228584289551 Steps: 8960/12117\n",
            "loss: 50.22254610733247 Steps: 9088/12117\n",
            "loss: 50.23731984032525 Steps: 9216/12117\n",
            "loss: 50.23835456534608 Steps: 9344/12117\n",
            "loss: 50.24422197084169 Steps: 9472/12117\n",
            "loss: 50.24995025634766 Steps: 9600/12117\n",
            "loss: 50.23799509751169 Steps: 9728/12117\n",
            "loss: 50.21817749816102 Steps: 9856/12117\n",
            "loss: 50.20728434049166 Steps: 9984/12117\n",
            "loss: 50.21062643316728 Steps: 10112/12117\n",
            "loss: 50.21196465492248 Steps: 10240/12117\n",
            "loss: 50.198806056269895 Steps: 10368/12117\n",
            "loss: 50.21147881484613 Steps: 10496/12117\n",
            "loss: 50.19327876079513 Steps: 10624/12117\n",
            "loss: 50.17593833378383 Steps: 10752/12117\n",
            "loss: 50.200282960779525 Steps: 10880/12117\n",
            "loss: 50.21109505586846 Steps: 11008/12117\n",
            "loss: 50.223087573873585 Steps: 11136/12117\n",
            "loss: 50.22595908425071 Steps: 11264/12117\n",
            "loss: 50.22493748182661 Steps: 11392/12117\n",
            "loss: 50.21215565999349 Steps: 11520/12117\n",
            "loss: 50.213412693568635 Steps: 11648/12117\n",
            "loss: 50.23753833770752 Steps: 11776/12117\n",
            "loss: 50.22138895014281 Steps: 11904/12117\n",
            "loss: 50.230580918332365 Steps: 12032/12117\n",
            "loss: 75.3676446750511 Steps: 12117/12117\n",
            "Epoch: 26  || Loss: 50.22643647363519\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            " 33%|███▎      | 27/81 [3:38:39<7:15:04, 483.41s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 50.436275482177734 Steps: 128/12117\n",
            "loss: 51.27895545959473 Steps: 256/12117\n",
            "loss: 50.72018814086914 Steps: 384/12117\n",
            "loss: 50.452176094055176 Steps: 512/12117\n",
            "loss: 50.02598648071289 Steps: 640/12117\n",
            "loss: 50.05006090799967 Steps: 768/12117\n",
            "loss: 49.981743403843474 Steps: 896/12117\n",
            "loss: 49.84634351730347 Steps: 1024/12117\n",
            "loss: 49.87387212117513 Steps: 1152/12117\n",
            "loss: 49.974727630615234 Steps: 1280/12117\n",
            "loss: 49.95232079245827 Steps: 1408/12117\n",
            "loss: 49.891844749450684 Steps: 1536/12117\n",
            "loss: 49.930841592641976 Steps: 1664/12117\n",
            "loss: 50.09318733215332 Steps: 1792/12117\n",
            "loss: 50.18232905069987 Steps: 1920/12117\n",
            "loss: 50.23295569419861 Steps: 2048/12117\n",
            "loss: 50.28368018655216 Steps: 2176/12117\n",
            "loss: 50.32345305548774 Steps: 2304/12117\n",
            "loss: 50.25975638941715 Steps: 2432/12117\n",
            "loss: 50.26215591430664 Steps: 2560/12117\n",
            "loss: 50.218303862072176 Steps: 2688/12117\n",
            "loss: 50.20534844831987 Steps: 2816/12117\n",
            "loss: 50.180758600649625 Steps: 2944/12117\n",
            "loss: 50.08353519439697 Steps: 3072/12117\n",
            "loss: 50.06067367553711 Steps: 3200/12117\n",
            "loss: 50.096439801729645 Steps: 3328/12117\n",
            "loss: 50.10086229112413 Steps: 3456/12117\n",
            "loss: 50.10612051827567 Steps: 3584/12117\n",
            "loss: 50.11278639168575 Steps: 3712/12117\n",
            "loss: 50.122426478068036 Steps: 3840/12117\n",
            "loss: 50.12738517022902 Steps: 3968/12117\n",
            "loss: 50.07817602157593 Steps: 4096/12117\n",
            "loss: 50.08563706369111 Steps: 4224/12117\n",
            "loss: 50.11557713676901 Steps: 4352/12117\n",
            "loss: 50.14849199567522 Steps: 4480/12117\n",
            "loss: 50.14644463857015 Steps: 4608/12117\n",
            "loss: 50.17020550289669 Steps: 4736/12117\n",
            "loss: 50.16145023546721 Steps: 4864/12117\n",
            "loss: 50.173248682266625 Steps: 4992/12117\n",
            "loss: 50.20451231002808 Steps: 5120/12117\n",
            "loss: 50.24136882875024 Steps: 5248/12117\n",
            "loss: 50.24879537309919 Steps: 5376/12117\n",
            "loss: 50.26768289610397 Steps: 5504/12117\n",
            "loss: 50.26438886469061 Steps: 5632/12117\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-fb7427b0645c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Finish training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-fb7427b0645c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloaders, criterion, optimizer, device, output_dir_name, num_epochs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;31m# Training phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mtrn_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0mtrn_losses\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrn_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch: {}  || Loss: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-fb7427b0645c>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(epoch, model, trn_dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0msaving_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlandmark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrn_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 971\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: Caught IndexError in DataLoader worker process 4.\nOriginal Traceback (most recent call last):\n  File \"<ipython-input-31-82c2cf84849c>\", line 90, in __getitem__\n    for i in list(landmark[0].keys()):\nIndexError: list index out of range\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/worker.py\", line 185, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-31-82c2cf84849c>\", line 95, in __getitem__\n    return self[randint(0, len(self))]\n  File \"<ipython-input-31-82c2cf84849c>\", line 53, in __getitem__\n    image_p = self.images[idx]\nIndexError: list index out of range\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNNWVCZRrq-A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_dir='/content/drive/My Drive/Speech2Face/'\n",
        "face_encoder_path = join(base_dir, \"Pretrained\", \"vgg_face_dag.pth\")\n",
        "face_encoder_model = vgg_face_dag(face_encoder_path)\n",
        "face_encoder_model.eval()\n",
        "face_encoder_model.to(device)\n",
        "\n",
        "color_mean, color_std = face_encoder_model.meta[\"mean\"], face_encoder_model.meta[\"std\"]\n",
        "color_mean = [tmp / 255.0 for tmp in color_mean]\n",
        "color_std = [tmp / 255.0 for tmp in color_std]\n",
        "model.eval()\n",
        "#img = Image.open('/content/338x338-10-graziano-frontal.png')\n",
        "#img = Image.open('/content/1966c850f1620718bc846793a0a0d78f.jpg')\n",
        "img = Image.open('jolie.jpg')\n",
        "transform_fe = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            _Normalize_Tensor(color_mean, color_std)\n",
        "        ])\n",
        "\n",
        "\n",
        "t_img = transform_fe(img).unsqueeze(0).to(device)\n",
        "emb = face_encoder_model(t_img)\n",
        "land_out, outputs  = model(emb)\n",
        "display(img)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXzOID98Agc5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "o_img = cv2.cvtColor(np.einsum('abc->bca',outputs[0].cpu().detach().numpy()*255),cv2.COLOR_BGR2RGB)\n",
        "cv2_imshow(o_img)\n",
        "\n",
        "outLandmark = land_out.cpu().detach().numpy()\n",
        "# outLandmark = np.dstack((outLandmark[:,0::2],outLandmark[:,1::2]))\n",
        "print(img.size)\n",
        "print(t_img.shape)\n",
        "\n",
        "outLandmark = outLandmark.reshape((-1, 72, 2))\n",
        "#outLandmark = np.interp(outLandmark, (outLandmark.min(), outLandmark.max()), (0, 224))\n",
        "\n",
        "\n",
        "# print(\"land np img np \", outL_.shape, img_.shape)\n",
        "img1 = np.einsum('dabc->dbca',outputs.cpu().detach().numpy()*255)\n",
        "img1 = (img1).astype(np.uint8)\n",
        "# print(\"img_t \",img_t.shape, img_t[0])\n",
        "src, flag = face_landmark(img1)\n",
        "if flag:\n",
        "    for r in flag:\n",
        "        src[r] = outLandmark[r] \n",
        "\n",
        "diff = np.median(outLandmark, axis=1) - np.median(src, axis=1)\n",
        "\n",
        "right_list = set()\n",
        "for i in range(outLandmark.shape[1]):\n",
        "  if not (outLandmark[0,i,0] == 0 or outLandmark[0,i,1] == 0) :\n",
        "    right_list.add(i)\n",
        "\n",
        "good_list = np.array(list(right_list))\n",
        "src_new = src[:,good_list]\n",
        "outLandmark_new = outLandmark[:,good_list]\n",
        "#outLandmark_new[:,:] -= diff\n",
        "result = image_warping(img1.astype(np.float32), src_new.astype(np.float32), outLandmark_new.astype(np.float32))\n",
        "w_img = cv2.cvtColor(result[0], cv2.COLOR_BGR2RGB)\n",
        "cv2_imshow(w_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnQkReH2dR2w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plotLandmarks(img, landmarks):\n",
        "  img_p = img.squeeze(0)\n",
        "  fig = plt.figure(figsize=(15, 5))\n",
        "  ax = fig.add_subplot(1, 3, 1)\n",
        "  ax.imshow(img_p)\n",
        "  ax = fig.add_subplot(1, 3, 2)\n",
        "  ax.scatter(landmarks[:,0], -landmarks[:,1], alpha=0.8)\n",
        "  ax = fig.add_subplot(1, 3, 3)\n",
        "  img2 = img_p.copy()\n",
        "\n",
        "  for p in landmarks[:].astype(np.uint8):\n",
        "      img2[p[1]-2:p[1]+2, p[0]-2:p[0]+2, :] = (255, 255, 255)\n",
        "      # note that the values -3 and +3 will make the landmarks\n",
        "      # overlayed on the image 6 pixels wide; depending on the\n",
        "      # resolution of the face image, you may want to change\n",
        "      # this value\n",
        "\n",
        "\n",
        "  ax.imshow(img2)\n",
        "  plt.show()\n",
        "\n",
        "plotLandmarks(img1, src.squeeze())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vno3gdM0Pr0r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "im = Image.fromarray(img1[0])\n",
        "im = im.resize((100,100))\n",
        "\n",
        "img1 = np.array(im).reshape((-1, 100, 100, 3))\n",
        "plotLandmarks(img1, outLandmark_new[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZVLaMrGZb19",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "im = Image.fromarray(np.uint8(o_img))\n",
        "b, g, r = im.split()\n",
        "im = Image.merge(\"RGB\", (r, g, b))\n",
        "\n",
        "im.save(\"/content/test/jolie.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0lfYwDma1UL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp donna.png /content/test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N2NhDOeeOfP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/hieubkset/Pytorch-Image-Deblurring.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyoiFg_XeTrc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -r /content/Pytorch-Image-Deblurring/requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2UZKzCueg7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python /content/Pytorch-Image-Deblurring/demo.py --gpu 0 --train_dir /content/Pytorch-Image-Deblurring/pretrained --exp_name multi_skip --image /content/test/jolie.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPFKj6VPhBHX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}