{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Speech2Face.ipynb",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antoniomuso/speech2face/blob/master/Speech2Face_newDataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Us6ergKlbJbh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nkKE256supL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip3 install face_recognition\n",
        "#! pip install --upgrade wandb\n",
        "#! wandb login f9cd4b35bf9733e5ead9d2b06e13ef2259b1284e"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5FJW1pc-YZw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import wandb\n",
        "#wandb.init(project=\"speech2face\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jXRWARmcebB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# path = \"/content/drive/My Drive/Speech2Face/vox\"\n",
        "# !curl --user voxceleb1912:0s42xuw6 -o \"/content/drive/My Drive/Speech2Face/ff/vox.zip\" http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_test_wav.zip\n",
        "\n",
        "\n",
        "\n",
        "! cp \"/content/drive/My Drive/Speech2Face/vox1_dataset/wav_filtered_20_per_actor.zip\" .\n",
        "#! cp \"/content/drive/My Drive/Speech2Face/zippedFaces.tar.gz\" /content\n",
        "! cp \"/content/drive/My Drive/Speech2Face/vox1_dataset/vox1_meta.csv\" .\n",
        "! cp \"/content/drive/My Drive/Speech2Face/face_features_10_per_actor.zip\" .\n",
        "! cp \"/content/drive/My Drive/Speech2Face/vox1_dataset/vox_audios/vox.zip\" .\n",
        "\n",
        "# ! tar zxvf zippedFaces.tar.gz\n",
        "! unzip face_features_10_per_actor.zip\n",
        "! unzip wav_filtered_20_per_actor.zip\n",
        "! unzip vox.zip\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOqMHNfkwaCL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from os import listdir\n",
        "from os.path import join\n",
        "from torch.utils.data import Dataset\n",
        "import glob\n",
        "import itertools\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from typing import Optional, Callable\n",
        "from tqdm.notebook import tqdm, trange\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "device = 'cuda'\n",
        "vgg_weights_path = '/content/drive/My Drive/Speech2Face/Pretrained/vgg_face_dag.pth'\n",
        "face_decoder_weights_path = ''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZEnesbYDawu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reproducibility stuff\n",
        "\n",
        "import random\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXILePzTdsI_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##################### DEPENDECIES ###########################\n",
        "class Vgg_face_dag(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Vgg_face_dag, self).__init__()\n",
        "        self.meta = {'mean': [129.186279296875, 104.76238250732422, 93.59396362304688],\n",
        "                     'std': [1, 1, 1],\n",
        "                     'imageSize': [224, 224, 3]}\n",
        "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu1_1 = nn.ReLU(inplace=True)\n",
        "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu1_2 = nn.ReLU(inplace=True)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
        "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu2_1 = nn.ReLU(inplace=True)\n",
        "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu2_2 = nn.ReLU(inplace=True)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
        "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu3_1 = nn.ReLU(inplace=True)\n",
        "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu3_2 = nn.ReLU(inplace=True)\n",
        "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu3_3 = nn.ReLU(inplace=True)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
        "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu4_1 = nn.ReLU(inplace=True)\n",
        "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu4_2 = nn.ReLU(inplace=True)\n",
        "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu4_3 = nn.ReLU(inplace=True)\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
        "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu5_1 = nn.ReLU(inplace=True)\n",
        "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu5_2 = nn.ReLU(inplace=True)\n",
        "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu5_3 = nn.ReLU(inplace=True)\n",
        "        self.pool5 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
        "        self.fc6 = nn.Linear(in_features=25088, out_features=4096, bias=True)\n",
        "        self.relu6 = nn.ReLU(inplace=True)\n",
        "        self.dropout6 = nn.Dropout(p=0.5)\n",
        "        self.fc7 = nn.Linear(in_features=4096, out_features=4096, bias=True)\n",
        "        self.relu7 = nn.ReLU(inplace=True)\n",
        "        self.dropout7 = nn.Dropout(p=0.5)\n",
        "        self.fc8 = nn.Linear(in_features=4096, out_features=2622, bias=True)\n",
        "\n",
        "    def forward(self, x0, is_fc8=False):\n",
        "      if not is_fc8:\n",
        "        x1 = self.conv1_1(x0)\n",
        "        x2 = self.relu1_1(x1)\n",
        "        x3 = self.conv1_2(x2)\n",
        "        x4 = self.relu1_2(x3)\n",
        "        x5 = self.pool1(x4)\n",
        "        x6 = self.conv2_1(x5)\n",
        "        x7 = self.relu2_1(x6)\n",
        "        x8 = self.conv2_2(x7)\n",
        "        x9 = self.relu2_2(x8)\n",
        "        x10 = self.pool2(x9)\n",
        "        x11 = self.conv3_1(x10)\n",
        "        x12 = self.relu3_1(x11)\n",
        "        x13 = self.conv3_2(x12)\n",
        "        x14 = self.relu3_2(x13)\n",
        "        x15 = self.conv3_3(x14)\n",
        "        x16 = self.relu3_3(x15)\n",
        "        x17 = self.pool3(x16)\n",
        "        x18 = self.conv4_1(x17)\n",
        "        x19 = self.relu4_1(x18)\n",
        "        x20 = self.conv4_2(x19)\n",
        "        x21 = self.relu4_2(x20)\n",
        "        x22 = self.conv4_3(x21)\n",
        "        x23 = self.relu4_3(x22)\n",
        "        x24 = self.pool4(x23)\n",
        "        x25 = self.conv5_1(x24)\n",
        "        x26 = self.relu5_1(x25)\n",
        "        x27 = self.conv5_2(x26)\n",
        "        x28 = self.relu5_2(x27)\n",
        "        x29 = self.conv5_3(x28)\n",
        "        x30 = self.relu5_3(x29)\n",
        "        x31_preflatten = self.pool5(x30)\n",
        "        x31 = x31_preflatten.view(x31_preflatten.size(0), -1)\n",
        "        x32 = self.fc6(x31)\n",
        "        x33 = self.relu6(x32)\n",
        "        x34 = self.dropout6(x33)\n",
        "        x35 = self.fc7(x34)\n",
        "        x36 = self.relu7(x35)\n",
        "        x37 = self.dropout7(x36)\n",
        "        x38 = x37\n",
        "      else:\n",
        "        x38 = self.fc8(x0)\n",
        "\n",
        "      return x38\n",
        "\n",
        "\n",
        "def vgg_face_dag(weights_path=None, **kwargs):\n",
        "    \"\"\"\n",
        "    load imported model instance\n",
        "    Args:\n",
        "        weights_path (str): If set, loads model weights from the given path\n",
        "    \"\"\"\n",
        "    model = Vgg_face_dag()\n",
        "    if weights_path:\n",
        "        state_dict = torch.load(weights_path)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrYW-RmTyS9M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### DEPENDENCY - standard DECODER (w/o warping) ##########################\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, n_hidden, bottom_width=4, channels=512):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.bottom_width = bottom_width\n",
        "\n",
        "        self.linear = nn.Linear(n_hidden, bottom_width*bottom_width*channels)\n",
        "        self.dconv1 = nn.ConvTranspose2d(channels, channels // 2, 4, 2, 1)\n",
        "        self.dconv2 = nn.ConvTranspose2d(channels // 2, channels // 4, 4, 2, 1)\n",
        "        self.dconv3 = nn.ConvTranspose2d(channels // 4, channels // 8, 4, 2, 1)\n",
        "        self.dconv4 = nn.ConvTranspose2d(channels // 8, 3, 4, 2, 1)\n",
        "\n",
        "        self.bn0 = nn.BatchNorm1d(bottom_width*bottom_width*channels)\n",
        "        self.bn1 = nn.BatchNorm2d(channels // 2)\n",
        "        self.bn2 = nn.BatchNorm2d(channels // 4)\n",
        "        self.bn3 = nn.BatchNorm2d(channels // 8)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn0(self.linear(x))).view(-1, self.channels, self.bottom_width, self.bottom_width)\n",
        "        x = F.relu(self.bn1(self.dconv1(x)))\n",
        "        x = F.relu(self.bn2(self.dconv2(x)))\n",
        "        x = F.relu(self.bn3(self.dconv3(x)))\n",
        "\n",
        "        x = torch.sigmoid(self.dconv4(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "def decoder(weights_path=\"/content/drive/My Drive/Speech2Face/models/face_decoder/epoch_3_steps_12800.pth\", fc3_only=False, **kwargs):\n",
        "    \"\"\"\n",
        "    load imported model instance\n",
        "    Args:\n",
        "        weights_path (str): If set, loads model weights from the given path\n",
        "    \"\"\"\n",
        "    model = Decoder(4096)\n",
        "    if weights_path:\n",
        "        state_dict = torch.load(weights_path)[\"model_state_dict\"]\n",
        "        model.load_state_dict(state_dict)\n",
        "        \n",
        "        if fc3_only:\n",
        "          fc3_layer = nn.Sequential(list(model.children())[0])\n",
        "          for param in fc3_layer.parameters():\n",
        "            param.requires_grad = False\n",
        "          #print(fc3_layer)\n",
        "          return fc3_layer        \n",
        "\n",
        "    return model\n",
        "\n",
        "# dec_fc3 = decoder(fc3_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UhlsETN5_3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### DEPENDENCY - DECODER w/ warping ##########################\n",
        "\n",
        "class DECODER(nn.Module):\n",
        "    def __init__(self, phase):\n",
        "        super(DECODER, self).__init__()\n",
        "        self.phase = phase\n",
        "        self.fc3 = nn.Linear(4096, 1000)\n",
        "        self.ReLU = nn.ReLU()\n",
        "        #self.fc_bn3 = nn.BatchNorm1d(1000)\n",
        "\n",
        "\n",
        "        self.fc4 = nn.Linear(1000, 14 * 14 * 256)\n",
        "        self.fc_bn4 = nn.BatchNorm1d(14 * 14 * 256)\n",
        "        def TransConv( i, kernal = 5, stride = 2, inp = None):\n",
        "            if not inp:\n",
        "                inp = max(256//2**(i-1), 32)\n",
        "\n",
        "            layer =  nn.Sequential(\n",
        "                nn.ConvTranspose2d(inp, max(256//2**i, 32), \n",
        "                                kernal, stride=stride, padding=2, output_padding=1, \n",
        "                                dilation=1, padding_mode='zeros'),\n",
        "                nn.ReLU(),\n",
        "                nn.BatchNorm2d(max(256//2**i, 32)))\n",
        "            return layer\n",
        "        self.T1_ = TransConv(1, inp = 256)\n",
        "        self.T2_ = TransConv(2)\n",
        "        self.T3_ = TransConv(3)\n",
        "        self.T4_ = TransConv(4)\n",
        "    \n",
        "        self.ConvLast = nn.Sequential(\n",
        "            nn.Conv2d(32, 3, (1,1), stride=1),\n",
        "            nn.BatchNorm2d(3),\n",
        "            nn.ReLU())\n",
        "\n",
        "\n",
        "        self.layerLandmark1 = nn.Linear(1000, 800)\n",
        "        self.layerLandmark2 = nn.Linear(800, 600)\n",
        "        self.layerLandmark3 = nn.Linear(600, 400)\n",
        "        self.layerLandmark4 = nn.Linear(400, 200)\n",
        "        self.layerLandmark5 = nn.Linear(200, 144)\n",
        "\n",
        "    def forward(self, x):\n",
        "        L1 = self.fc3(x)\n",
        "        L1 = self.ReLU(L1)\n",
        "\n",
        "\n",
        "        L2 = self.layerLandmark1(L1)\n",
        "        L2 = self.ReLU(L2)\n",
        "\n",
        "        L3 = self.layerLandmark2(L2)\n",
        "        L3 = self.ReLU(L3)\n",
        "\n",
        "        L4 = self.layerLandmark3(L3)\n",
        "        L4 = self.ReLU(L4)\n",
        "\n",
        "        L5 = self.layerLandmark4(L4)\n",
        "        L5 = self.ReLU(L5)\n",
        "\n",
        "        L6 = self.layerLandmark5(L5)\n",
        "        outL = self.ReLU(L6)\n",
        "\n",
        "\n",
        "        # B1 = self.fc_bn3(L1) \n",
        "        T0 = self.fc4(L1) \n",
        "        T0 = self.ReLU(T0)\n",
        "        # T0 = self.fc_bn4(T0)\n",
        "        T0 = T0.view(-1,256,14,14)\n",
        "\n",
        "\n",
        "\n",
        "        T1 = self.T1_(T0)\n",
        "        T2 = self.T2_(T1)\n",
        "        T3 = self.T3_(T2)\n",
        "        T4 = self.T4_(T3)\n",
        "\n",
        "        outT = self.ConvLast(T4)\n",
        "        if self.phase == \"train\":\n",
        "            return outL,  outT\n",
        "        elif self.phase == \"eval\":\n",
        "            return outT\n",
        "\n",
        "def decoder_warping(weights_path=\"/content/drive/My Drive/Speech2Face/models/face_decoder_warping/new_dataset_v4/epoch_94_steps_0.pth\", fc3_only=False, **kwargs):\n",
        "    \"\"\"\n",
        "    load imported model instance\n",
        "    Args:\n",
        "        weights_path (str): If set, loads model weights from the given path\n",
        "    \"\"\"\n",
        "    model = DECODER('eval')\n",
        "    if weights_path:\n",
        "        state_dict = torch.load(weights_path)[\"model_state_dict\"]\n",
        "        model.load_state_dict(state_dict)\n",
        "\n",
        "        if fc3_only:\n",
        "          fc3_layer = nn.Sequential(list(model.children())[0])\n",
        "          for param in fc3_layer.parameters():\n",
        "            param.requires_grad = False\n",
        "          #print(fc3_layer)\n",
        "          return fc3_layer\n",
        "\n",
        "    return model\n",
        "\n",
        "# dec_fc3 = decoder_warping(fc3_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bl1vNDOGpKnx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_wav(wav_path):\n",
        "  def adjust(stft):\n",
        "    if stft.shape[1] == 601:\n",
        "      return stft\n",
        "    else:\n",
        "      return np.concatenate((stft,stft[:,0:601 - stft.shape[1]]),axis = 1)\n",
        "\n",
        "  wav, sr = librosa.load(wav_path,sr = 16000, duration = 6.0 ,mono = True)\n",
        "  spectro = librosa.core.stft(wav, n_fft = 512, hop_length = int(np.ceil(0.01 * sr)),win_length = int(np.ceil(0.025 * sr)) , window='hann', center=True,pad_mode='reflect') \n",
        "  spectroComplex = adjust(spectro)\n",
        "  converted = np.zeros((spectroComplex.shape[0], spectroComplex.shape[1], 2))\n",
        "  i = np.arange(spectroComplex.shape[0])\n",
        "  j = np.arange(spectroComplex.shape[1])\n",
        "  \n",
        "  converted[i,j[:,np.newaxis], 0] = spectroComplex[i,j[:,np.newaxis]].real\n",
        "  converted[i,j[:,np.newaxis], 1] = spectroComplex[i,j[:,np.newaxis]].imag\n",
        "\n",
        "  return converted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoNPrJNGDfY7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_map_person2paths(path, format='wav'):\n",
        "  actor2data = dict()\n",
        "  \n",
        "  for person in listdir(path):\n",
        "    n_path = join(path, person)\n",
        "    files = glob.glob(n_path + '/**/*.'+format, recursive=True)\n",
        "    actor2data[person] = files\n",
        "  \n",
        "  return actor2data\n",
        "\n",
        "def get_map_person2paths_new_dataset(path):\n",
        "  actor2data = dict()\n",
        "  images_path = join(path, 'Faces')\n",
        "  for image in listdir(images_path):\n",
        "    person = '_'.join(image.split('_')[:-1])\n",
        "    # idx = int(image.split('_')[-1].split('.')[0])\n",
        "    if not person in actor2data.keys():\n",
        "      actor2data[person] = []\n",
        "    actor2data[person] += [join(images_path, image)]\n",
        "  \n",
        "  return actor2data\n",
        "\n",
        "\n",
        "def load_metadata(path):\n",
        "  meta = pd.read_csv(path,sep='\\t')\n",
        "\n",
        "  meta = meta.drop('Gender',axis=1)\n",
        "  meta = meta.drop('Nationality',axis=1)\n",
        "  meta = meta.drop('Set',axis=1)\n",
        "  return meta\n",
        "\n",
        "def couple_data(voice_map, face_map, meta):\n",
        "  count = 0\n",
        "  out = []\n",
        "  for index, row in meta.iterrows():\n",
        "    if (row['VoxCeleb1 ID'] not in voice_map.keys()) or (row['VGGFace1 ID'] not in face_map.keys()):\n",
        "      count += 1\n",
        "      continue\n",
        "    # max(len(voice_map[row['VoxCeleb1 ID']]), face_map[row['VGGFace1 ID']])\n",
        "    coupled = list(zip(voice_map[row['VoxCeleb1 ID']], face_map[row['VGGFace1 ID']]))\n",
        "    out += coupled\n",
        "  \n",
        "  print(\"elements not found:\", count)\n",
        "  return out\n",
        "\n",
        "def create_coupled_list(path_voices, path_faces, metaP):\n",
        "  voice_map = get_map_person2paths(path_voices)\n",
        "  face_map = get_map_person2paths_new_dataset(path_faces)\n",
        "  meta = load_metadata(metaP)\n",
        "  return couple_data(voice_map, face_map, meta)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zZbwKKW-riA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class _Normalize_Tensor(object):\n",
        "    def __init__(self, color_mean, color_std):\n",
        "        self.color_mean = color_mean\n",
        "        self.color_std = color_std\n",
        "\n",
        "    def __call__(self, img):\n",
        "        # Convert image to Tensor\n",
        "        img = transforms.functional.to_tensor(img)\n",
        "\n",
        "        # Normalize image by the parameter of pre-trained face-encoder\n",
        "        img = transforms.functional.normalize(\n",
        "            img, self.color_mean, self.color_std)\n",
        "\n",
        "        return img\n",
        "\n",
        "class Speech2FaceDataset(Dataset):\n",
        "  def __init__(self, path_voices, path_faces, metaP, size=224):\n",
        "        super().__init__()\n",
        "        self.path_voices = path_voices\n",
        "        self.path_faces = path_faces\n",
        "        self.size = size\n",
        "        self.coupled_list = create_coupled_list(path_voices, path_faces, metaP)\n",
        "        self.len = len(self.coupled_list)\n",
        "        self.features = np.load(join(path_faces,'facefeature.npy'))\n",
        "        self.features_fc8 = np.load(join(path_faces,'facefeature_fc8.npy'))\n",
        "\n",
        "        self.transform_fd = transforms.Compose([\n",
        "            transforms.Resize((self.size, self.size)),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "  \n",
        "  def __len__(self):\n",
        "        return self.len\n",
        "    \n",
        "  def __getitem__(self, idx):\n",
        "        wav_p, img_p = self.coupled_list[idx]\n",
        "        wav = load_wav(wav_p)\n",
        "        img = Image.open(img_p)\n",
        "        idx_feature = int(img_p.split('_')[-1].split('.')[0])\n",
        "\n",
        "        \n",
        "        #face_loc = face_locations(np_image)\n",
        "        # img_normalized = self.transform_fe(img)\n",
        "        image_preprocessed = self.transform_fd(img)\n",
        "        \n",
        "        return (torch.tensor(self.features[idx_feature]).float(),\n",
        "                torch.tensor(wav).reshape(2,257,601).float()), (torch.tensor(self.features_fc8[idx_feature]).float() , image_preprocessed)\n",
        "\n",
        "\n",
        "\n",
        "color_mean = [129.186279296875, 104.76238250732422, 93.59396362304688]\n",
        "color_std = [1, 1, 1]   \n",
        "color_mean = [tmp / 255.0 for tmp in color_mean]\n",
        "color_std = [tmp / 255.0 for tmp in color_std]\n",
        "\n",
        "\n",
        "data = Speech2FaceDataset('wav_filtered_20_per_actor', 'Face_Feature','vox1_meta.csv')\n",
        "data_test = Speech2FaceDataset('wav', 'Face_Feature','vox1_meta.csv')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgMzo1Qov37x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5sDbBkIa7Fm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SpeechEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(SpeechEncoder, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(2, 64, kernel_size=4,stride=1) \n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=4,stride=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4,stride=1) \n",
        "        self.pooling1 = nn.MaxPool2d(kernel_size=(2,1), stride=(2,1))\n",
        "        self.conv4 = nn.Conv2d(128, 128, kernel_size=4,stride=1) \n",
        "        self.pooling2 = nn.MaxPool2d(kernel_size=(2,1), stride=(2,1))\n",
        "        self.conv5 = nn.Conv2d(128, 128, kernel_size=4,stride=1) \n",
        "        self.pooling3 = nn.MaxPool2d(kernel_size=(2,1), stride=(2,1))\n",
        "        self.conv6 = nn.Conv2d(128, 256, kernel_size=4,stride=1) \n",
        "        self.pooling4 = nn.MaxPool2d(kernel_size=(2,1), stride=(2,1))\n",
        "        self.conv7 = nn.Conv2d(256, 512, kernel_size=4,stride=1) \n",
        "        self.conv8 = nn.Conv2d(512, 512, kernel_size=4,stride=2) \n",
        "\n",
        "        self.conv9 = nn.Conv2d(512, 512, kernel_size=3,stride=2) # Queste due celle sono diverse\n",
        "        self.pooling5 = nn.AvgPool2d(kernel_size=(1,1), stride=1)# Queste due celle sono diverse\n",
        "\n",
        "        self.fc1 = nn.Linear(512 * 1 * 144, 4096)\n",
        "        self.fc2 = nn.Linear(4096, 4096)\n",
        "\n",
        "        self.batch_norm1 = nn.BatchNorm2d(64)\n",
        "        self.batch_norm2 = nn.BatchNorm2d(64)\n",
        "        self.batch_norm3 = nn.BatchNorm2d(128)\n",
        "        self.batch_norm4 = nn.BatchNorm2d(128)\n",
        "        self.batch_norm5 = nn.BatchNorm2d(128)\n",
        "        self.batch_norm6 = nn.BatchNorm2d(256)\n",
        "        self.batch_norm7 = nn.BatchNorm2d(512)\n",
        "        self.batch_norm8 = nn.BatchNorm2d(512)\n",
        "        self.batch_norm9 = nn.BatchNorm2d(512)\n",
        "      \n",
        "\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.batch_norm1(self.relu(self.conv1(x)))\n",
        "        out = self.batch_norm2(self.relu(self.conv2(out)))\n",
        "        out = self.batch_norm3(self.relu(self.conv3(out)))\n",
        "        out = self.pooling1(out)\n",
        "        out = self.batch_norm4(self.relu(self.conv4(out)))\n",
        "        out = self.pooling2(out)\n",
        "        out = self.batch_norm5(self.relu(self.conv5(out)))\n",
        "        out = self.pooling3(out)\n",
        "        out = self.batch_norm6(self.relu(self.conv6(out)))\n",
        "        out = self.pooling4(out)\n",
        "        out = self.batch_norm7(self.relu(self.conv7(out)))\n",
        "        out = self.batch_norm8(self.relu(self.conv8(out)))\n",
        "        out = self.batch_norm9(self.relu(self.pooling5(self.conv9(out))))\n",
        "\n",
        "        batch = out.shape[0]\n",
        "        out = out.view((batch, 512 * 1 * 144))\n",
        "        out = self.relu(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "def speech_encoder(weights_path=\"/content/drive/My Drive/Speech2Face/models/speech_encoder/adam/adam_epoch_9.pth\", **kwargs):\n",
        "    \"\"\"\n",
        "    load imported model instance\n",
        "    Args:\n",
        "        weights_path (str): If set, loads model weights from the given path\n",
        "    \"\"\"\n",
        "    model = SpeechEncoder()\n",
        "    if weights_path:\n",
        "        state_dict = torch.load(weights_path)[\"model_state_dict\"]\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wE_p5eTAw0HD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vgg = vgg_face_dag(vgg_weights_path)\n",
        "vgg.eval()\n",
        "vgg_fc8 = vgg.fc8\n",
        "vgg_fc8.requires_grad = False\n",
        "vgg_fc8.to(device)\n",
        "\n",
        "# vgg = vgg.to(device)\n",
        "\n",
        "\n",
        "model = SpeechEncoder()\n",
        "model.to(device)\n",
        "\n",
        "decoder = decoder_warping(fc3_only=True)\n",
        "decoder.to(device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), eps=1e-04, betas=(0.5, 0.999))    #, weight_decay=0.95)\n",
        "#optimizer_decay = torch.optim.AdamW(model.parameters(), eps=1e-04, betas=(0.5, 0.999))    #, weight_decay=0.95)\n",
        "#optimizer = optimizer_decay\n",
        "\n",
        "datal = DataLoader(data, 6, True, num_workers=16)\n",
        "datal_test = DataLoader(data_test, 6, False, num_workers=16)\n",
        "# W&B\n",
        "#wandb.watch(model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "082fWCi0rJG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _save_model(epoch, model, optimizer, output_dir_name=\"/content/drive/My Drive/Speech2Face/models/speech_encoder/\"):\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        }, join(output_dir_name, 'adam_epoch_{}.pth'.format(epoch)))\n",
        "    \n",
        "def make_averager() -> Callable[[Optional[float]], float]:\n",
        "    \"\"\" Returns a function that maintains a running average\n",
        "\n",
        "    :returns: running average function\n",
        "    \"\"\"\n",
        "    count = 0\n",
        "    total = 0\n",
        "\n",
        "    def averager(new_value: Optional[float]) -> float:\n",
        "        \"\"\" Running averager\n",
        "\n",
        "        :param new_value: number to add to the running average,\n",
        "                          if None returns the current average\n",
        "        :returns: the current average\n",
        "        \"\"\"\n",
        "        nonlocal count, total\n",
        "        if new_value is None:\n",
        "            return total / count if count else float(\"nan\")\n",
        "        count += 1\n",
        "        total += new_value\n",
        "        return total / count\n",
        "\n",
        "    return averager\n",
        "\n",
        "def fit(\n",
        "    epochs: int,\n",
        "    train_dl: torch.utils.data.DataLoader,\n",
        "    test_dl: torch.utils.data.DataLoader,\n",
        "    model: torch.nn.Module,\n",
        "    VGG: torch.nn.Module,\n",
        "    decoder: torch.nn.Module,\n",
        "    opt: torch.optim.Optimizer,\n",
        "    tag: str,\n",
        "    device: str = \"cuda\",\n",
        "    restart_epoch: int = 0,\n",
        ") -> float:\n",
        "    \"\"\" Train the model and computes metrics on the test_loader at each epoch\n",
        "\n",
        "    :param epochs: number of epochs\n",
        "    :param train_dl: the train dataloader\n",
        "    :param test_dl: the test dataloader\n",
        "    :param model: the model to train\n",
        "    :param opt: the optimizer to use to train the model\n",
        "    :param tag: description of the current model\n",
        "    :param perm: if not None, permute the pixel in each image according to perm\n",
        "\n",
        "    :returns: accucary on the test set in the last epoch\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"VGG.training = \", VGG.training)\n",
        "    print(\"Speech2Face.training = \", model.training)\n",
        "\n",
        "    print(\"Starting training. Restart epoch:\", restart_epoch)\n",
        "\n",
        "    lambda1 = 0.025\n",
        "    lambda2 = 200.0\n",
        "\n",
        "    for epoch in trange(epochs, desc=\"train epoch\"):\n",
        "        if restart_epoch != 0:\n",
        "          epoch = restart_epoch\n",
        "          restart_epoch = 0\n",
        "          print(\"Epoch updated, current value:\", epoch)\n",
        "\n",
        "        model.train()\n",
        "        train_loss_averager = make_averager()  # mantain a running average of the loss\n",
        "\n",
        "        # TRAIN\n",
        "        tqdm_iterator = tqdm(\n",
        "            enumerate(train_dl),\n",
        "            total=len(train_dl),\n",
        "            desc=f\"batch [loss: None]\",\n",
        "            leave=False,\n",
        "        )\n",
        "        for batch_idx, (x, y) in tqdm_iterator:\n",
        "            embedding, wav = x\n",
        "            features_fc8, img = y\n",
        "            # send to device\n",
        "            wav = wav.to(device)\n",
        "            embedding = embedding.to(device)\n",
        "            features_fc8 = features_fc8.to(device)\n",
        "            # print(wav.shape)\n",
        "\n",
        "            # embedding = VGG(img_normal)# .to(device)\n",
        "\n",
        "            output = model(wav)\n",
        "\n",
        "            # print(output.shape)\n",
        "\n",
        "            fvgg_s = VGG(output)\n",
        "            fvgg_f = features_fc8\n",
        "\n",
        "            fdec_s = decoder(output)\n",
        "            fdec_f = decoder(embedding)\n",
        "            \n",
        "\n",
        "            # loss = F.l1_loss(output, embedding)\n",
        "            loss = F.l1_loss(fdec_f, fdec_s) + lambda1 * loss_2(output, embedding) + lambda2 * dist_loss(fvgg_f, fvgg_s)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "            #print('here')\n",
        "\n",
        "            train_loss_averager(loss.item())\n",
        "\n",
        "            tqdm_iterator.set_description(\n",
        "                f\"train batch [avg loss: {train_loss_averager(None):.3f}]\"\n",
        "            )\n",
        "            tqdm_iterator.refresh()\n",
        "\n",
        "         # TEST\n",
        "         model.eval()\n",
        "         test_loss_averager = make_averager()  # mantain a running average of the loss\n",
        "         correct = 0\n",
        "         for (x, y) in test_dl:\n",
        "             # send to device\n",
        "             embedding, wav = x\n",
        "             features_fc8, img = y\n",
        "             # send to device\n",
        "             wav = wav.to(device)\n",
        "             embedding = embedding.to(device)\n",
        "             features_fc8 = features_fc8.to(device)\n",
        "             # print(wav.shape)\n",
        " \n",
        "             # embedding = VGG(img_normal)# .to(device)\n",
        " \n",
        "             output = model(wav)\n",
        " \n",
        "             # print(output.shape)\n",
        " \n",
        "             fvgg_s = VGG(output)\n",
        "             fvgg_f = features_fc8\n",
        " \n",
        "             fdec_s = decoder(output)\n",
        "             fdec_f = decoder(embedding)\n",
        "             \n",
        "\n",
        "             # loss = F.l1_loss(output, embedding)\n",
        "             loss = F.l1_loss(fdec_f, fdec_s) + lambda1 * loss_2(output, embedding) + lambda2 * dist_loss(fvgg_f, fvgg_s)\n",
        "             test_loss_averager(loss)\n",
        "\n",
        "        print(\n",
        "            f\"Epoch: {epoch}\\n\"\n",
        "            f\"Train set: Average loss: {train_loss_averager(None):.4f}\\n\"\n",
        "            f\"Test set: Average loss: {test_loss_averager(None):.4f}, \"\n",
        "            #f\"Accuracy: {correct}/{len(test_dl.dataset)} ({accuracy:.0f}%)\\n\"\n",
        "        )\n",
        "        _save_model(epoch, model, optimizer)\n",
        "        #torch.save(model.state_dict(), join(wandb.run.dir, 'model.pt'))\n",
        "        #wandb.log({\"Train set Average loss:\": train_loss_averager(None)})\n",
        "    # models_accuracy[tag] = accuracy\n",
        "    # return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEyFPE9NBXYP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# taken from https://discuss.pytorch.org/t/is-this-loss-function-for-knowledge-distillation-correct/69863\n",
        "\n",
        "def dist_loss(t, s):\n",
        "  T = 2\n",
        "  prob_t = F.softmax(t/T, dim=1)\n",
        "  log_prob_s = F.log_softmax(s/T, dim=1)\n",
        "  dist_loss = -(prob_t*log_prob_s).sum(dim=1).mean()\n",
        "  return dist_loss\n",
        "\n",
        "def loss_2(vf, vs):\n",
        "  return F.mse_loss(F.normalize(vf), F.normalize(vs))\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jETsH8Nx7JU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "check_path = None #\"/content/drive/My Drive/Speech2Face/models/speech_encoder/adam_epoch_2.pth\"\n",
        "\n",
        "def _load_checkpoint(checkpoint_path):\n",
        "  checkpoint = torch.load(checkpoint_path)\n",
        "  global_ep = checkpoint[\"epoch\"]\n",
        "  model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "  optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "  print(\"Loaded checkpoint, restart epoch is: \", global_ep)\n",
        "  return global_ep + 1\n",
        "\n",
        "global_ep = 0\n",
        "if check_path is not None:\n",
        "  global_ep = _load_checkpoint(check_path)  \n",
        "\n",
        "fit(10, datal, datal_test, model, vgg_fc8, decoder, optimizer, \"Speech2Face Training\", restart_epoch=global_ep)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTVgjwV0dHL6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "model = SpeechEncoder()\n",
        "input = torch.unsqueeze(torch.tensor(converted).reshape(2,257,601), 0)\n",
        "\n",
        "model(input.type(torch.float32)).shape\n",
        "summary(model, (2,257,601))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBS5g-ckzeV9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Quick testing -- models creation\n",
        "\n",
        "enc = speech_encoder()\n",
        "dec = decoder()\n",
        "dec_w = decoder_warping()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLpG4SnDzzhh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Quick testing -- actual test\n",
        "\n",
        "enc.eval()\n",
        "dec.eval()\n",
        "#dec_w.eval() # Is it needed??\n",
        "\n",
        "test_wav_path = \"/content/drive/My Drive/Speech2Face/vox1_dataset/vox_audios/ext/wav/id10279/3qAxPgeIvCQ/00001.wav\"\n",
        "test_wav = load_wav(test_wav_path)\n",
        "test_wav = torch.tensor(test_wav).reshape(2,257,601).float().unsqueeze(0)\n",
        "\n",
        "test_wav_path2 = \"/content/drive/My Drive/Speech2Face/vox1_dataset/vox_audios/ext/wav/id10277/0rpfN7wThsg/00001.wav\"\n",
        "test_wav2 = load_wav(test_wav_path2)\n",
        "test_wav2 = torch.tensor(test_wav2).reshape(2,257,601).float().unsqueeze(0)\n",
        "\n",
        "print(torch.equal(test_wav, test_wav2))\n",
        "\n",
        "#print(test_wav, test_wav2)\n",
        "\n",
        "\n",
        "out = enc(test_wav)\n",
        "decoded = dec(out)\n",
        "decoded_w = dec_w(out)\n",
        "\n",
        "\n",
        "out2 = enc(test_wav2)\n",
        "decoded2 = dec(out2)\n",
        "decoded_w2 = dec_w(out2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ld7jvncs3IJE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Quick testing -- showing output\n",
        "\n",
        "#1 - Decoder w/ warping\n",
        "o_img_w = cv2.cvtColor(np.einsum('abc->bca',decoded_w[0].detach().numpy()*255), cv2.COLOR_BGR2RGB)\n",
        "cv2_imshow(o_img_w)\n",
        "\n",
        "o_img_w2 = cv2.cvtColor(np.einsum('abc->bca',decoded_w2[0].detach().numpy()*255), cv2.COLOR_BGR2RGB)\n",
        "cv2_imshow(o_img_w2)\n",
        "\n",
        "#2 - Decoder w/o warping\n",
        "o_img = cv2.cvtColor(np.einsum('abc->bca',decoded[0].detach().numpy()*255),cv2.COLOR_BGR2RGB)\n",
        "cv2_imshow(o_img)\n",
        "\n",
        "o_img2 = cv2.cvtColor(np.einsum('abc->bca',decoded2[0].detach().numpy()*255),cv2.COLOR_BGR2RGB)\n",
        "cv2_imshow(o_img2)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}