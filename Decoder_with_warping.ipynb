{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antoniomuso/speech2face/blob/master/Decoder_with_warping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYdMtmJooq6M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6fcc2233-1bc5-4d7e-b4a2-b9d72448371e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yk88JxlKquen",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vox1_path = \"/content/content/Faces_rearranged\"\n",
        "img = vox1_path+\"unzippedFaces\"\n",
        "# ! pip3 install face_recognition\n",
        "# ! pip3 install tensorflow-gpu==1.15"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWWYuEBuoTYg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! cp /content/drive/My\\ Drive/Speech2Face/preprocessed_faces_rearranged.zip .\n",
        "# ! unzip preprocessed_faces_rearranged.zip\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4XC5nS4thFS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !wget http://www.robots.ox.ac.uk/~vgg/research/CMBiometrics/data/zippedFaces.tar.gz \n",
        "# tar zxvf zippedFaces.tar.gz -C \"/content/drive/My Drive/Speech2Face/vox1_dataset/\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eFsvXPivKgI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !curl --user voxceleb1912:0s42xuw6 -o \"/content/drive/My Drive/Speech2Face/ff/vox.zip\" http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_test_wav.zip"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VepCRxxbJlAW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import  torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from face_recognition import face_landmarks, face_locations, face_encodings\n",
        "from random import randint\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "\n",
        "import os\n",
        "from os.path import join\n",
        "\n",
        "from tqdm import tqdm\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rQZIr9MJlR4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reproducibility stuff\n",
        "\n",
        "import random\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(0)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6SgTIZoqwYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_metadata(data_file_path):\n",
        "    data_list = []\n",
        "    with open(data_file_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            data_list.append(line.rstrip(\"\\n\").split(\",\"))\n",
        "    return data_list\n",
        "\n",
        "\n",
        "class _Normalize_Tensor(object):\n",
        "    def __init__(self, color_mean, color_std):\n",
        "        self.color_mean = color_mean\n",
        "        self.color_std = color_std\n",
        "\n",
        "    def __call__(self, img):\n",
        "        # Convert image to Tensor\n",
        "        img = transforms.functional.to_tensor(img)\n",
        "\n",
        "        # Normalize image by the parameter of pre-trained face-encoder\n",
        "        img = transforms.functional.normalize(\n",
        "            img, self.color_mean, self.color_std)\n",
        "\n",
        "        return img\n",
        "\n",
        "class EmbedImagePairs(Dataset):\n",
        "    def __init__(self, data_list_path, face_encoder, mean, std, device, size=64):\n",
        "        super().__init__()\n",
        "        self.images = ImageFolder(data_list_path)\n",
        "\n",
        "        self.face_encoder = face_encoder\n",
        "\n",
        "        self.color_mean = mean\n",
        "        self.color_std = std\n",
        "        \n",
        "        self.device = device\n",
        "        self.size = size\n",
        "\n",
        "        # Transform input for face-encoder\n",
        "        self.transform_fe = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            _Normalize_Tensor(self.color_mean, self.color_std)\n",
        "        ])\n",
        "\n",
        "        self.transform_fd = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor()\n",
        "            \n",
        "        ])\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # Load face-image\n",
        "        image = self.images[idx][0]\n",
        "\n",
        "        # convert face-images into 4096-dim vectors\n",
        "        self.face_encoder.eval()\n",
        "\n",
        "        \n",
        "        # Localize Face\n",
        "        np_image = np.array(image)\n",
        "        #face_loc = face_locations(np_image)\n",
        "        \n",
        "        try:\n",
        "          #x,y1,x1,y = face_loc[0]\n",
        "          #image = image.crop((x,y,x1,y1))\n",
        "\n",
        "          resultant = []\n",
        "          #np_image = np_image[x:x1,y:y1]\n",
        "          landmark = face_landmarks(np_image)\n",
        "          for i in list(landmark[0].keys()):\n",
        "            resultant += landmark[0][i]\n",
        "          landmark = np.ravel(np.array(resultant))\n",
        "\n",
        "        except IndexError:\n",
        "          return self[randint(0, len(self))]\n",
        "\n",
        "        face_vectors = self.face_encoder(\n",
        "            self.transform_fe(image).unsqueeze_(0).to(self.device))\n",
        "\n",
        "        image_ = self.transform_fd(image)\n",
        "\n",
        "        return face_vectors, torch.tensor(landmark).float(), image_"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Io7T0B-dq8eG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Vgg_face_dag(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Vgg_face_dag, self).__init__()\n",
        "        self.meta = {'mean': [129.186279296875, 104.76238250732422, 93.59396362304688],\n",
        "                     'std': [1, 1, 1],\n",
        "                     'imageSize': [224, 224, 3]}\n",
        "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu1_1 = nn.ReLU(inplace=True)\n",
        "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu1_2 = nn.ReLU(inplace=True)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
        "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu2_1 = nn.ReLU(inplace=True)\n",
        "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu2_2 = nn.ReLU(inplace=True)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
        "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu3_1 = nn.ReLU(inplace=True)\n",
        "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu3_2 = nn.ReLU(inplace=True)\n",
        "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu3_3 = nn.ReLU(inplace=True)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
        "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu4_1 = nn.ReLU(inplace=True)\n",
        "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu4_2 = nn.ReLU(inplace=True)\n",
        "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu4_3 = nn.ReLU(inplace=True)\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
        "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu5_1 = nn.ReLU(inplace=True)\n",
        "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu5_2 = nn.ReLU(inplace=True)\n",
        "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu5_3 = nn.ReLU(inplace=True)\n",
        "        self.pool5 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
        "        self.fc6 = nn.Linear(in_features=25088, out_features=4096, bias=True)\n",
        "        self.relu6 = nn.ReLU(inplace=True)\n",
        "        self.dropout6 = nn.Dropout(p=0.5)\n",
        "        self.fc7 = nn.Linear(in_features=4096, out_features=4096, bias=True)\n",
        "        self.relu7 = nn.ReLU(inplace=True)\n",
        "        self.dropout7 = nn.Dropout(p=0.5)\n",
        "        self.fc8 = nn.Linear(in_features=4096, out_features=2622, bias=True)\n",
        "\n",
        "    def forward(self, x0, is_fc8=False):\n",
        "        x1 = self.conv1_1(x0)\n",
        "        x2 = self.relu1_1(x1)\n",
        "        x3 = self.conv1_2(x2)\n",
        "        x4 = self.relu1_2(x3)\n",
        "        x5 = self.pool1(x4)\n",
        "        x6 = self.conv2_1(x5)\n",
        "        x7 = self.relu2_1(x6)\n",
        "        x8 = self.conv2_2(x7)\n",
        "        x9 = self.relu2_2(x8)\n",
        "        x10 = self.pool2(x9)\n",
        "        x11 = self.conv3_1(x10)\n",
        "        x12 = self.relu3_1(x11)\n",
        "        x13 = self.conv3_2(x12)\n",
        "        x14 = self.relu3_2(x13)\n",
        "        x15 = self.conv3_3(x14)\n",
        "        x16 = self.relu3_3(x15)\n",
        "        x17 = self.pool3(x16)\n",
        "        x18 = self.conv4_1(x17)\n",
        "        x19 = self.relu4_1(x18)\n",
        "        x20 = self.conv4_2(x19)\n",
        "        x21 = self.relu4_2(x20)\n",
        "        x22 = self.conv4_3(x21)\n",
        "        x23 = self.relu4_3(x22)\n",
        "        x24 = self.pool4(x23)\n",
        "        x25 = self.conv5_1(x24)\n",
        "        x26 = self.relu5_1(x25)\n",
        "        x27 = self.conv5_2(x26)\n",
        "        x28 = self.relu5_2(x27)\n",
        "        x29 = self.conv5_3(x28)\n",
        "        x30 = self.relu5_3(x29)\n",
        "        x31_preflatten = self.pool5(x30)\n",
        "        x31 = x31_preflatten.view(x31_preflatten.size(0), -1)\n",
        "        x32 = self.fc6(x31)\n",
        "        x33 = self.relu6(x32)\n",
        "        x34 = self.dropout6(x33)\n",
        "        x35 = self.fc7(x34)\n",
        "        x36 = self.relu7(x35)\n",
        "        x37 = self.dropout7(x36)\n",
        "        if is_fc8:\n",
        "            x38 = self.fc8(x37)\n",
        "        else:\n",
        "            x38 = x37\n",
        "        return x38\n",
        "\n",
        "\n",
        "def vgg_face_dag(weights_path=None, **kwargs):\n",
        "    \"\"\"\n",
        "    load imported model instance\n",
        "    Args:\n",
        "        weights_path (str): If set, loads model weights from the given path\n",
        "    \"\"\"\n",
        "    model = Vgg_face_dag()\n",
        "    if weights_path:\n",
        "        state_dict = torch.load(weights_path)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5xUu6b9rBsl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DECODER(nn.Module):\n",
        "    def __init__(self, phase):\n",
        "        super(DECODER, self).__init__()\n",
        "        self.phase = phase\n",
        "        self.fc3 = nn.Linear(4096, 1000)\n",
        "        self.ReLU = nn.ReLU()\n",
        "        #self.fc_bn3 = nn.BatchNorm1d(1000)\n",
        "\n",
        "\n",
        "        self.fc4 = nn.Linear(1000, 14 * 14 * 256)\n",
        "        self.fc_bn4 = nn.BatchNorm1d(14 * 14 * 256)\n",
        "        def TransConv( i, kernal = 5, stride = 2, inp = None):\n",
        "            if not inp:\n",
        "                inp = max(256//2**(i-1), 32)\n",
        "\n",
        "            layer =  nn.Sequential(\n",
        "                nn.ConvTranspose2d(inp, max(256//2**i, 32), \n",
        "                                kernal, stride=stride, padding=2, output_padding=1, \n",
        "                                dilation=1, padding_mode='zeros'),\n",
        "                nn.ReLU(),\n",
        "                nn.BatchNorm2d(max(256//2**i, 32)))\n",
        "            return layer\n",
        "        self.T1_ = TransConv(1, inp = 256)\n",
        "        self.T2_ = TransConv(2)\n",
        "        self.T3_ = TransConv(3)\n",
        "        self.T4_ = TransConv(4)\n",
        "    \n",
        "        self.ConvLast = nn.Sequential(\n",
        "            nn.Conv2d(32, 3, (1,1), stride=1),\n",
        "            nn.BatchNorm2d(3),\n",
        "            nn.ReLU())\n",
        "\n",
        "\n",
        "        self.layerLandmark1 = nn.Linear(1000, 800)\n",
        "        self.layerLandmark2 = nn.Linear(800, 600)\n",
        "        self.layerLandmark3 = nn.Linear(600, 400)\n",
        "        self.layerLandmark4 = nn.Linear(400, 200)\n",
        "        self.layerLandmark5 = nn.Linear(200, 144)\n",
        "\n",
        "    def forward(self, x):\n",
        "        L1 = self.fc3(x)\n",
        "        L1 = self.ReLU(L1)\n",
        "\n",
        "\n",
        "        L2 = self.layerLandmark1(L1)\n",
        "        L2 = self.ReLU(L2)\n",
        "\n",
        "        L3 = self.layerLandmark2(L2)\n",
        "        L3 = self.ReLU(L3)\n",
        "\n",
        "        L4 = self.layerLandmark3(L3)\n",
        "        L4 = self.ReLU(L4)\n",
        "\n",
        "        L5 = self.layerLandmark4(L4)\n",
        "        L5 = self.ReLU(L5)\n",
        "\n",
        "        L6 = self.layerLandmark5(L5)\n",
        "        outL = self.ReLU(L6)\n",
        "\n",
        "\n",
        "        # B1 = self.fc_bn3(L1) \n",
        "        T0 = self.fc4(L1) \n",
        "        T0 = self.ReLU(T0)\n",
        "        # T0 = self.fc_bn4(T0)\n",
        "        T0 = T0.view(-1,256,14,14)\n",
        "\n",
        "\n",
        "\n",
        "        T1 = self.T1_(T0)\n",
        "        T2 = self.T2_(T1)\n",
        "        T3 = self.T3_(T2)\n",
        "        T4 = self.T4_(T3)\n",
        "\n",
        "        outT = self.ConvLast(T4)\n",
        "        if self.phase == \"train\":\n",
        "            return outL,  outT "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6bEqmFYrNyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Compose(object):\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "        \n",
        "    def __call__(self, x):\n",
        "        for t in self.transforms:\n",
        "            x = t(x)\n",
        "        return x\n",
        "    \n",
        "class Resize(object):\n",
        "    def __init__(self, input_size):\n",
        "        self.input_size = input_size\n",
        "        \n",
        "    def __call__(self, img):\n",
        "        img = img.resize((self.input_size, self.input_size),\n",
        "                                Image.BICUBIC)\n",
        "        return img\n",
        "    \n",
        "class Normalize_Tensor(object):\n",
        "    def __init__(self, color_mean, color_std):\n",
        "        self.color_mean = color_mean\n",
        "        self.color_std = color_std\n",
        "\n",
        "    def __call__(self, img):\n",
        "\n",
        "        # PIL画像をTensorに。大きさは最大1に規格化される\n",
        "        img = transforms.functional.to_tensor(img)\n",
        "\n",
        "        # 色情報の標準化\n",
        "        img = transforms.functional.normalize(\n",
        "            img, self.color_mean, self.color_std)\n",
        "\n",
        "        return img\n",
        "\n",
        "class ImageTransform():\n",
        "    def __init__(self, input_size, color_mean, color_std):\n",
        "        self.data_transform = {\n",
        "            'train' : Compose([\n",
        "                Resize(input_size),\n",
        "                Normalize_Tensor(color_mean, color_std)\n",
        "            ]),\n",
        "            'val' : Compose([\n",
        "                Resize(input_size),\n",
        "                Normalize_Tensor(color_mean, color_std)\n",
        "            ])\n",
        "        }\n",
        "    \n",
        "    def __call_(self, phase, img):\n",
        "        return self.data_transform[phase](img)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvB-OcCG8s8m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############### THIS CELL IS COPIED FROM DEEP SPEECH MOZILLA #########################\n",
        "\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tfv1\n",
        "from tensorflow.compat import dimension_value\n",
        "from tensorflow.contrib.image import dense_image_warp\n",
        "from tensorflow.contrib.image import interpolate_spline\n",
        "\n",
        "from tensorflow.python.framework import ops\n",
        "from tensorflow.python.ops import array_ops\n",
        "\n",
        "def _to_float32(value):\n",
        "    return tf.cast(value, tf.float32)\n",
        "\n",
        "def _to_int32(value):\n",
        "    return tf.cast(value, tf.int32)\n",
        "\n",
        "def _get_grid_locations(image_height, image_width):\n",
        "    \"\"\"Wrapper for np.meshgrid.\"\"\"\n",
        "    tfv1.assert_type(image_height, tf.int32)\n",
        "    tfv1.assert_type(image_width, tf.int32)\n",
        "\n",
        "    y_range = tf.range(image_height)\n",
        "    x_range = tf.range(image_width)\n",
        "    y_grid, x_grid = tf.meshgrid(y_range, x_range, indexing='ij')\n",
        "    return tf.stack((y_grid, x_grid), -1)\n",
        "\n",
        "\n",
        "def _expand_to_minibatch(tensor, batch_size):\n",
        "    \"\"\"Tile arbitrarily-sized np_array to include new batch dimension.\"\"\"\n",
        "    ndim = tf.size(tf.shape(tensor))\n",
        "    ones = tf.ones((ndim,), tf.int32)\n",
        "\n",
        "    tiles = tf.concat(([batch_size], ones), 0)\n",
        "    return tf.tile(tf.expand_dims(tensor, 0), tiles)\n",
        "\n",
        "\n",
        "def _get_boundary_locations(image_height, image_width, num_points_per_edge):\n",
        "    \"\"\"Compute evenly-spaced indices along edge of image.\"\"\"\n",
        "    image_height_end = _to_float32(tf.math.subtract(image_height, 1))\n",
        "    image_width_end = _to_float32(tf.math.subtract(image_width, 1))\n",
        "    y_range = tf.linspace(0.0, image_height_end, num_points_per_edge + 2)\n",
        "    x_range = tf.linspace(0.0, image_height_end, num_points_per_edge + 2)\n",
        "    ys, xs = tf.meshgrid(y_range, x_range, indexing='ij')\n",
        "    is_boundary = tf.logical_or(\n",
        "        tf.logical_or(tf.equal(xs, 0.0), tf.equal(xs, image_width_end)),\n",
        "        tf.logical_or(tf.equal(ys, 0.0), tf.equal(ys, image_height_end)))\n",
        "    return tf.stack([tf.boolean_mask(ys, is_boundary), tf.boolean_mask(xs, is_boundary)], axis=-1)\n",
        "\n",
        "\n",
        "def _add_zero_flow_controls_at_boundary(control_point_locations,\n",
        "                                        control_point_flows, image_height,\n",
        "                                        image_width, boundary_points_per_edge):\n",
        "    \"\"\"Add control points for zero-flow boundary conditions.\n",
        "     Augment the set of control points with extra points on the\n",
        "     boundary of the image that have zero flow.\n",
        "    Args:\n",
        "      control_point_locations: input control points\n",
        "      control_point_flows: their flows\n",
        "      image_height: image height\n",
        "      image_width: image width\n",
        "      boundary_points_per_edge: number of points to add in the middle of each\n",
        "                             edge (not including the corners).\n",
        "                             The total number of points added is\n",
        "                             4 + 4*(boundary_points_per_edge).\n",
        "    Returns:\n",
        "      merged_control_point_locations: augmented set of control point locations\n",
        "      merged_control_point_flows: augmented set of control point flows\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size = dimension_value(tf.shape(control_point_locations)[0])\n",
        "\n",
        "    boundary_point_locations = _get_boundary_locations(image_height, image_width,\n",
        "                                                       boundary_points_per_edge)\n",
        "    boundary_point_shape = tf.shape(boundary_point_locations)\n",
        "    boundary_point_flows = tf.zeros([boundary_point_shape[0], 2])\n",
        "\n",
        "    minbatch_locations = _expand_to_minibatch(boundary_point_locations, batch_size)\n",
        "    type_to_use = control_point_locations.dtype\n",
        "    boundary_point_locations = tf.cast(minbatch_locations, type_to_use)\n",
        "\n",
        "    minbatch_flows = _expand_to_minibatch(boundary_point_flows, batch_size)\n",
        "\n",
        "    boundary_point_flows = tf.cast(minbatch_flows, type_to_use)\n",
        "\n",
        "    merged_control_point_locations = tf.concat(\n",
        "        [control_point_locations, boundary_point_locations], 1)\n",
        "\n",
        "    merged_control_point_flows = tf.concat(\n",
        "        [control_point_flows, boundary_point_flows], 1)\n",
        "\n",
        "    return merged_control_point_locations, merged_control_point_flows\n",
        "\n",
        "\n",
        "def sparse_image_warp(image,\n",
        "                      source_control_point_locations,\n",
        "                      dest_control_point_locations,\n",
        "                      interpolation_order=2,\n",
        "                      regularization_weight=0.0,\n",
        "                      num_boundary_points=0,\n",
        "                      name='sparse_image_warp'):\n",
        "    \"\"\"Image warping using correspondences between sparse control points.\n",
        "    Apply a non-linear warp to the image, where the warp is specified by\n",
        "    the source and destination locations of a (potentially small) number of\n",
        "    control points. First, we use a polyharmonic spline\n",
        "    (`tf.contrib.image.interpolate_spline`) to interpolate the displacements\n",
        "    between the corresponding control points to a dense flow field.\n",
        "    Then, we warp the image using this dense flow field\n",
        "    (`tf.contrib.image.dense_image_warp`).\n",
        "    Let t index our control points. For regularization_weight=0, we have:\n",
        "    warped_image[b, dest_control_point_locations[b, t, 0],\n",
        "                    dest_control_point_locations[b, t, 1], :] =\n",
        "    image[b, source_control_point_locations[b, t, 0],\n",
        "             source_control_point_locations[b, t, 1], :].\n",
        "    For regularization_weight > 0, this condition is met approximately, since\n",
        "    regularized interpolation trades off smoothness of the interpolant vs.\n",
        "    reconstruction of the interpolant at the control points.\n",
        "    See `tf.contrib.image.interpolate_spline` for further documentation of the\n",
        "    interpolation_order and regularization_weight arguments.\n",
        "    Args:\n",
        "      image: `[batch, height, width, channels]` float `Tensor`\n",
        "      source_control_point_locations: `[batch, num_control_points, 2]` float\n",
        "        `Tensor`\n",
        "      dest_control_point_locations: `[batch, num_control_points, 2]` float\n",
        "        `Tensor`\n",
        "      interpolation_order: polynomial order used by the spline interpolation\n",
        "      regularization_weight: weight on smoothness regularizer in interpolation\n",
        "      num_boundary_points: How many zero-flow boundary points to include at\n",
        "        each image edge.Usage:\n",
        "          num_boundary_points=0: don't add zero-flow points\n",
        "          num_boundary_points=1: 4 corners of the image\n",
        "          num_boundary_points=2: 4 corners and one in the middle of each edge\n",
        "            (8 points total)\n",
        "          num_boundary_points=n: 4 corners and n-1 along each edge\n",
        "      name: A name for the operation (optional).\n",
        "      Note that image and offsets can be of type tf.half, tf.float32, or\n",
        "      tf.float64, and do not necessarily have to be the same type.\n",
        "    Returns:\n",
        "      warped_image: `[batch, height, width, channels]` float `Tensor` with same\n",
        "        type as input image.\n",
        "      flow_field: `[batch, height, width, 2]` float `Tensor` containing the dense\n",
        "        flow field produced by the interpolation.\n",
        "    \"\"\"\n",
        "\n",
        "    image = ops.convert_to_tensor(image)\n",
        "    source_control_point_locations = ops.convert_to_tensor(\n",
        "        source_control_point_locations)\n",
        "    dest_control_point_locations = ops.convert_to_tensor(\n",
        "        dest_control_point_locations)\n",
        "\n",
        "    control_point_flows = (\n",
        "        dest_control_point_locations - source_control_point_locations)\n",
        "\n",
        "    clamp_boundaries = num_boundary_points > 0\n",
        "    boundary_points_per_edge = num_boundary_points - 1\n",
        "\n",
        "    with ops.name_scope(name):\n",
        "        image_shape = tf.shape(image)\n",
        "        batch_size, image_height, image_width = image_shape[0], image_shape[1], image_shape[2]\n",
        "\n",
        "        # This generates the dense locations where the interpolant\n",
        "        # will be evaluated.\n",
        "        grid_locations = _get_grid_locations(image_height, image_width)\n",
        "\n",
        "        flattened_grid_locations = tf.reshape(grid_locations,\n",
        "                                              [tf.multiply(image_height, image_width), 2])\n",
        "\n",
        "        # flattened_grid_locations = constant_op.constant(\n",
        "        #     _expand_to_minibatch(flattened_grid_locations, batch_size), image.dtype)\n",
        "        flattened_grid_locations = _expand_to_minibatch(flattened_grid_locations, batch_size)\n",
        "        flattened_grid_locations = tf.cast(flattened_grid_locations, dtype=image.dtype)\n",
        "\n",
        "        if clamp_boundaries:\n",
        "            (dest_control_point_locations,\n",
        "             control_point_flows) = _add_zero_flow_controls_at_boundary(\n",
        "                 dest_control_point_locations, control_point_flows, image_height,\n",
        "                 image_width, boundary_points_per_edge)\n",
        "\n",
        "        flattened_flows = interpolate_spline(\n",
        "            dest_control_point_locations, control_point_flows,\n",
        "            flattened_grid_locations, interpolation_order, regularization_weight)\n",
        "\n",
        "        dense_flows = array_ops.reshape(flattened_flows,\n",
        "                                        [batch_size, image_height, image_width, 2])\n",
        "\n",
        "        warped_image = dense_image_warp(image, dense_flows)\n",
        "\n",
        "        return warped_image, dense_flows"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvjYFqcm8t6E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def image_warping(src_img, src_landmarks, dest_landmarks):\n",
        "    # expanded_src_landmarks = np.expand_dims(np.float32(src_landmarks), axis=0)\n",
        "    # expanded_dest_landmarks = np.expand_dims(np.float32(dest_landmarks), axis=0)\n",
        "    # expanded_src_img = np.expand_dims(np.float32(src_img) / 255, axis=0)\n",
        "\n",
        "    warped_img, dense_flows = sparse_image_warp(src_img,\n",
        "                          src_landmarks,\n",
        "                          dest_landmarks,\n",
        "                          interpolation_order=1,\n",
        "                          regularization_weight=0.1,\n",
        "                          num_boundary_points=2,\n",
        "                          name='sparse_image_warp')\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        out_img = sess.run(warped_img)\n",
        "        warp_img = np.uint8(out_img[:, :, :, :] * 255)\n",
        "    \n",
        "    return warp_img\n",
        "    \n",
        "def face_landmark(img):\n",
        "    X = np.zeros((img.shape[0], 72 ,2))\n",
        "    flag = []\n",
        "    for i in range(img.shape[0]):\n",
        "\n",
        "        landmark = face_landmarks(img[i].reshape(224,224,3))\n",
        "        resultant = []\n",
        "        try:\n",
        "            for j in list(landmark[0].keys()):\n",
        "                resultant += landmark[0][j] \n",
        "        except IndexError:\n",
        "            flag.append(i)\n",
        "            continue\n",
        "        X[i] = np.array(resultant)\n",
        "    return  X, flag"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s86Vf2YTHzvp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path='/content/drive/My Drive/Speech2Face/models/face_decoder_warping/epoch_0.pth'\n",
        "# checkpoint_path=None\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Set Model\n",
        "model = DECODER('train').to(device)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "if checkpoint_path:\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    global_epoch = checkpoint[\"epoch\"]\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "\n",
        "# Set loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "criterion2 = nn.L1Loss()\n",
        "alpha = 0.0002\n",
        "beta  = 1.0\n",
        "\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4UAJu92re1f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "61e6096c-3479-42fa-d330-6a382a03310e"
      },
      "source": [
        "#import argparse\n",
        "\n",
        "\n",
        "\n",
        "#from dataloader import EmbedImagePairs\n",
        "#from face_encoder_model import vgg_face_dag\n",
        "#from face_decoder_model import Generator\n",
        "\n",
        "\n",
        "global_epoch = 0\n",
        "\n",
        "def _save_model(epoch, model, optimizer, steps, fname):\n",
        "    torch.save({\n",
        "        'steps': steps,\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        }, join(output_dir_name, 'epoch_{}_steps_{}.pth'.format(epoch, steps)))\n",
        "\n",
        "\n",
        "def train_epoch(epoch,model, trn_dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "\n",
        "    running_loss = 0\n",
        "    dataset_len = len(trn_dataloader.dataset)\n",
        "\n",
        "    count = 0\n",
        "    numOfElements = 0\n",
        "    saving_steps = 50\n",
        "\n",
        "    for x,landmark, y in trn_dataloader:\n",
        "        \n",
        "        count += 1\n",
        "        x = x.squeeze().to(device)  \n",
        "\n",
        "        # y_img = cv2.cvtColor(np.einsum('abc->bca',y[0].numpy()*255),cv2.COLOR_BGR2RGB)\n",
        "        # cv2_imshow(y_img)\n",
        "\n",
        "        y = y.to(device)\n",
        "        landmark = landmark.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outLandmark, outputs  = model(x)\n",
        "\n",
        "        loss2 = criterion2(outputs, y)\n",
        "        loss1 = criterion(outLandmark, landmark)\n",
        "\n",
        "        loss = (alpha*loss1 + beta*loss2 )\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        # o_img = cv2.cvtColor(np.einsum('abc->bca',outputs[0].cpu().detach().numpy()*255),cv2.COLOR_BGR2RGB)\n",
        "        #cv2_imshow(o_img)\n",
        "        running_loss += loss.item() * x.size(0)\n",
        "        numOfElements += x.shape[0]\n",
        "        \n",
        "\n",
        "        \n",
        "        # outLandmark = outLandmark.cpu().detach().numpy()\n",
        "        # outLandmark = np.dstack((outLandmark[:,0::2],outLandmark[:,1::2]))\n",
        "        # print(\"land np img np \", outL_.shape, img_.shape)\n",
        "        # img1 = np.einsum('dabc->dbca',outputs.cpu().detach().numpy()*255)\n",
        "        # img1 = (img1).astype(np.uint8)\n",
        "        #print(\"img_t \",img_t.shape, img_t[0])\n",
        "        # src, flag = face_landmark(img1)\n",
        "        # if flag:\n",
        "        #     for r in flag:\n",
        "        #         src[r] = outLandmark[r]\n",
        "        # result = image_warping(img1.astype(np.float32), src.astype(np.float32), outLandmark.astype(np.float32))\n",
        "\n",
        "        # w_img = cv2.cvtColor(result[0], cv2.COLOR_BGR2RGB)\n",
        "        #cv2_imshow(w_img)\n",
        "\n",
        "        print(\"loss:\", running_loss / count * x.size(0), \"Steps:\", str(numOfElements) + \"/\" + str(dataset_len) )\n",
        "\n",
        "        if (numOfElements % (x.shape[0] * saving_steps)) == 0:\n",
        "          _save_model(epoch, model, optimizer, numOfElements, output_dir_name)\n",
        "    \n",
        "    trn_loss = running_loss/len(trn_dataloader.dataset)\n",
        "    \n",
        "    return trn_loss\n",
        "\n",
        "\n",
        "def train(model, dataloaders, criterion, optimizer, device, output_dir_name, num_epochs=100):\n",
        "    trn_losses = []\n",
        "\n",
        "    # make dir where trained model saved\n",
        "    os.makedirs(output_dir_name, exist_ok=True)\n",
        "\n",
        "    # start at zero, end at num_epochs (including)\n",
        "    print(\"Start Training\")\n",
        "    for epoch in tqdm(range(global_epoch, num_epochs+1)):\n",
        "        # Training phase\n",
        "        trn_loss = train_epoch(epoch, model, dataloaders, criterion, optimizer, device)\n",
        "        trn_losses += [trn_loss]\n",
        "        print(\"Epoch: {}  || Loss: {}\".format(epoch, trn_loss))\n",
        "    \n",
        "        _save_model(epoch, model, optimizer, 0, output_dir_name)\n",
        "\n",
        "\n",
        "base_dir='/content/drive/My Drive/Speech2Face/'\n",
        "input=\"vox1_dataset/unzippedFaces\"\n",
        "batch_size=32\n",
        "num_epochs=15\n",
        "\n",
        "# Load face-encoder model\n",
        "face_encoder_path = join(base_dir, \"Pretrained\", \"vgg_face_dag.pth\")\n",
        "face_encoder_model = vgg_face_dag(face_encoder_path).to(device)\n",
        "color_mean, color_std = face_encoder_model.meta[\"mean\"], face_encoder_model.meta[\"std\"]\n",
        "color_mean = [tmp / 255.0 for tmp in color_mean]\n",
        "color_std = [tmp / 255.0 for tmp in color_std]\n",
        "\n",
        "# Set DataLoader\n",
        "input_path = '/content/content/Faces_rearranged'\n",
        "print('Image Pairing')\n",
        "dataset = EmbedImagePairs(input_path, face_encoder_model,\n",
        "                          color_mean, color_std, device)\n",
        "print('Creating dataloader')\n",
        "trn_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "output_dir_name = join(base_dir, \"models\", \"face_decoder_warping\")\n",
        "\n",
        "# Train\n",
        "train(model, trn_dataloader, criterion, optimizer, device, output_dir_name, num_epochs=num_epochs)\n",
        "print(\"Finish training\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/15 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Image Pairing\n",
            "Creating dataloader\n",
            "Start Training\n",
            "loss: 21.815397262573242 Steps: 32/8323\n",
            "loss: 23.52227783203125 Steps: 64/8323\n",
            "loss: 22.260961532592773 Steps: 96/8323\n",
            "loss: 22.113040447235107 Steps: 128/8323\n",
            "loss: 24.051726150512696 Steps: 160/8323\n",
            "loss: 23.740596453348797 Steps: 192/8323\n",
            "loss: 23.95648901803153 Steps: 224/8323\n",
            "loss: 24.187323808670044 Steps: 256/8323\n",
            "loss: 24.6948668162028 Steps: 288/8323\n",
            "loss: 24.683781814575195 Steps: 320/8323\n",
            "loss: 24.642976240678266 Steps: 352/8323\n",
            "loss: 24.47999890645345 Steps: 384/8323\n",
            "loss: 24.772592251117413 Steps: 416/8323\n",
            "loss: 24.726157324654714 Steps: 448/8323\n",
            "loss: 24.545639928181966 Steps: 480/8323\n",
            "loss: 24.481321454048157 Steps: 512/8323\n",
            "loss: 24.31159008250517 Steps: 544/8323\n",
            "loss: 24.209909439086914 Steps: 576/8323\n",
            "loss: 24.163415708039935 Steps: 608/8323\n",
            "loss: 24.031691455841063 Steps: 640/8323\n",
            "loss: 23.934128443400066 Steps: 672/8323\n",
            "loss: 23.814473672346637 Steps: 704/8323\n",
            "loss: 23.779913197393004 Steps: 736/8323\n",
            "loss: 23.722652276357014 Steps: 768/8323\n",
            "loss: 23.90994918823242 Steps: 800/8323\n",
            "loss: 23.80082893371582 Steps: 832/8323\n",
            "loss: 23.825669959739404 Steps: 864/8323\n",
            "loss: 23.740295546395437 Steps: 896/8323\n",
            "loss: 23.786873061081458 Steps: 928/8323\n",
            "loss: 23.89208876291911 Steps: 960/8323\n",
            "loss: 23.846800158100745 Steps: 992/8323\n",
            "loss: 23.830349624156952 Steps: 1024/8323\n",
            "loss: 23.895993261626273 Steps: 1056/8323\n",
            "loss: 23.858464241027832 Steps: 1088/8323\n",
            "loss: 23.778328432355607 Steps: 1120/8323\n",
            "loss: 23.805313428243 Steps: 1152/8323\n",
            "loss: 23.7941999177675 Steps: 1184/8323\n",
            "loss: 23.827052417554352 Steps: 1216/8323\n",
            "loss: 23.906453450520832 Steps: 1248/8323\n",
            "loss: 23.790239143371583 Steps: 1280/8323\n",
            "loss: 23.714740567091035 Steps: 1312/8323\n",
            "loss: 23.64704481760661 Steps: 1344/8323\n",
            "loss: 23.665408688922263 Steps: 1376/8323\n",
            "loss: 23.659018516540527 Steps: 1408/8323\n",
            "loss: 23.69002422756619 Steps: 1440/8323\n",
            "loss: 23.684948548026707 Steps: 1472/8323\n",
            "loss: 23.69586664565066 Steps: 1504/8323\n",
            "loss: 23.671687245368958 Steps: 1536/8323\n",
            "loss: 23.58818462916783 Steps: 1568/8323\n",
            "loss: 23.581470375061034 Steps: 1600/8323\n",
            "loss: 23.560093636606254 Steps: 1632/8323\n",
            "loss: 23.571284697606014 Steps: 1664/8323\n",
            "loss: 23.53886496345952 Steps: 1696/8323\n",
            "loss: 23.464947206002694 Steps: 1728/8323\n",
            "loss: 23.42618144642223 Steps: 1760/8323\n",
            "loss: 23.357194628034318 Steps: 1792/8323\n",
            "loss: 23.325918197631836 Steps: 1824/8323\n",
            "loss: 23.262267408699824 Steps: 1856/8323\n",
            "loss: 23.21376034364862 Steps: 1888/8323\n",
            "loss: 23.176535606384277 Steps: 1920/8323\n",
            "loss: 23.16125478900847 Steps: 1952/8323\n",
            "loss: 23.151242625328802 Steps: 1984/8323\n",
            "loss: 23.142772038777668 Steps: 2016/8323\n",
            "loss: 23.134612560272217 Steps: 2048/8323\n",
            "loss: 23.196793306790866 Steps: 2080/8323\n",
            "loss: 23.216954000068434 Steps: 2112/8323\n",
            "loss: 23.200264745683814 Steps: 2144/8323\n",
            "loss: 23.176524134243238 Steps: 2176/8323\n",
            "loss: 23.201111282127492 Steps: 2208/8323\n",
            "loss: 23.223805018833705 Steps: 2240/8323\n",
            "loss: 23.243394475587657 Steps: 2272/8323\n",
            "loss: 23.26974442270067 Steps: 2304/8323\n",
            "loss: 23.239746224390316 Steps: 2336/8323\n",
            "loss: 23.314672650517643 Steps: 2368/8323\n",
            "loss: 23.27237818400065 Steps: 2400/8323\n",
            "loss: 23.31099864056236 Steps: 2432/8323\n",
            "loss: 23.332629488660142 Steps: 2464/8323\n",
            "loss: 23.358732321323494 Steps: 2496/8323\n",
            "loss: 23.357528396799594 Steps: 2528/8323\n",
            "loss: 23.365812587738038 Steps: 2560/8323\n",
            "loss: 23.400397924729337 Steps: 2592/8323\n",
            "loss: 23.3731859253674 Steps: 2624/8323\n",
            "loss: 23.321562962359693 Steps: 2656/8323\n",
            "loss: 23.281639303479874 Steps: 2688/8323\n",
            "loss: 23.23563409693101 Steps: 2720/8323\n",
            "loss: 23.24815668061722 Steps: 2752/8323\n",
            "loss: 23.363328626786156 Steps: 2784/8323\n",
            "loss: 23.36435144597834 Steps: 2816/8323\n",
            "loss: 23.353057368417804 Steps: 2848/8323\n",
            "loss: 23.405737537807887 Steps: 2880/8323\n",
            "loss: 23.38100209078946 Steps: 2912/8323\n",
            "loss: 23.3486618788346 Steps: 2944/8323\n",
            "loss: 23.38125296561949 Steps: 2976/8323\n",
            "loss: 23.405241925665674 Steps: 3008/8323\n",
            "loss: 23.38785753752056 Steps: 3040/8323\n",
            "loss: 23.387036979198456 Steps: 3072/8323\n",
            "loss: 23.430098720432557 Steps: 3104/8323\n",
            "loss: 23.485875227013413 Steps: 3136/8323\n",
            "loss: 23.539864703862353 Steps: 3168/8323\n",
            "loss: 23.509850788116456 Steps: 3200/8323\n",
            "loss: 23.50430235532251 Steps: 3232/8323\n",
            "loss: 23.463598139145795 Steps: 3264/8323\n",
            "loss: 23.4426107128847 Steps: 3296/8323\n",
            "loss: 23.475308620012722 Steps: 3328/8323\n",
            "loss: 23.47635741460891 Steps: 3360/8323\n",
            "loss: 23.474425387832355 Steps: 3392/8323\n",
            "loss: 23.45992633783929 Steps: 3424/8323\n",
            "loss: 23.472620699140762 Steps: 3456/8323\n",
            "loss: 23.44896537010823 Steps: 3488/8323\n",
            "loss: 23.44345581748269 Steps: 3520/8323\n",
            "loss: 23.42126921920089 Steps: 3552/8323\n",
            "loss: 23.417593019349233 Steps: 3584/8323\n",
            "loss: 23.42546739831435 Steps: 3616/8323\n",
            "loss: 23.4774147502163 Steps: 3648/8323\n",
            "loss: 23.461227317478347 Steps: 3680/8323\n",
            "loss: 23.4723308826315 Steps: 3712/8323\n",
            "loss: 23.455451622987404 Steps: 3744/8323\n",
            "loss: 23.446818076958092 Steps: 3776/8323\n",
            "loss: 23.43729235544926 Steps: 3808/8323\n",
            "loss: 23.423377180099486 Steps: 3840/8323\n",
            "loss: 23.429798819802024 Steps: 3872/8323\n",
            "loss: 23.429606734729205 Steps: 3904/8323\n",
            "loss: 23.43190861523636 Steps: 3936/8323\n",
            "loss: 23.403891225014963 Steps: 3968/8323\n",
            "loss: 23.409832382202147 Steps: 4000/8323\n",
            "loss: 23.418529707287984 Steps: 4032/8323\n",
            "loss: 23.41225302688719 Steps: 4064/8323\n",
            "loss: 23.436500653624535 Steps: 4096/8323\n",
            "loss: 23.464610003685767 Steps: 4128/8323\n",
            "loss: 23.442855776273287 Steps: 4160/8323\n",
            "loss: 23.429615574028656 Steps: 4192/8323\n",
            "loss: 23.44183788877545 Steps: 4224/8323\n",
            "loss: 23.421338361008722 Steps: 4256/8323\n",
            "loss: 23.39317735985144 Steps: 4288/8323\n",
            "loss: 23.385379777131256 Steps: 4320/8323\n",
            "loss: 23.36683914240669 Steps: 4352/8323\n",
            "loss: 23.354906820032717 Steps: 4384/8323\n",
            "loss: 23.348632826321367 Steps: 4416/8323\n",
            "loss: 23.34910226725846 Steps: 4448/8323\n",
            "loss: 23.335889911651613 Steps: 4480/8323\n",
            "loss: 23.306025768848176 Steps: 4512/8323\n",
            "loss: 23.302811971852478 Steps: 4544/8323\n",
            "loss: 23.320507169603466 Steps: 4576/8323\n",
            "loss: 23.29240145948198 Steps: 4608/8323\n",
            "loss: 23.286164132479964 Steps: 4640/8323\n",
            "loss: 23.295059008141088 Steps: 4672/8323\n",
            "loss: 23.29344619699076 Steps: 4704/8323\n",
            "loss: 23.289265310442126 Steps: 4736/8323\n",
            "loss: 23.285507854999313 Steps: 4768/8323\n",
            "loss: 23.28540887196859 Steps: 4800/8323\n",
            "loss: 23.26638286161107 Steps: 4832/8323\n",
            "loss: 23.245530956669857 Steps: 4864/8323\n",
            "loss: 23.227240917729397 Steps: 4896/8323\n",
            "loss: 23.226392993679294 Steps: 4928/8323\n",
            "loss: 23.229009086854997 Steps: 4960/8323\n",
            "loss: 23.203273528661484 Steps: 4992/8323\n",
            "loss: 23.21696662902832 Steps: 5024/8323\n",
            "loss: 23.207780765581735 Steps: 5056/8323\n",
            "loss: 23.201166020999164 Steps: 5088/8323\n",
            "loss: 23.189041781425477 Steps: 5120/8323\n",
            "loss: 23.206641108352944 Steps: 5152/8323\n",
            "loss: 23.230807598726248 Steps: 5184/8323\n",
            "loss: 23.265817337972255 Steps: 5216/8323\n",
            "loss: 23.31318519173599 Steps: 5248/8323\n",
            "loss: 23.296096986712833 Steps: 5280/8323\n",
            "loss: 23.326691466641712 Steps: 5312/8323\n",
            "loss: 23.32902436627599 Steps: 5344/8323\n",
            "loss: 23.326301517940703 Steps: 5376/8323\n",
            "loss: 23.30846058546439 Steps: 5408/8323\n",
            "loss: 23.288673816007726 Steps: 5440/8323\n",
            "loss: 23.30141776904725 Steps: 5472/8323\n",
            "loss: 23.32241625009581 Steps: 5504/8323\n",
            "loss: 23.32924861577205 Steps: 5536/8323\n",
            "loss: 23.322906000860804 Steps: 5568/8323\n",
            "loss: 23.33848628452846 Steps: 5600/8323\n",
            "loss: 23.31915171579881 Steps: 5632/8323\n",
            "loss: 23.330699866774392 Steps: 5664/8323\n",
            "loss: 23.320641914110507 Steps: 5696/8323\n",
            "loss: 23.32264895945288 Steps: 5728/8323\n",
            "loss: 23.359632862938774 Steps: 5760/8323\n",
            "loss: 23.388478347609716 Steps: 5792/8323\n",
            "loss: 23.378355634081494 Steps: 5824/8323\n",
            "loss: 23.386147983738635 Steps: 5856/8323\n",
            "loss: 23.38787188737289 Steps: 5888/8323\n",
            "loss: 23.389385037808804 Steps: 5920/8323\n",
            "loss: 23.37342610923193 Steps: 5952/8323\n",
            "loss: 23.418809798949543 Steps: 5984/8323\n",
            "loss: 23.423607217504625 Steps: 6016/8323\n",
            "loss: 23.40406998124703 Steps: 6048/8323\n",
            "loss: 23.407204929151032 Steps: 6080/8323\n",
            "loss: 23.39655185120268 Steps: 6112/8323\n",
            "loss: 23.40990823507309 Steps: 6144/8323\n",
            "loss: 23.41260451479897 Steps: 6176/8323\n",
            "loss: 23.42756646933015 Steps: 6208/8323\n",
            "loss: 23.460064677703073 Steps: 6240/8323\n",
            "loss: 23.46858600694306 Steps: 6272/8323\n",
            "loss: 23.44282858020763 Steps: 6304/8323\n",
            "loss: 23.442396626327977 Steps: 6336/8323\n",
            "loss: 23.4403151123967 Steps: 6368/8323\n",
            "loss: 23.423647174835207 Steps: 6400/8323\n",
            "loss: 23.426922224054287 Steps: 6432/8323\n",
            "loss: 23.43517728371195 Steps: 6464/8323\n",
            "loss: 23.45766842658884 Steps: 6496/8323\n",
            "loss: 23.44240531734392 Steps: 6528/8323\n",
            "loss: 23.455850498850754 Steps: 6560/8323\n",
            "loss: 23.481682555189412 Steps: 6592/8323\n",
            "loss: 23.477988957206986 Steps: 6624/8323\n",
            "loss: 23.471399884957535 Steps: 6656/8323\n",
            "loss: 23.471438476343476 Steps: 6688/8323\n",
            "loss: 23.462170700799852 Steps: 6720/8323\n",
            "loss: 23.46396357640271 Steps: 6752/8323\n",
            "loss: 23.470151055533933 Steps: 6784/8323\n",
            "loss: 23.46420404496887 Steps: 6816/8323\n",
            "loss: 23.471400608526213 Steps: 6848/8323\n",
            "loss: 23.450368872354197 Steps: 6880/8323\n",
            "loss: 23.446935618365252 Steps: 6912/8323\n",
            "loss: 23.437631598266037 Steps: 6944/8323\n",
            "loss: 23.440878614373162 Steps: 6976/8323\n",
            "loss: 23.43233718175322 Steps: 7008/8323\n",
            "loss: 23.440040293606845 Steps: 7040/8323\n",
            "loss: 23.43103399319886 Steps: 7072/8323\n",
            "loss: 23.42496426041062 Steps: 7104/8323\n",
            "loss: 23.427387186230032 Steps: 7136/8323\n",
            "loss: 23.407320533479965 Steps: 7168/8323\n",
            "loss: 23.39709822760688 Steps: 7200/8323\n",
            "loss: 23.388804680478255 Steps: 7232/8323\n",
            "loss: 23.373834584778102 Steps: 7264/8323\n",
            "loss: 23.368620772110788 Steps: 7296/8323\n",
            "loss: 23.36386734741744 Steps: 7328/8323\n",
            "loss: 23.366152680438496 Steps: 7360/8323\n",
            "loss: 23.341484441385642 Steps: 7392/8323\n",
            "loss: 23.337478440383386 Steps: 7424/8323\n",
            "loss: 23.334278385015004 Steps: 7456/8323\n",
            "loss: 23.327923701359676 Steps: 7488/8323\n",
            "loss: 23.340109626283038 Steps: 7520/8323\n",
            "loss: 23.345811569084557 Steps: 7552/8323\n",
            "loss: 23.35060618597747 Steps: 7584/8323\n",
            "loss: 23.349930402611484 Steps: 7616/8323\n",
            "loss: 23.33442635316729 Steps: 7648/8323\n",
            "loss: 23.331804672876995 Steps: 7680/8323\n",
            "loss: 23.329846014125714 Steps: 7712/8323\n",
            "loss: 23.33737963684334 Steps: 7744/8323\n",
            "loss: 23.335356370902357 Steps: 7776/8323\n",
            "loss: 23.320284038293558 Steps: 7808/8323\n",
            "loss: 23.303565940078425 Steps: 7840/8323\n",
            "loss: 23.302123294613224 Steps: 7872/8323\n",
            "loss: 23.303001882576268 Steps: 7904/8323\n",
            "loss: 23.31551267254737 Steps: 7936/8323\n",
            "loss: 23.318427549308563 Steps: 7968/8323\n",
            "loss: 23.306082870483397 Steps: 8000/8323\n",
            "loss: 23.32521801070863 Steps: 8032/8323\n",
            "loss: 23.332308958447168 Steps: 8064/8323\n",
            "loss: 23.333536992431156 Steps: 8096/8323\n",
            "loss: 23.34105692885992 Steps: 8128/8323\n",
            "loss: 23.329104539459827 Steps: 8160/8323\n",
            "loss: 23.32278110831976 Steps: 8192/8323\n",
            "loss: 23.32511460270863 Steps: 8224/8323\n",
            "loss: 23.313084794569384 Steps: 8256/8323\n",
            "loss: 23.318342702269092 Steps: 8288/8323\n",
            "loss: 23.32316669317392 Steps: 8320/8323\n",
            "loss: 23.246303878524753 Steps: 8323/8323\n",
            "Epoch: 1  || Loss: 0.7289781704066995\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  7%|▋         | 1/15 [16:39<3:53:11, 999.40s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 21.175472259521484 Steps: 32/8323\n",
            "loss: 23.528953552246094 Steps: 64/8323\n",
            "loss: 23.539430618286133 Steps: 96/8323\n",
            "loss: 24.41048002243042 Steps: 128/8323\n",
            "loss: 23.88224220275879 Steps: 160/8323\n",
            "loss: 24.32130527496338 Steps: 192/8323\n",
            "loss: 24.40085247584752 Steps: 224/8323\n",
            "loss: 24.172513961791992 Steps: 256/8323\n",
            "loss: 24.700237062242294 Steps: 288/8323\n",
            "loss: 24.229636573791502 Steps: 320/8323\n",
            "loss: 23.851240504871715 Steps: 352/8323\n",
            "loss: 23.357487678527832 Steps: 384/8323\n",
            "loss: 23.382201561560997 Steps: 416/8323\n",
            "loss: 23.31819248199463 Steps: 448/8323\n",
            "loss: 23.391589482625324 Steps: 480/8323\n",
            "loss: 23.284669756889343 Steps: 512/8323\n",
            "loss: 23.021327635821173 Steps: 544/8323\n",
            "loss: 22.91047689649794 Steps: 576/8323\n",
            "loss: 22.817291259765625 Steps: 608/8323\n",
            "loss: 22.93889923095703 Steps: 640/8323\n",
            "loss: 22.98745545886812 Steps: 672/8323\n",
            "loss: 22.95108881863681 Steps: 704/8323\n",
            "loss: 22.82956761899202 Steps: 736/8323\n",
            "loss: 22.76664423942566 Steps: 768/8323\n",
            "loss: 22.75066505432129 Steps: 800/8323\n",
            "loss: 22.79974189171424 Steps: 832/8323\n",
            "loss: 22.682495187830042 Steps: 864/8323\n",
            "loss: 22.59463092258998 Steps: 896/8323\n",
            "loss: 22.565475398096545 Steps: 928/8323\n",
            "loss: 22.547723897298177 Steps: 960/8323\n",
            "loss: 22.564718738678962 Steps: 992/8323\n",
            "loss: 22.511951982975006 Steps: 1024/8323\n",
            "loss: 22.548860492128313 Steps: 1056/8323\n",
            "loss: 22.66002099654254 Steps: 1088/8323\n",
            "loss: 22.66816553388323 Steps: 1120/8323\n",
            "loss: 22.592807398902046 Steps: 1152/8323\n",
            "loss: 22.533467885610218 Steps: 1184/8323\n",
            "loss: 22.719186230709678 Steps: 1216/8323\n",
            "loss: 22.767164817223183 Steps: 1248/8323\n",
            "loss: 22.68449196815491 Steps: 1280/8323\n",
            "loss: 22.827182909337484 Steps: 1312/8323\n",
            "loss: 22.72609129406157 Steps: 1344/8323\n",
            "loss: 22.665047712104265 Steps: 1376/8323\n",
            "loss: 22.634905728426848 Steps: 1408/8323\n",
            "loss: 22.658705859714082 Steps: 1440/8323\n",
            "loss: 22.671150373375934 Steps: 1472/8323\n",
            "loss: 22.584965442089324 Steps: 1504/8323\n",
            "loss: 22.575725555419922 Steps: 1536/8323\n",
            "loss: 22.57193962408572 Steps: 1568/8323\n",
            "loss: 22.574916343688965 Steps: 1600/8323\n",
            "loss: 22.601014754351446 Steps: 1632/8323\n",
            "loss: 22.634085985330437 Steps: 1664/8323\n",
            "loss: 22.568260516760486 Steps: 1696/8323\n",
            "loss: 22.558446990119087 Steps: 1728/8323\n",
            "loss: 22.573035396229137 Steps: 1760/8323\n",
            "loss: 22.592631340026855 Steps: 1792/8323\n",
            "loss: 22.562294508281507 Steps: 1824/8323\n",
            "loss: 22.533233346610235 Steps: 1856/8323\n",
            "loss: 22.491828110258457 Steps: 1888/8323\n",
            "loss: 22.526680278778077 Steps: 1920/8323\n",
            "loss: 22.47803500441254 Steps: 1952/8323\n",
            "loss: 22.511167064789802 Steps: 1984/8323\n",
            "loss: 22.492469817873033 Steps: 2016/8323\n",
            "loss: 22.47788369655609 Steps: 2048/8323\n",
            "loss: 22.480105942946214 Steps: 2080/8323\n",
            "loss: 22.42065834276604 Steps: 2112/8323\n",
            "loss: 22.443666657405114 Steps: 2144/8323\n",
            "loss: 22.510532827938306 Steps: 2176/8323\n",
            "loss: 22.495925709821176 Steps: 2208/8323\n",
            "loss: 22.499646840776716 Steps: 2240/8323\n",
            "loss: 22.517738476605484 Steps: 2272/8323\n",
            "loss: 22.522186491224502 Steps: 2304/8323\n",
            "loss: 22.546291534214802 Steps: 2336/8323\n",
            "loss: 22.556930516217207 Steps: 2368/8323\n",
            "loss: 22.54676081339518 Steps: 2400/8323\n",
            "loss: 22.606437984265778 Steps: 2432/8323\n",
            "loss: 22.674453239936334 Steps: 2464/8323\n",
            "loss: 22.683325694157528 Steps: 2496/8323\n",
            "loss: 22.72651071186307 Steps: 2528/8323\n",
            "loss: 22.693023610115052 Steps: 2560/8323\n",
            "loss: 22.686326321260427 Steps: 2592/8323\n",
            "loss: 22.660458192592714 Steps: 2624/8323\n",
            "loss: 22.66873304527926 Steps: 2656/8323\n",
            "loss: 22.67855426243373 Steps: 2688/8323\n",
            "loss: 22.683826020184686 Steps: 2720/8323\n",
            "loss: 22.73112427911093 Steps: 2752/8323\n",
            "loss: 22.721481586324757 Steps: 2784/8323\n",
            "loss: 22.73347683386369 Steps: 2816/8323\n",
            "loss: 22.717035786489422 Steps: 2848/8323\n",
            "loss: 22.721322102016874 Steps: 2880/8323\n",
            "loss: 22.721143114697803 Steps: 2912/8323\n",
            "loss: 22.690708948218305 Steps: 2944/8323\n",
            "loss: 22.722918110509074 Steps: 2976/8323\n",
            "loss: 22.697446052064286 Steps: 3008/8323\n",
            "loss: 22.70917161640368 Steps: 3040/8323\n",
            "loss: 22.72405234972636 Steps: 3072/8323\n",
            "loss: 22.70057890587246 Steps: 3104/8323\n",
            "loss: 22.686857924169423 Steps: 3136/8323\n",
            "loss: 22.685235649648337 Steps: 3168/8323\n",
            "loss: 22.686645736694334 Steps: 3200/8323\n",
            "loss: 22.69062895822053 Steps: 3232/8323\n",
            "loss: 22.654489629408893 Steps: 3264/8323\n",
            "loss: 22.65823438329604 Steps: 3296/8323\n",
            "loss: 22.651156498835636 Steps: 3328/8323\n",
            "loss: 22.668009712582542 Steps: 3360/8323\n",
            "loss: 22.63629862947284 Steps: 3392/8323\n",
            "loss: 22.651481467986777 Steps: 3424/8323\n",
            "loss: 22.64034354245221 Steps: 3456/8323\n",
            "loss: 22.60452891708514 Steps: 3488/8323\n",
            "loss: 22.600612466985528 Steps: 3520/8323\n",
            "loss: 22.600247426075978 Steps: 3552/8323\n",
            "loss: 22.578549027442932 Steps: 3584/8323\n",
            "loss: 22.5756832055286 Steps: 3616/8323\n",
            "loss: 22.565377034639056 Steps: 3648/8323\n",
            "loss: 22.571414897752845 Steps: 3680/8323\n",
            "loss: 22.577541532187627 Steps: 3712/8323\n",
            "loss: 22.571664891691288 Steps: 3744/8323\n",
            "loss: 22.592585418183926 Steps: 3776/8323\n",
            "loss: 22.597140785024948 Steps: 3808/8323\n",
            "loss: 22.576821772257485 Steps: 3840/8323\n",
            "loss: 22.576864699686855 Steps: 3872/8323\n",
            "loss: 22.55843379849293 Steps: 3904/8323\n",
            "loss: 22.537809790634526 Steps: 3936/8323\n",
            "loss: 22.53640221011254 Steps: 3968/8323\n",
            "loss: 22.53513018798828 Steps: 4000/8323\n",
            "loss: 22.52875135815333 Steps: 4032/8323\n",
            "loss: 22.512396444485884 Steps: 4064/8323\n",
            "loss: 22.48988889157772 Steps: 4096/8323\n",
            "loss: 22.495165196500082 Steps: 4128/8323\n",
            "loss: 22.47746020096999 Steps: 4160/8323\n",
            "loss: 22.505474629292962 Steps: 4192/8323\n",
            "loss: 22.50390966010816 Steps: 4224/8323\n",
            "loss: 22.50215756982789 Steps: 4256/8323\n",
            "loss: 22.491921681076732 Steps: 4288/8323\n",
            "loss: 22.486944396407516 Steps: 4320/8323\n",
            "loss: 22.463478789610022 Steps: 4352/8323\n",
            "loss: 22.457114560760722 Steps: 4384/8323\n",
            "loss: 22.427315269691356 Steps: 4416/8323\n",
            "loss: 22.394954379513965 Steps: 4448/8323\n",
            "loss: 22.37532995768956 Steps: 4480/8323\n",
            "loss: 22.371535226808373 Steps: 4512/8323\n",
            "loss: 22.411838289717554 Steps: 4544/8323\n",
            "loss: 22.405781619198674 Steps: 4576/8323\n",
            "loss: 22.397539681858486 Steps: 4608/8323\n",
            "loss: 22.409647921858163 Steps: 4640/8323\n",
            "loss: 22.42612308345429 Steps: 4672/8323\n",
            "loss: 22.406409244148097 Steps: 4704/8323\n",
            "loss: 22.37571113174026 Steps: 4736/8323\n",
            "loss: 22.347273589780666 Steps: 4768/8323\n",
            "loss: 22.34783358256022 Steps: 4800/8323\n",
            "loss: 22.36955979959854 Steps: 4832/8323\n",
            "loss: 22.359861373901367 Steps: 4864/8323\n",
            "loss: 22.375994912939134 Steps: 4896/8323\n",
            "loss: 22.381635145707563 Steps: 4928/8323\n",
            "loss: 22.383919328258884 Steps: 4960/8323\n",
            "loss: 22.37640464000213 Steps: 4992/8323\n",
            "loss: 22.35882445657329 Steps: 5024/8323\n",
            "loss: 22.34338923345638 Steps: 5056/8323\n",
            "loss: 22.34442364194858 Steps: 5088/8323\n",
            "loss: 22.383378672599793 Steps: 5120/8323\n",
            "loss: 22.387655210791166 Steps: 5152/8323\n",
            "loss: 22.376887957255047 Steps: 5184/8323\n",
            "loss: 22.386492933963705 Steps: 5216/8323\n",
            "loss: 22.39617410520228 Steps: 5248/8323\n",
            "loss: 22.39159658027418 Steps: 5280/8323\n",
            "loss: 22.361645261925386 Steps: 5312/8323\n",
            "loss: 22.370959675954488 Steps: 5344/8323\n",
            "loss: 22.348407393410092 Steps: 5376/8323\n",
            "loss: 22.346714821087538 Steps: 5408/8323\n",
            "loss: 22.35207764120663 Steps: 5440/8323\n",
            "loss: 22.360501964189854 Steps: 5472/8323\n",
            "loss: 22.36773286863815 Steps: 5504/8323\n",
            "loss: 22.367699253765835 Steps: 5536/8323\n",
            "loss: 22.35342008217998 Steps: 5568/8323\n",
            "loss: 22.35281602042062 Steps: 5600/8323\n",
            "loss: 22.35397925160148 Steps: 5632/8323\n",
            "loss: 22.361188770014014 Steps: 5664/8323\n",
            "loss: 22.33748418829414 Steps: 5696/8323\n",
            "loss: 22.331085396878546 Steps: 5728/8323\n",
            "loss: 22.333898120456272 Steps: 5760/8323\n",
            "loss: 22.344733896835073 Steps: 5792/8323\n",
            "loss: 22.34908940241887 Steps: 5824/8323\n",
            "loss: 22.34233531013864 Steps: 5856/8323\n",
            "loss: 22.333033126333486 Steps: 5888/8323\n",
            "loss: 22.30988734477275 Steps: 5920/8323\n",
            "loss: 22.311407796798214 Steps: 5952/8323\n",
            "loss: 22.29833127598074 Steps: 5984/8323\n",
            "loss: 22.285025282109036 Steps: 6016/8323\n",
            "loss: 22.298907093270117 Steps: 6048/8323\n",
            "loss: 22.302327417072497 Steps: 6080/8323\n",
            "loss: 22.290788720415524 Steps: 6112/8323\n",
            "loss: 22.297499080499012 Steps: 6144/8323\n",
            "loss: 22.303985615468395 Steps: 6176/8323\n",
            "loss: 22.312176035851547 Steps: 6208/8323\n",
            "loss: 22.298047080406775 Steps: 6240/8323\n",
            "loss: 22.287632630795848 Steps: 6272/8323\n",
            "loss: 22.286461854344093 Steps: 6304/8323\n",
            "loss: 22.267014609442818 Steps: 6336/8323\n",
            "loss: 22.274425746208458 Steps: 6368/8323\n",
            "loss: 22.266187772750854 Steps: 6400/8323\n",
            "loss: 22.259798828049085 Steps: 6432/8323\n",
            "loss: 22.25774014350211 Steps: 6464/8323\n",
            "loss: 22.229467147676814 Steps: 6496/8323\n",
            "loss: 22.23058104982563 Steps: 6528/8323\n",
            "loss: 22.23178429022068 Steps: 6560/8323\n",
            "loss: 22.225366981284132 Steps: 6592/8323\n",
            "loss: 22.206218765553643 Steps: 6624/8323\n",
            "loss: 22.204179076048042 Steps: 6656/8323\n",
            "loss: 22.190406571164655 Steps: 6688/8323\n",
            "loss: 22.180193092709494 Steps: 6720/8323\n",
            "loss: 22.19050105488131 Steps: 6752/8323\n",
            "loss: 22.17911303718135 Steps: 6784/8323\n",
            "loss: 22.203372570270663 Steps: 6816/8323\n",
            "loss: 22.194445048537208 Steps: 6848/8323\n",
            "loss: 22.20852874046148 Steps: 6880/8323\n",
            "loss: 22.199675312748663 Steps: 6912/8323\n",
            "loss: 22.200576703119935 Steps: 6944/8323\n",
            "loss: 22.203217436414246 Steps: 6976/8323\n",
            "loss: 22.20768318873018 Steps: 7008/8323\n",
            "loss: 22.194620149785823 Steps: 7040/8323\n",
            "loss: 22.191890086523546 Steps: 7072/8323\n",
            "loss: 22.19458533622123 Steps: 7104/8323\n",
            "loss: 22.184823108895475 Steps: 7136/8323\n",
            "loss: 22.182109858308518 Steps: 7168/8323\n",
            "loss: 22.172464175754122 Steps: 7200/8323\n",
            "loss: 22.16761817763337 Steps: 7232/8323\n",
            "loss: 22.17904162301891 Steps: 7264/8323\n",
            "loss: 22.165890057881672 Steps: 7296/8323\n",
            "loss: 22.161499306624634 Steps: 7328/8323\n",
            "loss: 22.159571755450706 Steps: 7360/8323\n",
            "loss: 22.16522624688747 Steps: 7392/8323\n",
            "loss: 22.153676592070482 Steps: 7424/8323\n",
            "loss: 22.14882461809805 Steps: 7456/8323\n",
            "loss: 22.16663877373068 Steps: 7488/8323\n",
            "loss: 22.167457702312063 Steps: 7520/8323\n",
            "loss: 22.18266818482997 Steps: 7552/8323\n",
            "loss: 22.192081644565246 Steps: 7584/8323\n",
            "loss: 22.190760981135007 Steps: 7616/8323\n",
            "loss: 22.190217189709013 Steps: 7648/8323\n",
            "loss: 22.193520855903625 Steps: 7680/8323\n",
            "loss: 22.2094308766092 Steps: 7712/8323\n",
            "loss: 22.193309397736858 Steps: 7744/8323\n",
            "loss: 22.18719246161818 Steps: 7776/8323\n",
            "loss: 22.19725706538216 Steps: 7808/8323\n",
            "loss: 22.191854445788326 Steps: 7840/8323\n",
            "loss: 22.192142246215322 Steps: 7872/8323\n",
            "loss: 22.207438889785333 Steps: 7904/8323\n",
            "loss: 22.202440661768758 Steps: 7936/8323\n",
            "loss: 22.192636765629413 Steps: 7968/8323\n",
            "loss: 22.17786205291748 Steps: 8000/8323\n",
            "loss: 22.17893783599732 Steps: 8032/8323\n",
            "loss: 22.168724695841473 Steps: 8064/8323\n",
            "loss: 22.175560110642504 Steps: 8096/8323\n",
            "loss: 22.16260011928288 Steps: 8128/8323\n",
            "loss: 22.148939319685393 Steps: 8160/8323\n",
            "loss: 22.148316845297813 Steps: 8192/8323\n",
            "loss: 22.16355355704341 Steps: 8224/8323\n",
            "loss: 22.1568473771561 Steps: 8256/8323\n",
            "loss: 22.155641599971815 Steps: 8288/8323\n",
            "loss: 22.16294061954205 Steps: 8320/8323\n",
            "loss: 22.087062695245635 Steps: 8323/8323\n",
            "Epoch: 2  || Loss: 0.6926256594327899\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 13%|█▎        | 2/15 [33:16<3:36:24, 998.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 18.742443084716797 Steps: 32/8323\n",
            "loss: 19.090493202209473 Steps: 64/8323\n",
            "loss: 21.46852684020996 Steps: 96/8323\n",
            "loss: 20.767963886260986 Steps: 128/8323\n",
            "loss: 20.522693634033203 Steps: 160/8323\n",
            "loss: 20.60506025950114 Steps: 192/8323\n",
            "loss: 20.44831711905343 Steps: 224/8323\n",
            "loss: 20.4463369846344 Steps: 256/8323\n",
            "loss: 20.531721327039932 Steps: 288/8323\n",
            "loss: 20.606787490844727 Steps: 320/8323\n",
            "loss: 20.69139862060547 Steps: 352/8323\n",
            "loss: 20.841279824574787 Steps: 384/8323\n",
            "loss: 20.772659741915188 Steps: 416/8323\n",
            "loss: 20.70659405844552 Steps: 448/8323\n",
            "loss: 20.717230606079102 Steps: 480/8323\n",
            "loss: 20.86604404449463 Steps: 512/8323\n",
            "loss: 21.170073004329907 Steps: 544/8323\n",
            "loss: 21.267259915669758 Steps: 576/8323\n",
            "loss: 21.318069859554893 Steps: 608/8323\n",
            "loss: 21.29060344696045 Steps: 640/8323\n",
            "loss: 21.310591743105935 Steps: 672/8323\n",
            "loss: 21.202570655129172 Steps: 704/8323\n",
            "loss: 21.30577551800272 Steps: 736/8323\n",
            "loss: 21.324251731236775 Steps: 768/8323\n",
            "loss: 21.312924728393554 Steps: 800/8323\n",
            "loss: 21.347769077007587 Steps: 832/8323\n",
            "loss: 21.35616542674877 Steps: 864/8323\n",
            "loss: 21.28317063195365 Steps: 896/8323\n",
            "loss: 21.256770758793273 Steps: 928/8323\n",
            "loss: 21.208391253153483 Steps: 960/8323\n",
            "loss: 21.23478255733367 Steps: 992/8323\n",
            "loss: 21.1723450422287 Steps: 1024/8323\n",
            "loss: 21.066082578716856 Steps: 1056/8323\n",
            "loss: 21.09006898543414 Steps: 1088/8323\n",
            "loss: 21.070012555803572 Steps: 1120/8323\n",
            "loss: 21.04652362399631 Steps: 1152/8323\n",
            "loss: 21.000763506502718 Steps: 1184/8323\n",
            "loss: 21.000108969838994 Steps: 1216/8323\n",
            "loss: 20.93123783209385 Steps: 1248/8323\n",
            "loss: 20.949186420440675 Steps: 1280/8323\n",
            "loss: 20.98959620405988 Steps: 1312/8323\n",
            "loss: 20.962359292166575 Steps: 1344/8323\n",
            "loss: 21.06722334928291 Steps: 1376/8323\n",
            "loss: 21.01862894405018 Steps: 1408/8323\n",
            "loss: 21.05744700961643 Steps: 1440/8323\n",
            "loss: 21.160228107286535 Steps: 1472/8323\n",
            "loss: 21.22021666993486 Steps: 1504/8323\n",
            "loss: 21.14485494295756 Steps: 1536/8323\n",
            "loss: 21.163312600583446 Steps: 1568/8323\n",
            "loss: 21.221661224365235 Steps: 1600/8323\n",
            "loss: 21.20193272010953 Steps: 1632/8323\n",
            "loss: 21.234434641324558 Steps: 1664/8323\n",
            "loss: 21.276835531558632 Steps: 1696/8323\n",
            "loss: 21.20456677896005 Steps: 1728/8323\n",
            "loss: 21.29093562039462 Steps: 1760/8323\n",
            "loss: 21.251199415751866 Steps: 1792/8323\n",
            "loss: 21.287227864851033 Steps: 1824/8323\n",
            "loss: 21.30381344104635 Steps: 1856/8323\n",
            "loss: 21.315515194909047 Steps: 1888/8323\n",
            "loss: 21.342430464426677 Steps: 1920/8323\n",
            "loss: 21.38214008143691 Steps: 1952/8323\n",
            "loss: 21.364277039804765 Steps: 1984/8323\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNNWVCZRrq-A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# _save_model(1, model, optimizer,3200 ,'no_preprocessing')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}