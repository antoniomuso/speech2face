{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antoniomuso/speech2face/blob/master/Decoder_with_warping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYdMtmJooq6M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6fcc2233-1bc5-4d7e-b4a2-b9d72448371e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yk88JxlKquen",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vox1_path = \"/content/content/Faces_rearranged\"\n",
        "img = vox1_path+\"unzippedFaces\"\n",
        "# ! pip3 install face_recognition\n",
        "# ! pip3 install tensorflow-gpu==1.15"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWWYuEBuoTYg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ab9838c5-be68-4309-939f-4705eb5fd204"
      },
      "source": [
        "# ! rm -r Face_Feature\n",
        "# ! cp /content/drive/My\\ Drive/Speech2Face/face_features/face_features_hopenet.zip .\n",
        "# ! unzip face_features_hopenet.zip\n",
        "# ! ls -l Face_Feature/Faces | wc -l"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7408\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4XC5nS4thFS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !wget http://www.robots.ox.ac.uk/~vgg/research/CMBiometrics/data/zippedFaces.tar.gz \n",
        "# tar zxvf zippedFaces.tar.gz -C \"/content/drive/My Drive/Speech2Face/vox1_dataset/\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eFsvXPivKgI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !curl --user voxceleb1912:0s42xuw6 -o \"/content/drive/My Drive/Speech2Face/ff/vox.zip\" http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_test_wav.zip"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VepCRxxbJlAW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import  torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from face_recognition import face_landmarks, face_locations, face_encodings\n",
        "from random import randint\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "\n",
        "import os\n",
        "from os.path import join\n",
        "from os import listdir\n",
        "\n",
        "from tqdm import tqdm\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rQZIr9MJlR4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reproducibility stuff\n",
        "\n",
        "import random\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(0)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6SgTIZoqwYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This dataset use .npy embedding VGG\n",
        "def load_metadata(data_file_path):\n",
        "    data_list = []\n",
        "    with open(data_file_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            data_list.append(line.rstrip(\"\\n\").split(\",\"))\n",
        "    return data_list\n",
        "\n",
        "\n",
        "class _Normalize_Tensor(object):\n",
        "    def __init__(self, color_mean, color_std):\n",
        "        self.color_mean = color_mean\n",
        "        self.color_std = color_std\n",
        "\n",
        "    def __call__(self, img):\n",
        "        # Convert image to Tensor\n",
        "        img = transforms.functional.to_tensor(img)\n",
        "\n",
        "        # Normalize image by the parameter of pre-trained face-encoder\n",
        "        img = transforms.functional.normalize(\n",
        "            img, self.color_mean, self.color_std)\n",
        "\n",
        "        return img\n",
        "\n",
        "class EmbedImagePairs(Dataset):\n",
        "    def __init__(self, data_list_path, size=64):\n",
        "        super().__init__()\n",
        "        self.face_features = np.load(join(data_list_path,'facefeature.npy'))\n",
        "        self.image_path = join(data_list_path, 'Faces')\n",
        "        self.images =  listdir(join(data_list_path, 'Faces'))\n",
        "        \n",
        "        self.size = size\n",
        "\n",
        "\n",
        "        self.transform_fd = transforms.Compose([\n",
        "            transforms.ToTensor()\n",
        "            \n",
        "        ])\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # Load face-image\n",
        "        image_p = self.images[idx]\n",
        "        features_index = int(image_p.split('_')[-1].split('.')[0])\n",
        "\n",
        "        image = Image.open(join(self.image_path,image_p))\n",
        "        \n",
        "        \n",
        "        image_ = image.resize((224,224))\n",
        "        # Localize Face\n",
        "        np_image = np.array(image_)\n",
        "        # print(np_image.shape)\n",
        "        try:\n",
        "\n",
        "          resultant = []\n",
        "          #np_image = np_image[x:x1,y:y1]\n",
        "          landmark = face_landmarks(np_image)\n",
        "          for i in list(landmark[0].keys()):\n",
        "            resultant += landmark[0][i]\n",
        "          landmark = np.ravel(np.array(resultant))\n",
        "\n",
        "        except IndexError:\n",
        "          return self[randint(0, len(self))]\n",
        "\n",
        "        face_vectors = torch.tensor(self.face_features[features_index])\n",
        "\n",
        "        return face_vectors.float(), torch.tensor(landmark).float(), self.transform_fd(image_).float()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvLYwe3mnDh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_metadata(data_file_path):\n",
        "    data_list = []\n",
        "    with open(data_file_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            data_list.append(line.rstrip(\"\\n\").split(\",\"))\n",
        "    return data_list\n",
        "\n",
        "\n",
        "class _Normalize_Tensor(object):\n",
        "    def __init__(self, color_mean, color_std):\n",
        "        self.color_mean = color_mean\n",
        "        self.color_std = color_std\n",
        "\n",
        "    def __call__(self, img):\n",
        "        # Convert image to Tensor\n",
        "        img = transforms.functional.to_tensor(img)\n",
        "\n",
        "        # Normalize image by the parameter of pre-trained face-encoder\n",
        "        img = transforms.functional.normalize(\n",
        "            img, self.color_mean, self.color_std)\n",
        "\n",
        "        return img\n",
        "\n",
        "class EmbedImagePairs(Dataset):\n",
        "    def __init__(self, data_list_path, face_encoder, mean, std, device, size=64):\n",
        "        super().__init__()\n",
        "        self.images = ImageFolder(data_list_path)\n",
        "\n",
        "        self.face_encoder = face_encoder\n",
        "\n",
        "        self.color_mean = mean\n",
        "        self.color_std = std\n",
        "        \n",
        "        self.device = device\n",
        "        self.size = size\n",
        "\n",
        "        # Transform input for face-encoder\n",
        "        self.transform_fe = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            _Normalize_Tensor(self.color_mean, self.color_std)\n",
        "        ])\n",
        "\n",
        "        self.transform_fd = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor()\n",
        "            \n",
        "        ])\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # Load face-image\n",
        "        image = self.images[idx][0]\n",
        "\n",
        "        # convert face-images into 4096-dim vectors\n",
        "        self.face_encoder.eval()\n",
        "\n",
        "        \n",
        "        # Localize Face\n",
        "        np_image = np.array(image)\n",
        "        #face_loc = face_locations(np_image)\n",
        "        \n",
        "        try:\n",
        "          #x,y1,x1,y = face_loc[0]\n",
        "          #image = image.crop((x,y,x1,y1))\n",
        "\n",
        "          resultant = []\n",
        "          #np_image = np_image[x:x1,y:y1]\n",
        "          landmark = face_landmarks(np_image)\n",
        "          for i in list(landmark[0].keys()):\n",
        "            resultant += landmark[0][i]\n",
        "          landmark = np.ravel(np.array(resultant))\n",
        "\n",
        "        except IndexError:\n",
        "          return self[randint(0, len(self))]\n",
        "\n",
        "        face_vectors = self.face_encoder(\n",
        "            self.transform_fe(image).unsqueeze_(0).to(self.device))\n",
        "\n",
        "        image_ = self.transform_fd(image)\n",
        "\n",
        "        return face_vectors, torch.tensor(landmark).float(), image_"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Io7T0B-dq8eG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Vgg_face_dag(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Vgg_face_dag, self).__init__()\n",
        "        self.meta = {'mean': [129.186279296875, 104.76238250732422, 93.59396362304688],\n",
        "                     'std': [1, 1, 1],\n",
        "                     'imageSize': [224, 224, 3]}\n",
        "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu1_1 = nn.ReLU(inplace=True)\n",
        "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu1_2 = nn.ReLU(inplace=True)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
        "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu2_1 = nn.ReLU(inplace=True)\n",
        "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu2_2 = nn.ReLU(inplace=True)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
        "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu3_1 = nn.ReLU(inplace=True)\n",
        "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu3_2 = nn.ReLU(inplace=True)\n",
        "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu3_3 = nn.ReLU(inplace=True)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
        "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu4_1 = nn.ReLU(inplace=True)\n",
        "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu4_2 = nn.ReLU(inplace=True)\n",
        "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu4_3 = nn.ReLU(inplace=True)\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
        "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu5_1 = nn.ReLU(inplace=True)\n",
        "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu5_2 = nn.ReLU(inplace=True)\n",
        "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu5_3 = nn.ReLU(inplace=True)\n",
        "        self.pool5 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
        "        self.fc6 = nn.Linear(in_features=25088, out_features=4096, bias=True)\n",
        "        self.relu6 = nn.ReLU(inplace=True)\n",
        "        self.dropout6 = nn.Dropout(p=0.5)\n",
        "        self.fc7 = nn.Linear(in_features=4096, out_features=4096, bias=True)\n",
        "        self.relu7 = nn.ReLU(inplace=True)\n",
        "        self.dropout7 = nn.Dropout(p=0.5)\n",
        "        self.fc8 = nn.Linear(in_features=4096, out_features=2622, bias=True)\n",
        "\n",
        "    def forward(self, x0, is_fc8=False):\n",
        "        x1 = self.conv1_1(x0)\n",
        "        x2 = self.relu1_1(x1)\n",
        "        x3 = self.conv1_2(x2)\n",
        "        x4 = self.relu1_2(x3)\n",
        "        x5 = self.pool1(x4)\n",
        "        x6 = self.conv2_1(x5)\n",
        "        x7 = self.relu2_1(x6)\n",
        "        x8 = self.conv2_2(x7)\n",
        "        x9 = self.relu2_2(x8)\n",
        "        x10 = self.pool2(x9)\n",
        "        x11 = self.conv3_1(x10)\n",
        "        x12 = self.relu3_1(x11)\n",
        "        x13 = self.conv3_2(x12)\n",
        "        x14 = self.relu3_2(x13)\n",
        "        x15 = self.conv3_3(x14)\n",
        "        x16 = self.relu3_3(x15)\n",
        "        x17 = self.pool3(x16)\n",
        "        x18 = self.conv4_1(x17)\n",
        "        x19 = self.relu4_1(x18)\n",
        "        x20 = self.conv4_2(x19)\n",
        "        x21 = self.relu4_2(x20)\n",
        "        x22 = self.conv4_3(x21)\n",
        "        x23 = self.relu4_3(x22)\n",
        "        x24 = self.pool4(x23)\n",
        "        x25 = self.conv5_1(x24)\n",
        "        x26 = self.relu5_1(x25)\n",
        "        x27 = self.conv5_2(x26)\n",
        "        x28 = self.relu5_2(x27)\n",
        "        x29 = self.conv5_3(x28)\n",
        "        x30 = self.relu5_3(x29)\n",
        "        x31_preflatten = self.pool5(x30)\n",
        "        x31 = x31_preflatten.view(x31_preflatten.size(0), -1)\n",
        "        x32 = self.fc6(x31)\n",
        "        x33 = self.relu6(x32)\n",
        "        x34 = self.dropout6(x33)\n",
        "        x35 = self.fc7(x34)\n",
        "        x36 = self.relu7(x35)\n",
        "        x37 = self.dropout7(x36)\n",
        "        if is_fc8:\n",
        "            x38 = self.fc8(x37)\n",
        "        else:\n",
        "            x38 = x37\n",
        "        return x38\n",
        "\n",
        "\n",
        "def vgg_face_dag(weights_path=None, **kwargs):\n",
        "    \"\"\"\n",
        "    load imported model instance\n",
        "    Args:\n",
        "        weights_path (str): If set, loads model weights from the given path\n",
        "    \"\"\"\n",
        "    model = Vgg_face_dag()\n",
        "    if weights_path:\n",
        "        state_dict = torch.load(weights_path)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5xUu6b9rBsl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DECODER(nn.Module):\n",
        "    def __init__(self, phase):\n",
        "        super(DECODER, self).__init__()\n",
        "        self.phase = phase\n",
        "        self.fc3 = nn.Linear(4096, 1000)\n",
        "        self.ReLU = nn.ReLU()\n",
        "        #self.fc_bn3 = nn.BatchNorm1d(1000)\n",
        "\n",
        "\n",
        "        self.fc4 = nn.Linear(1000, 14 * 14 * 256)\n",
        "        self.fc_bn4 = nn.BatchNorm1d(14 * 14 * 256)\n",
        "        def TransConv( i, kernal = 5, stride = 2, inp = None):\n",
        "            if not inp:\n",
        "                inp = max(256//2**(i-1), 32)\n",
        "\n",
        "            layer =  nn.Sequential(\n",
        "                nn.ConvTranspose2d(inp, max(256//2**i, 32), \n",
        "                                kernal, stride=stride, padding=2, output_padding=1, \n",
        "                                dilation=1, padding_mode='zeros'),\n",
        "                nn.ReLU(),\n",
        "                nn.BatchNorm2d(max(256//2**i, 32)))\n",
        "            return layer\n",
        "        self.T1_ = TransConv(1, inp = 256)\n",
        "        self.T2_ = TransConv(2)\n",
        "        self.T3_ = TransConv(3)\n",
        "        self.T4_ = TransConv(4)\n",
        "    \n",
        "        self.ConvLast = nn.Sequential(\n",
        "            nn.Conv2d(32, 3, (1,1), stride=1),\n",
        "            nn.BatchNorm2d(3),\n",
        "            nn.ReLU())\n",
        "\n",
        "\n",
        "        self.layerLandmark1 = nn.Linear(1000, 800)\n",
        "        self.layerLandmark2 = nn.Linear(800, 600)\n",
        "        self.layerLandmark3 = nn.Linear(600, 400)\n",
        "        self.layerLandmark4 = nn.Linear(400, 200)\n",
        "        self.layerLandmark5 = nn.Linear(200, 144)\n",
        "\n",
        "    def forward(self, x):\n",
        "        L1 = self.fc3(x)\n",
        "        L1 = self.ReLU(L1)\n",
        "\n",
        "\n",
        "        L2 = self.layerLandmark1(L1)\n",
        "        L2 = self.ReLU(L2)\n",
        "\n",
        "        L3 = self.layerLandmark2(L2)\n",
        "        L3 = self.ReLU(L3)\n",
        "\n",
        "        L4 = self.layerLandmark3(L3)\n",
        "        L4 = self.ReLU(L4)\n",
        "\n",
        "        L5 = self.layerLandmark4(L4)\n",
        "        L5 = self.ReLU(L5)\n",
        "\n",
        "        L6 = self.layerLandmark5(L5)\n",
        "        outL = self.ReLU(L6)\n",
        "\n",
        "\n",
        "        # B1 = self.fc_bn3(L1) \n",
        "        T0 = self.fc4(L1) \n",
        "        T0 = self.ReLU(T0)\n",
        "        # T0 = self.fc_bn4(T0)\n",
        "        T0 = T0.view(-1,256,14,14)\n",
        "\n",
        "\n",
        "\n",
        "        T1 = self.T1_(T0)\n",
        "        T2 = self.T2_(T1)\n",
        "        T3 = self.T3_(T2)\n",
        "        T4 = self.T4_(T3)\n",
        "\n",
        "        outT = self.ConvLast(T4)\n",
        "        if self.phase == \"train\":\n",
        "            return outL,  outT "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6bEqmFYrNyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Compose(object):\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "        \n",
        "    def __call__(self, x):\n",
        "        for t in self.transforms:\n",
        "            x = t(x)\n",
        "        return x\n",
        "    \n",
        "class Resize(object):\n",
        "    def __init__(self, input_size):\n",
        "        self.input_size = input_size\n",
        "        \n",
        "    def __call__(self, img):\n",
        "        img = img.resize((self.input_size, self.input_size),\n",
        "                                Image.BICUBIC)\n",
        "        return img\n",
        "    \n",
        "class Normalize_Tensor(object):\n",
        "    def __init__(self, color_mean, color_std):\n",
        "        self.color_mean = color_mean\n",
        "        self.color_std = color_std\n",
        "\n",
        "    def __call__(self, img):\n",
        "\n",
        "        # PIL画像をTensorに。大きさは最大1に規格化される\n",
        "        img = transforms.functional.to_tensor(img)\n",
        "\n",
        "        # 色情報の標準化\n",
        "        img = transforms.functional.normalize(\n",
        "            img, self.color_mean, self.color_std)\n",
        "\n",
        "        return img\n",
        "\n",
        "class ImageTransform():\n",
        "    def __init__(self, input_size, color_mean, color_std):\n",
        "        self.data_transform = {\n",
        "            'train' : Compose([\n",
        "                Resize(input_size),\n",
        "                Normalize_Tensor(color_mean, color_std)\n",
        "            ]),\n",
        "            'val' : Compose([\n",
        "                Resize(input_size),\n",
        "                Normalize_Tensor(color_mean, color_std)\n",
        "            ])\n",
        "        }\n",
        "    \n",
        "    def __call_(self, phase, img):\n",
        "        return self.data_transform[phase](img)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvB-OcCG8s8m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############### THIS CELL IS COPIED FROM DEEP SPEECH MOZILLA #########################\n",
        "\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tfv1\n",
        "from tensorflow.compat import dimension_value\n",
        "from tensorflow.contrib.image import dense_image_warp\n",
        "from tensorflow.contrib.image import interpolate_spline\n",
        "\n",
        "from tensorflow.python.framework import ops\n",
        "from tensorflow.python.ops import array_ops\n",
        "\n",
        "def _to_float32(value):\n",
        "    return tf.cast(value, tf.float32)\n",
        "\n",
        "def _to_int32(value):\n",
        "    return tf.cast(value, tf.int32)\n",
        "\n",
        "def _get_grid_locations(image_height, image_width):\n",
        "    \"\"\"Wrapper for np.meshgrid.\"\"\"\n",
        "    tfv1.assert_type(image_height, tf.int32)\n",
        "    tfv1.assert_type(image_width, tf.int32)\n",
        "\n",
        "    y_range = tf.range(image_height)\n",
        "    x_range = tf.range(image_width)\n",
        "    y_grid, x_grid = tf.meshgrid(y_range, x_range, indexing='ij')\n",
        "    return tf.stack((y_grid, x_grid), -1)\n",
        "\n",
        "\n",
        "def _expand_to_minibatch(tensor, batch_size):\n",
        "    \"\"\"Tile arbitrarily-sized np_array to include new batch dimension.\"\"\"\n",
        "    ndim = tf.size(tf.shape(tensor))\n",
        "    ones = tf.ones((ndim,), tf.int32)\n",
        "\n",
        "    tiles = tf.concat(([batch_size], ones), 0)\n",
        "    return tf.tile(tf.expand_dims(tensor, 0), tiles)\n",
        "\n",
        "\n",
        "def _get_boundary_locations(image_height, image_width, num_points_per_edge):\n",
        "    \"\"\"Compute evenly-spaced indices along edge of image.\"\"\"\n",
        "    image_height_end = _to_float32(tf.math.subtract(image_height, 1))\n",
        "    image_width_end = _to_float32(tf.math.subtract(image_width, 1))\n",
        "    y_range = tf.linspace(0.0, image_height_end, num_points_per_edge + 2)\n",
        "    x_range = tf.linspace(0.0, image_height_end, num_points_per_edge + 2)\n",
        "    ys, xs = tf.meshgrid(y_range, x_range, indexing='ij')\n",
        "    is_boundary = tf.logical_or(\n",
        "        tf.logical_or(tf.equal(xs, 0.0), tf.equal(xs, image_width_end)),\n",
        "        tf.logical_or(tf.equal(ys, 0.0), tf.equal(ys, image_height_end)))\n",
        "    return tf.stack([tf.boolean_mask(ys, is_boundary), tf.boolean_mask(xs, is_boundary)], axis=-1)\n",
        "\n",
        "\n",
        "def _add_zero_flow_controls_at_boundary(control_point_locations,\n",
        "                                        control_point_flows, image_height,\n",
        "                                        image_width, boundary_points_per_edge):\n",
        "    \"\"\"Add control points for zero-flow boundary conditions.\n",
        "     Augment the set of control points with extra points on the\n",
        "     boundary of the image that have zero flow.\n",
        "    Args:\n",
        "      control_point_locations: input control points\n",
        "      control_point_flows: their flows\n",
        "      image_height: image height\n",
        "      image_width: image width\n",
        "      boundary_points_per_edge: number of points to add in the middle of each\n",
        "                             edge (not including the corners).\n",
        "                             The total number of points added is\n",
        "                             4 + 4*(boundary_points_per_edge).\n",
        "    Returns:\n",
        "      merged_control_point_locations: augmented set of control point locations\n",
        "      merged_control_point_flows: augmented set of control point flows\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size = dimension_value(tf.shape(control_point_locations)[0])\n",
        "\n",
        "    boundary_point_locations = _get_boundary_locations(image_height, image_width,\n",
        "                                                       boundary_points_per_edge)\n",
        "    boundary_point_shape = tf.shape(boundary_point_locations)\n",
        "    boundary_point_flows = tf.zeros([boundary_point_shape[0], 2])\n",
        "\n",
        "    minbatch_locations = _expand_to_minibatch(boundary_point_locations, batch_size)\n",
        "    type_to_use = control_point_locations.dtype\n",
        "    boundary_point_locations = tf.cast(minbatch_locations, type_to_use)\n",
        "\n",
        "    minbatch_flows = _expand_to_minibatch(boundary_point_flows, batch_size)\n",
        "\n",
        "    boundary_point_flows = tf.cast(minbatch_flows, type_to_use)\n",
        "\n",
        "    merged_control_point_locations = tf.concat(\n",
        "        [control_point_locations, boundary_point_locations], 1)\n",
        "\n",
        "    merged_control_point_flows = tf.concat(\n",
        "        [control_point_flows, boundary_point_flows], 1)\n",
        "\n",
        "    return merged_control_point_locations, merged_control_point_flows\n",
        "\n",
        "\n",
        "def sparse_image_warp(image,\n",
        "                      source_control_point_locations,\n",
        "                      dest_control_point_locations,\n",
        "                      interpolation_order=2,\n",
        "                      regularization_weight=0.0,\n",
        "                      num_boundary_points=0,\n",
        "                      name='sparse_image_warp'):\n",
        "    \"\"\"Image warping using correspondences between sparse control points.\n",
        "    Apply a non-linear warp to the image, where the warp is specified by\n",
        "    the source and destination locations of a (potentially small) number of\n",
        "    control points. First, we use a polyharmonic spline\n",
        "    (`tf.contrib.image.interpolate_spline`) to interpolate the displacements\n",
        "    between the corresponding control points to a dense flow field.\n",
        "    Then, we warp the image using this dense flow field\n",
        "    (`tf.contrib.image.dense_image_warp`).\n",
        "    Let t index our control points. For regularization_weight=0, we have:\n",
        "    warped_image[b, dest_control_point_locations[b, t, 0],\n",
        "                    dest_control_point_locations[b, t, 1], :] =\n",
        "    image[b, source_control_point_locations[b, t, 0],\n",
        "             source_control_point_locations[b, t, 1], :].\n",
        "    For regularization_weight > 0, this condition is met approximately, since\n",
        "    regularized interpolation trades off smoothness of the interpolant vs.\n",
        "    reconstruction of the interpolant at the control points.\n",
        "    See `tf.contrib.image.interpolate_spline` for further documentation of the\n",
        "    interpolation_order and regularization_weight arguments.\n",
        "    Args:\n",
        "      image: `[batch, height, width, channels]` float `Tensor`\n",
        "      source_control_point_locations: `[batch, num_control_points, 2]` float\n",
        "        `Tensor`\n",
        "      dest_control_point_locations: `[batch, num_control_points, 2]` float\n",
        "        `Tensor`\n",
        "      interpolation_order: polynomial order used by the spline interpolation\n",
        "      regularization_weight: weight on smoothness regularizer in interpolation\n",
        "      num_boundary_points: How many zero-flow boundary points to include at\n",
        "        each image edge.Usage:\n",
        "          num_boundary_points=0: don't add zero-flow points\n",
        "          num_boundary_points=1: 4 corners of the image\n",
        "          num_boundary_points=2: 4 corners and one in the middle of each edge\n",
        "            (8 points total)\n",
        "          num_boundary_points=n: 4 corners and n-1 along each edge\n",
        "      name: A name for the operation (optional).\n",
        "      Note that image and offsets can be of type tf.half, tf.float32, or\n",
        "      tf.float64, and do not necessarily have to be the same type.\n",
        "    Returns:\n",
        "      warped_image: `[batch, height, width, channels]` float `Tensor` with same\n",
        "        type as input image.\n",
        "      flow_field: `[batch, height, width, 2]` float `Tensor` containing the dense\n",
        "        flow field produced by the interpolation.\n",
        "    \"\"\"\n",
        "\n",
        "    image = ops.convert_to_tensor(image)\n",
        "    source_control_point_locations = ops.convert_to_tensor(\n",
        "        source_control_point_locations)\n",
        "    dest_control_point_locations = ops.convert_to_tensor(\n",
        "        dest_control_point_locations)\n",
        "\n",
        "    control_point_flows = (\n",
        "        dest_control_point_locations - source_control_point_locations)\n",
        "\n",
        "    clamp_boundaries = num_boundary_points > 0\n",
        "    boundary_points_per_edge = num_boundary_points - 1\n",
        "\n",
        "    with ops.name_scope(name):\n",
        "        image_shape = tf.shape(image)\n",
        "        batch_size, image_height, image_width = image_shape[0], image_shape[1], image_shape[2]\n",
        "\n",
        "        # This generates the dense locations where the interpolant\n",
        "        # will be evaluated.\n",
        "        grid_locations = _get_grid_locations(image_height, image_width)\n",
        "\n",
        "        flattened_grid_locations = tf.reshape(grid_locations,\n",
        "                                              [tf.multiply(image_height, image_width), 2])\n",
        "\n",
        "        # flattened_grid_locations = constant_op.constant(\n",
        "        #     _expand_to_minibatch(flattened_grid_locations, batch_size), image.dtype)\n",
        "        flattened_grid_locations = _expand_to_minibatch(flattened_grid_locations, batch_size)\n",
        "        flattened_grid_locations = tf.cast(flattened_grid_locations, dtype=image.dtype)\n",
        "\n",
        "        if clamp_boundaries:\n",
        "            (dest_control_point_locations,\n",
        "             control_point_flows) = _add_zero_flow_controls_at_boundary(\n",
        "                 dest_control_point_locations, control_point_flows, image_height,\n",
        "                 image_width, boundary_points_per_edge)\n",
        "\n",
        "        flattened_flows = interpolate_spline(\n",
        "            dest_control_point_locations, control_point_flows,\n",
        "            flattened_grid_locations, interpolation_order, regularization_weight)\n",
        "\n",
        "        dense_flows = array_ops.reshape(flattened_flows,\n",
        "                                        [batch_size, image_height, image_width, 2])\n",
        "\n",
        "        warped_image = dense_image_warp(image, dense_flows)\n",
        "\n",
        "        return warped_image, dense_flows"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvjYFqcm8t6E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def image_warping(src_img, src_landmarks, dest_landmarks):\n",
        "    # expanded_src_landmarks = np.expand_dims(np.float32(src_landmarks), axis=0)\n",
        "    # expanded_dest_landmarks = np.expand_dims(np.float32(dest_landmarks), axis=0)\n",
        "    # expanded_src_img = np.expand_dims(np.float32(src_img) / 255, axis=0)\n",
        "\n",
        "    warped_img, dense_flows = sparse_image_warp(src_img,\n",
        "                          src_landmarks,\n",
        "                          dest_landmarks,\n",
        "                          interpolation_order=1,\n",
        "                          regularization_weight=0.1,\n",
        "                          num_boundary_points=2,\n",
        "                          name='sparse_image_warp')\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        out_img = sess.run(warped_img)\n",
        "        warp_img = np.uint8(out_img[:, :, :, :])\n",
        "    \n",
        "    return warp_img\n",
        "    \n",
        "def face_landmark(img):\n",
        "    X = np.zeros((img.shape[0], 72 ,2))\n",
        "    flag = []\n",
        "    for i in range(img.shape[0]):\n",
        "\n",
        "        landmark = face_landmarks(img[i].reshape(224,224,3))\n",
        "        resultant = []\n",
        "        try:\n",
        "            for j in list(landmark[0].keys()):\n",
        "                resultant += landmark[0][j] \n",
        "        except IndexError:\n",
        "            flag.append(i)\n",
        "            continue\n",
        "        X[i] = np.array(resultant)\n",
        "    return  X, flag"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s86Vf2YTHzvp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# checkpoint_path='/content/drive/My Drive/Speech2Face/models/face_decoder_warping/epoch_14_steps_0.pth'\n",
        "checkpoint_path=None\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Set Model\n",
        "model = DECODER('train').to(device)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "if checkpoint_path:\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    global_epoch = checkpoint[\"epoch\"]\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "\n",
        "# Set loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "criterion2 = nn.L1Loss()\n",
        "alpha = 0.0002\n",
        "beta  = 1.0\n",
        "\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4UAJu92re1f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b1e1e8d3-e639-4a9e-821e-101c55646922"
      },
      "source": [
        "#import argparse\n",
        "\n",
        "\n",
        "\n",
        "#from dataloader import EmbedImagePairs\n",
        "#from face_encoder_model import vgg_face_dag\n",
        "#from face_decoder_model import Generator\n",
        "\n",
        "\n",
        "global_epoch = 0\n",
        "\n",
        "def _save_model(epoch, model, optimizer, steps, fname):\n",
        "    torch.save({\n",
        "        'steps': steps,\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        }, join(output_dir_name, 'epoch_{}_steps_{}.pth'.format(epoch, steps)))\n",
        "\n",
        "\n",
        "def train_epoch(epoch,model, trn_dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "\n",
        "    running_loss = 0\n",
        "    dataset_len = len(trn_dataloader.dataset)\n",
        "\n",
        "    count = 0\n",
        "    numOfElements = 0\n",
        "    saving_steps = 50\n",
        "\n",
        "    for x,landmark, y in trn_dataloader:\n",
        "        \n",
        "        count += 1\n",
        "        x = x.squeeze().to(device)  \n",
        "\n",
        "        # y_img = cv2.cvtColor(np.einsum('abc->bca',y[0].numpy()*255),cv2.COLOR_BGR2RGB)\n",
        "        # cv2_imshow(y_img)\n",
        "\n",
        "        y = y.to(device)\n",
        "        landmark = landmark.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outLandmark, outputs  = model(x)\n",
        "\n",
        "        loss2 = criterion2(outputs, y)\n",
        "        loss1 = criterion(outLandmark, landmark)\n",
        "\n",
        "        loss = (alpha*loss1 + beta*loss2 )\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        # o_img = cv2.cvtColor(np.einsum('abc->bca',outputs[0].cpu().detach().numpy()*255),cv2.COLOR_BGR2RGB)\n",
        "        #cv2_imshow(o_img)\n",
        "        running_loss += loss.item() * x.size(0)\n",
        "        numOfElements += x.shape[0]\n",
        "        \n",
        "\n",
        "        \n",
        "        # outLandmark = outLandmark.cpu().detach().numpy()\n",
        "        # outLandmark = np.dstack((outLandmark[:,0::2],outLandmark[:,1::2]))\n",
        "        # print(\"land np img np \", outL_.shape, img_.shape)\n",
        "        # img1 = np.einsum('dabc->dbca',outputs.cpu().detach().numpy()*255)\n",
        "        # img1 = (img1).astype(np.uint8)\n",
        "        #print(\"img_t \",img_t.shape, img_t[0])\n",
        "        # src, flag = face_landmark(img1)\n",
        "        # if flag:\n",
        "        #     for r in flag:\n",
        "        #         src[r] = outLandmark[r]\n",
        "        # result = image_warping(img1.astype(np.float32), src.astype(np.float32), outLandmark.astype(np.float32))\n",
        "\n",
        "        # w_img = cv2.cvtColor(result[0], cv2.COLOR_BGR2RGB)\n",
        "        #cv2_imshow(w_img)\n",
        "\n",
        "        print(\"loss:\", running_loss / (x.shape[0] * count) , \"Steps:\", str(numOfElements) + \"/\" + str(dataset_len) )\n",
        "\n",
        "        if (numOfElements % (x.shape[0] * saving_steps)) == 0:\n",
        "          _save_model(epoch, model, optimizer, numOfElements, output_dir_name)\n",
        "    \n",
        "    trn_loss = running_loss/len(trn_dataloader.dataset)\n",
        "    \n",
        "    return trn_loss\n",
        "\n",
        "\n",
        "def train(model, dataloaders, criterion, optimizer, device, output_dir_name, num_epochs=100):\n",
        "    trn_losses = []\n",
        "\n",
        "    # make dir where trained model saved\n",
        "    os.makedirs(output_dir_name, exist_ok=True)\n",
        "\n",
        "    # start at zero, end at num_epochs (including)\n",
        "    print(\"Start Training\")\n",
        "    for epoch in tqdm(range(global_epoch, num_epochs+1)):\n",
        "        # Training phase\n",
        "        trn_loss = train_epoch(epoch, model, dataloaders, criterion, optimizer, device)\n",
        "        trn_losses += [trn_loss]\n",
        "        print(\"Epoch: {}  || Loss: {}\".format(epoch, trn_loss))\n",
        "    \n",
        "        _save_model(epoch, model, optimizer, 0, output_dir_name)\n",
        "\n",
        "\n",
        "base_dir='/content/drive/My Drive/Speech2Face/'\n",
        "input=\"vox1_dataset/unzippedFaces\"\n",
        "batch_size=128\n",
        "num_epochs=20\n",
        "\n",
        "# Load face-encoder model\n",
        "# face_encoder_path = join(base_dir, \"Pretrained\", \"vgg_face_dag.pth\")\n",
        "# face_encoder_model = vgg_face_dag(face_encoder_path).to(device)\n",
        "# color_mean, color_std = face_encoder_model.meta[\"mean\"], face_encoder_model.meta[\"std\"]\n",
        "# color_mean = [tmp / 255.0 for tmp in color_mean]\n",
        "# color_std = [tmp / 255.0 for tmp in color_std]\n",
        "\n",
        "# Set DataLoader\n",
        "input_path = '/content/Face_Feature/'\n",
        "print('Image Pairing')\n",
        "dataset = EmbedImagePairs(input_path)\n",
        "print('Creating dataloader')\n",
        "trn_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "output_dir_name = join(base_dir, \"models\", \"face_decoder_warping\")\n",
        "\n",
        "# Train\n",
        "train(model, trn_dataloader, criterion, optimizer, device, output_dir_name, num_epochs=num_epochs)\n",
        "print(\"Finish training\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/21 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Image Pairing\n",
            "Creating dataloader\n",
            "Start Training\n",
            "loss: 2.382193088531494 Steps: 128/7407\n",
            "loss: 2.054392158985138 Steps: 256/7407\n",
            "loss: 2.4831404288609824 Steps: 384/7407\n",
            "loss: 2.2200202345848083 Steps: 512/7407\n",
            "loss: 2.0598589897155763 Steps: 640/7407\n",
            "loss: 1.9956419070561726 Steps: 768/7407\n",
            "loss: 1.9558447769709997 Steps: 896/7407\n",
            "loss: 1.927164614200592 Steps: 1024/7407\n",
            "loss: 1.8812722629970975 Steps: 1152/7407\n",
            "loss: 1.8159977674484253 Steps: 1280/7407\n",
            "loss: 1.7370710643855007 Steps: 1408/7407\n",
            "loss: 1.681618109345436 Steps: 1536/7407\n",
            "loss: 1.6616719731917748 Steps: 1664/7407\n",
            "loss: 1.6234591220106398 Steps: 1792/7407\n",
            "loss: 1.575554859638214 Steps: 1920/7407\n",
            "loss: 1.5344884507358074 Steps: 2048/7407\n",
            "loss: 1.5062388216747957 Steps: 2176/7407\n",
            "loss: 1.4820720321602292 Steps: 2304/7407\n",
            "loss: 1.4561854569535506 Steps: 2432/7407\n",
            "loss: 1.4289794743061066 Steps: 2560/7407\n",
            "loss: 1.4011505643526714 Steps: 2688/7407\n",
            "loss: 1.3780344914306293 Steps: 2816/7407\n",
            "loss: 1.3596617216649263 Steps: 2944/7407\n",
            "loss: 1.3435796772440274 Steps: 3072/7407\n",
            "loss: 1.325410885810852 Steps: 3200/7407\n",
            "loss: 1.3062398594159346 Steps: 3328/7407\n",
            "loss: 1.2892546764126531 Steps: 3456/7407\n",
            "loss: 1.2758324827466692 Steps: 3584/7407\n",
            "loss: 1.2635305544425701 Steps: 3712/7407\n",
            "loss: 1.2494589686393738 Steps: 3840/7407\n",
            "loss: 1.2356351248679622 Steps: 3968/7407\n",
            "loss: 1.2229570969939232 Steps: 4096/7407\n",
            "loss: 1.2126683321866123 Steps: 4224/7407\n",
            "loss: 1.202831052681979 Steps: 4352/7407\n",
            "loss: 1.1924765263284955 Steps: 4480/7407\n",
            "loss: 1.1814971665541332 Steps: 4608/7407\n",
            "loss: 1.172387355082744 Steps: 4736/7407\n",
            "loss: 1.1640899902895878 Steps: 4864/7407\n",
            "loss: 1.1556083743388836 Steps: 4992/7407\n",
            "loss: 1.14616928845644 Steps: 5120/7407\n",
            "loss: 1.1373509502992398 Steps: 5248/7407\n",
            "loss: 1.1286618666989463 Steps: 5376/7407\n",
            "loss: 1.1215716974679815 Steps: 5504/7407\n",
            "loss: 1.1138796440579675 Steps: 5632/7407\n",
            "loss: 1.1057235876719156 Steps: 5760/7407\n",
            "loss: 1.0983721924864727 Steps: 5888/7407\n",
            "loss: 1.0913003784544923 Steps: 6016/7407\n",
            "loss: 1.0850004541377227 Steps: 6144/7407\n",
            "loss: 1.0785780950468413 Steps: 6272/7407\n",
            "loss: 1.072029528617859 Steps: 6400/7407\n",
            "loss: 1.0656567987273722 Steps: 6528/7407\n",
            "loss: 1.059639688867789 Steps: 6656/7407\n",
            "loss: 1.0540851973137766 Steps: 6784/7407\n",
            "loss: 1.0483225186665852 Steps: 6912/7407\n",
            "loss: 1.043057982488112 Steps: 7040/7407\n",
            "loss: 1.0377184163246835 Steps: 7168/7407\n",
            "loss: 1.0329558671566479 Steps: 7296/7407\n",
            "loss: 1.183548100137607 Steps: 7407/7407\n",
            "Epoch: 0  || Loss: 1.0287137395282724\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  5%|▍         | 1/21 [08:11<2:43:46, 491.31s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 0.7452476024627686 Steps: 128/7407\n",
            "loss: 0.7487684488296509 Steps: 256/7407\n",
            "loss: 0.7489922245343527 Steps: 384/7407\n",
            "loss: 0.7456096708774567 Steps: 512/7407\n",
            "loss: 0.7432042837142945 Steps: 640/7407\n",
            "loss: 0.7437833150227865 Steps: 768/7407\n",
            "loss: 0.745011934212276 Steps: 896/7407\n",
            "loss: 0.7455275654792786 Steps: 1024/7407\n",
            "loss: 0.7445623808436923 Steps: 1152/7407\n",
            "loss: 0.7439715266227722 Steps: 1280/7407\n",
            "loss: 0.7449884089556608 Steps: 1408/7407\n",
            "loss: 0.7447529484828314 Steps: 1536/7407\n",
            "loss: 0.7439993115571829 Steps: 1664/7407\n",
            "loss: 0.744098356791905 Steps: 1792/7407\n",
            "loss: 0.7433688640594482 Steps: 1920/7407\n",
            "loss: 0.7431864589452744 Steps: 2048/7407\n",
            "loss: 0.74335777759552 Steps: 2176/7407\n",
            "loss: 0.7423043184810214 Steps: 2304/7407\n",
            "loss: 0.7413695235001413 Steps: 2432/7407\n",
            "loss: 0.7413421720266342 Steps: 2560/7407\n",
            "loss: 0.7404796254067194 Steps: 2688/7407\n",
            "loss: 0.7392418574203145 Steps: 2816/7407\n",
            "loss: 0.7392229126847308 Steps: 2944/7407\n",
            "loss: 0.7385694136222204 Steps: 3072/7407\n",
            "loss: 0.7383058023452759 Steps: 3200/7407\n",
            "loss: 0.7378284885333135 Steps: 3328/7407\n",
            "loss: 0.7375544662828799 Steps: 3456/7407\n",
            "loss: 0.7370569450514657 Steps: 3584/7407\n",
            "loss: 0.7365592533144457 Steps: 3712/7407\n",
            "loss: 0.736439577738444 Steps: 3840/7407\n",
            "loss: 0.736049025289474 Steps: 3968/7407\n",
            "loss: 0.7355035319924355 Steps: 4096/7407\n",
            "loss: 0.7355900930635857 Steps: 4224/7407\n",
            "loss: 0.7355563623063704 Steps: 4352/7407\n",
            "loss: 0.7358673862048558 Steps: 4480/7407\n",
            "loss: 0.7354225897126727 Steps: 4608/7407\n",
            "loss: 0.7348619425618971 Steps: 4736/7407\n",
            "loss: 0.735019785793204 Steps: 4864/7407\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNNWVCZRrq-A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.eval()\n",
        "img = Image.open('/content/338x338-10-graziano-frontal.png')\n",
        "transform_fe = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            _Normalize_Tensor(color_mean, color_std)\n",
        "        ])\n",
        "\n",
        "\n",
        "t_img = transform_fe(img).unsqueeze(0).to(device)\n",
        "emb = face_encoder_model(t_img)\n",
        "land_out, outputs  = model(emb)\n",
        "display(img)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXzOID98Agc5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "o_img = cv2.cvtColor(np.einsum('abc->bca',outputs[0].cpu().detach().numpy()*255),cv2.COLOR_BGR2RGB)\n",
        "cv2_imshow(o_img)\n",
        "\n",
        "outLandmark = land_out.cpu().detach().numpy()\n",
        "# outLandmark = np.dstack((outLandmark[:,0::2],outLandmark[:,1::2]))\n",
        "\n",
        "outLandmark = outLandmark.reshape((-1, 72, 2))\n",
        "\n",
        "\n",
        "\n",
        "# print(\"land np img np \", outL_.shape, img_.shape)\n",
        "img1 = np.einsum('dabc->dbca',outputs.cpu().detach().numpy()*255)\n",
        "img1 = (img1).astype(np.uint8)\n",
        "# print(\"img_t \",img_t.shape, img_t[0])\n",
        "src, flag = face_landmark(img1)\n",
        "if flag:\n",
        "    for r in flag:\n",
        "        src[r] = outLandmark[r] \n",
        "\n",
        "\n",
        "\n",
        "for i in range(outLandmark.shape[1]):\n",
        "  outLandmark[0,i,0] = src[0, i, 0] if outLandmark[0,i,0] == 0 else outLandmark[0,i, 0]\n",
        "  outLandmark[0,i,1] = src[0, i, 1] if outLandmark[0,i,1] == 0 else outLandmark[0,i, 1]\n",
        "\n",
        "\n",
        "result = image_warping(img1.astype(np.float32), src.astype(np.float32), outLandmark.astype(np.float32))\n",
        "w_img = cv2.cvtColor(result[0], cv2.COLOR_BGR2RGB)\n",
        "cv2_imshow(w_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnQkReH2dR2w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def plotLandmarks(img, landmarks):\n",
        "  img_p = img.squeeze(0)\n",
        "  fig = plt.figure(figsize=(15, 5))\n",
        "  ax = fig.add_subplot(1, 3, 1)\n",
        "  ax.imshow(img_p)\n",
        "  ax = fig.add_subplot(1, 3, 2)\n",
        "  ax.scatter(landmarks[:,0], -landmarks[:,1], alpha=0.8)\n",
        "  ax = fig.add_subplot(1, 3, 3)\n",
        "  img2 = img_p.copy()\n",
        "\n",
        "  for p in landmarks[:].astype(np.uint8):\n",
        "      img2[p[1]-2:p[1]+2, p[0]-2:p[0]+2, :] = (255, 255, 255)\n",
        "      # note that the values -3 and +3 will make the landmarks\n",
        "      # overlayed on the image 6 pixels wide; depending on the\n",
        "      # resolution of the face image, you may want to change\n",
        "      # this value\n",
        "\n",
        "\n",
        "  ax.imshow(img2)\n",
        "  plt.show()\n",
        "\n",
        "plotLandmarks(img1, points)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vno3gdM0Pr0r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plotLandmarks(img1, outLandmark[0])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}