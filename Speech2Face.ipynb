{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Speech2Face.ipynb",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antoniomuso/speech2face/blob/master/Speech2Face.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Us6ergKlbJbh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nkKE256supL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip3 install face_recognition\n",
        "#! pip install --upgrade wandb\n",
        "#! wandb login f9cd4b35bf9733e5ead9d2b06e13ef2259b1284e"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5FJW1pc-YZw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import wandb\n",
        "#wandb.init(project=\"speech2face\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jXRWARmcebB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# path = \"/content/drive/My Drive/Speech2Face/vox\"\n",
        "# !curl --user voxceleb1912:0s42xuw6 -o \"/content/drive/My Drive/Speech2Face/ff/vox.zip\" http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_test_wav.zip\n",
        "\n",
        "\n",
        "\n",
        "! cp \"/content/drive/My Drive/Speech2Face/vox1_dataset/vox_audios/vox.zip\" /content\n",
        "! cp \"/content/drive/My Drive/Speech2Face/zippedFaces.tar.gz\" /content\n",
        "! cp \"/content/drive/My Drive/Speech2Face/vox1_dataset/vox1_meta.csv\" /content\n",
        "\n",
        "! tar zxvf zippedFaces.tar.gz\n",
        "! unzip vox.zip\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOqMHNfkwaCL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from os import listdir\n",
        "from os.path import join\n",
        "from torch.utils.data import Dataset\n",
        "import glob\n",
        "import itertools\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "device = 'cuda'\n",
        "vgg_weights_path = '/content/drive/My Drive/Speech2Face/Pretrained/vgg_face_dag.pth'\n",
        "face_decoder_weights_path = ''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZEnesbYDawu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reproducibility stuff\n",
        "\n",
        "import random\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXILePzTdsI_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##################### DEPENDECIES ###########################\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Vgg_face_dag(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Vgg_face_dag, self).__init__()\n",
        "        self.meta = {'mean': [129.186279296875, 104.76238250732422, 93.59396362304688],\n",
        "                     'std': [1, 1, 1],\n",
        "                     'imageSize': [224, 224, 3]}\n",
        "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu1_1 = nn.ReLU(inplace=True)\n",
        "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu1_2 = nn.ReLU(inplace=True)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
        "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu2_1 = nn.ReLU(inplace=True)\n",
        "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu2_2 = nn.ReLU(inplace=True)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
        "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu3_1 = nn.ReLU(inplace=True)\n",
        "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu3_2 = nn.ReLU(inplace=True)\n",
        "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu3_3 = nn.ReLU(inplace=True)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
        "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu4_1 = nn.ReLU(inplace=True)\n",
        "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu4_2 = nn.ReLU(inplace=True)\n",
        "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu4_3 = nn.ReLU(inplace=True)\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
        "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu5_1 = nn.ReLU(inplace=True)\n",
        "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu5_2 = nn.ReLU(inplace=True)\n",
        "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu5_3 = nn.ReLU(inplace=True)\n",
        "        self.pool5 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
        "        self.fc6 = nn.Linear(in_features=25088, out_features=4096, bias=True)\n",
        "        self.relu6 = nn.ReLU(inplace=True)\n",
        "        self.dropout6 = nn.Dropout(p=0.5)\n",
        "        self.fc7 = nn.Linear(in_features=4096, out_features=4096, bias=True)\n",
        "        self.relu7 = nn.ReLU(inplace=True)\n",
        "        self.dropout7 = nn.Dropout(p=0.5)\n",
        "        self.fc8 = nn.Linear(in_features=4096, out_features=2622, bias=True)\n",
        "\n",
        "    def forward(self, x0, is_fc8=False):\n",
        "      if not is_fc8:\n",
        "        x1 = self.conv1_1(x0)\n",
        "        x2 = self.relu1_1(x1)\n",
        "        x3 = self.conv1_2(x2)\n",
        "        x4 = self.relu1_2(x3)\n",
        "        x5 = self.pool1(x4)\n",
        "        x6 = self.conv2_1(x5)\n",
        "        x7 = self.relu2_1(x6)\n",
        "        x8 = self.conv2_2(x7)\n",
        "        x9 = self.relu2_2(x8)\n",
        "        x10 = self.pool2(x9)\n",
        "        x11 = self.conv3_1(x10)\n",
        "        x12 = self.relu3_1(x11)\n",
        "        x13 = self.conv3_2(x12)\n",
        "        x14 = self.relu3_2(x13)\n",
        "        x15 = self.conv3_3(x14)\n",
        "        x16 = self.relu3_3(x15)\n",
        "        x17 = self.pool3(x16)\n",
        "        x18 = self.conv4_1(x17)\n",
        "        x19 = self.relu4_1(x18)\n",
        "        x20 = self.conv4_2(x19)\n",
        "        x21 = self.relu4_2(x20)\n",
        "        x22 = self.conv4_3(x21)\n",
        "        x23 = self.relu4_3(x22)\n",
        "        x24 = self.pool4(x23)\n",
        "        x25 = self.conv5_1(x24)\n",
        "        x26 = self.relu5_1(x25)\n",
        "        x27 = self.conv5_2(x26)\n",
        "        x28 = self.relu5_2(x27)\n",
        "        x29 = self.conv5_3(x28)\n",
        "        x30 = self.relu5_3(x29)\n",
        "        x31_preflatten = self.pool5(x30)\n",
        "        x31 = x31_preflatten.view(x31_preflatten.size(0), -1)\n",
        "        x32 = self.fc6(x31)\n",
        "        x33 = self.relu6(x32)\n",
        "        x34 = self.dropout6(x33)\n",
        "        x35 = self.fc7(x34)\n",
        "        x36 = self.relu7(x35)\n",
        "        x37 = self.dropout7(x36)\n",
        "        x38 = x37\n",
        "      else:\n",
        "        x38 = self.fc8(x0)\n",
        "\n",
        "      return x38\n",
        "\n",
        "\n",
        "def vgg_face_dag(weights_path=None, **kwargs):\n",
        "    \"\"\"\n",
        "    load imported model instance\n",
        "    Args:\n",
        "        weights_path (str): If set, loads model weights from the given path\n",
        "    \"\"\"\n",
        "    model = Vgg_face_dag()\n",
        "    if weights_path:\n",
        "        state_dict = torch.load(weights_path)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bl1vNDOGpKnx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_wav(wav_path):\n",
        "  def adjust(stft):\n",
        "    if stft.shape[1] == 601:\n",
        "      return stft\n",
        "    else:\n",
        "      return np.concatenate((stft,stft[:,0:601 - stft.shape[1]]),axis = 1)\n",
        "\n",
        "  wav, sr = librosa.load(wav_path,sr = 16000, duration = 6.0 ,mono = True)\n",
        "  spectro = librosa.core.stft(wav, n_fft = 512, hop_length = int(np.ceil(0.01 * sr)),win_length = int(np.ceil(0.025 * sr)) , window='hann', center=True,pad_mode='reflect') \n",
        "  spectroComplex = adjust(spectro)\n",
        "  converted = np.zeros((spectroComplex.shape[0], spectroComplex.shape[1], 2))\n",
        "  i = np.arange(spectroComplex.shape[0])\n",
        "  j = np.arange(spectroComplex.shape[1])\n",
        "  \n",
        "  converted[i,j[:,np.newaxis], 0] = spectroComplex[i,j[:,np.newaxis]].real\n",
        "  converted[i,j[:,np.newaxis], 1] = spectroComplex[i,j[:,np.newaxis]].imag\n",
        "\n",
        "  return converted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zZbwKKW-riA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_map_person2paths(path, format='wav'):\n",
        "  actor2data = dict()\n",
        "  \n",
        "  for person in listdir(path):\n",
        "    n_path = join(path, person)\n",
        "    files = glob.glob(n_path + '/**/*.'+format, recursive=True)\n",
        "    actor2data[person] = files\n",
        "  \n",
        "  return actor2data\n",
        "\n",
        "def load_metadata(path):\n",
        "  meta = pd.read_csv(path,sep='\\t')\n",
        "\n",
        "  meta = meta.drop('Gender',axis=1)\n",
        "  meta = meta.drop('Nationality',axis=1)\n",
        "  meta = meta.drop('Set',axis=1)\n",
        "  return meta\n",
        "\n",
        "def couple_data(voice_map, face_map, meta):\n",
        "  count = 0\n",
        "  out = []\n",
        "  for index, row in meta.iterrows():\n",
        "    if (row['VoxCeleb1 ID'] not in voice_map.keys()) or (row['VGGFace1 ID'] not in face_map.keys()):\n",
        "      count += 1\n",
        "      continue\n",
        "    # max(len(voice_map[row['VoxCeleb1 ID']]), face_map[row['VGGFace1 ID']])\n",
        "    coupled = list(zip(voice_map[row['VoxCeleb1 ID']], face_map[row['VGGFace1 ID']]))\n",
        "    out += coupled\n",
        "  \n",
        "  print(\"elements not found:\", count)\n",
        "  return out\n",
        "\n",
        "def create_coupled_list(path_voices, path_faces, metaP):\n",
        "  voice_map = get_map_person2paths(path_voices)\n",
        "  face_map = get_map_person2paths(path_faces, 'jpg')\n",
        "  meta = load_metadata(metaP)\n",
        "  return couple_data(voice_map, face_map, meta)\n",
        "\n",
        "class _Normalize_Tensor(object):\n",
        "    def __init__(self, color_mean, color_std):\n",
        "        self.color_mean = color_mean\n",
        "        self.color_std = color_std\n",
        "\n",
        "    def __call__(self, img):\n",
        "        # Convert image to Tensor\n",
        "        img = transforms.functional.to_tensor(img)\n",
        "\n",
        "        # Normalize image by the parameter of pre-trained face-encoder\n",
        "        img = transforms.functional.normalize(\n",
        "            img, self.color_mean, self.color_std)\n",
        "\n",
        "        return img\n",
        "\n",
        "class Speech2FaceDataset(Dataset):\n",
        "  def __init__(self, path_voices, path_faces, metaP, device, mean, std, size=224):\n",
        "        super().__init__()\n",
        "        self.path_voices = path_voices\n",
        "        self.path_faces = path_faces\n",
        "        self.size = size\n",
        "        self.coupled_list = create_coupled_list(path_voices, path_faces, metaP)\n",
        "        self.len = len(self.coupled_list)\n",
        "\n",
        "        self.color_mean = mean\n",
        "        self.color_std = std\n",
        "\n",
        "        # Transform input for face-encoder\n",
        "        self.transform_fe = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            _Normalize_Tensor(self.color_mean, self.color_std)\n",
        "        ])\n",
        "\n",
        "        self.transform_fd = transforms.Compose([\n",
        "            transforms.Resize((self.size, self.size)),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "  \n",
        "  def __len__(self):\n",
        "        return self.len\n",
        "    \n",
        "  def __getitem__(self, idx):\n",
        "        wav_p, img_p = self.coupled_list[idx]\n",
        "\n",
        "        wav = load_wav(wav_p)\n",
        "        img = Image.open(img_p)\n",
        "\n",
        "        #face_loc = face_locations(np_image)\n",
        "        img_normalized = self.transform_fe(img)\n",
        "        image_preprocessed = self.transform_fd(img)\n",
        "        \n",
        "        return torch.tensor(wav).reshape(2,257,601).float(), img_normalized, image_preprocessed\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "color_mean = [129.186279296875, 104.76238250732422, 93.59396362304688]\n",
        "color_std = [1, 1, 1]   \n",
        "color_mean = [tmp / 255.0 for tmp in color_mean]\n",
        "color_std = [tmp / 255.0 for tmp in color_std]\n",
        "\n",
        "\n",
        "data = Speech2FaceDataset('wav', 'unzippedFaces','vox1_meta.csv',\n",
        "                  mean = color_mean, std = color_std, device = device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5sDbBkIa7Fm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SpeechEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(SpeechEncoder, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(2, 64, kernel_size=4,stride=1) \n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=4,stride=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4,stride=1) \n",
        "        self.pooling1 = nn.MaxPool2d(kernel_size=(2,1), stride=(2,1))\n",
        "        self.conv4 = nn.Conv2d(128, 128, kernel_size=4,stride=1) \n",
        "        self.pooling2 = nn.MaxPool2d(kernel_size=(2,1), stride=(2,1))\n",
        "        self.conv5 = nn.Conv2d(128, 128, kernel_size=4,stride=1) \n",
        "        self.pooling3 = nn.MaxPool2d(kernel_size=(2,1), stride=(2,1))\n",
        "        self.conv6 = nn.Conv2d(128, 256, kernel_size=4,stride=1) \n",
        "        self.pooling4 = nn.MaxPool2d(kernel_size=(2,1), stride=(2,1))\n",
        "        self.conv7 = nn.Conv2d(256, 512, kernel_size=4,stride=1) \n",
        "        self.conv8 = nn.Conv2d(512, 512, kernel_size=4,stride=2) \n",
        "\n",
        "        self.conv9 = nn.Conv2d(512, 512, kernel_size=3,stride=2) # Queste due celle sono diverse\n",
        "        self.pooling5 = nn.AvgPool2d(kernel_size=(1,1), stride=1)# Queste due celle sono diverse\n",
        "\n",
        "        self.fc1 = nn.Linear(512 * 1 * 144, 4096)\n",
        "        self.fc2 = nn.Linear(4096, 4096)\n",
        "\n",
        "        self.batch_norm1 = nn.BatchNorm2d(64)\n",
        "        self.batch_norm2 = nn.BatchNorm2d(64)\n",
        "        self.batch_norm3 = nn.BatchNorm2d(128)\n",
        "        self.batch_norm4 = nn.BatchNorm2d(128)\n",
        "        self.batch_norm5 = nn.BatchNorm2d(128)\n",
        "        self.batch_norm6 = nn.BatchNorm2d(256)\n",
        "        self.batch_norm7 = nn.BatchNorm2d(512)\n",
        "        self.batch_norm8 = nn.BatchNorm2d(512)\n",
        "        self.batch_norm9 = nn.BatchNorm2d(512)\n",
        "      \n",
        "\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.batch_norm1(self.relu(self.conv1(x)))\n",
        "        out = self.batch_norm2(self.relu(self.conv2(out)))\n",
        "        out = self.batch_norm3(self.relu(self.conv3(out)))\n",
        "        out = self.pooling1(out)\n",
        "        out = self.batch_norm4(self.relu(self.conv4(out)))\n",
        "        out = self.pooling2(out)\n",
        "        out = self.batch_norm5(self.relu(self.conv5(out)))\n",
        "        out = self.pooling3(out)\n",
        "        out = self.batch_norm6(self.relu(self.conv6(out)))\n",
        "        out = self.pooling4(out)\n",
        "        out = self.batch_norm7(self.relu(self.conv7(out)))\n",
        "        out = self.batch_norm8(self.relu(self.conv8(out)))\n",
        "        out = self.batch_norm9(self.relu(self.pooling5(self.conv9(out))))\n",
        "\n",
        "        batch = out.shape[0]\n",
        "        out = out.view((batch, 512 * 1 * 144))\n",
        "        out = self.relu(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        return out\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wE_p5eTAw0HD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vgg = vgg_face_dag(vgg_weights_path)\n",
        "vgg.eval()\n",
        "vgg = vgg.to(device)\n",
        "\n",
        "model = SpeechEncoder()\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), eps=1e-04, betas=(0.5, 0.999))    #, weight_decay=0.95)\n",
        "#optimizer_decay = torch.optim.AdamW(model.parameters(), eps=1e-04, betas=(0.5, 0.999))    #, weight_decay=0.95)\n",
        "#optimizer = optimizer_decay\n",
        "\n",
        "datal = DataLoader(data, 3, True, num_workers=4)\n",
        "# W&B\n",
        "#wandb.watch(model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "082fWCi0rJG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from typing import Optional, Callable\n",
        "from tqdm.notebook import tqdm, trange\n",
        "\n",
        "def _save_model(epoch, model, optimizer, output_dir_name=\"/content/drive/My Drive/Speech2Face/models/speech_encoder/\"):\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        }, join(output_dir_name, 'adam_epoch_{}.pth'.format(epoch)))\n",
        "    \n",
        "def make_averager() -> Callable[[Optional[float]], float]:\n",
        "    \"\"\" Returns a function that maintains a running average\n",
        "\n",
        "    :returns: running average function\n",
        "    \"\"\"\n",
        "    count = 0\n",
        "    total = 0\n",
        "\n",
        "    def averager(new_value: Optional[float]) -> float:\n",
        "        \"\"\" Running averager\n",
        "\n",
        "        :param new_value: number to add to the running average,\n",
        "                          if None returns the current average\n",
        "        :returns: the current average\n",
        "        \"\"\"\n",
        "        nonlocal count, total\n",
        "        if new_value is None:\n",
        "            return total / count if count else float(\"nan\")\n",
        "        count += 1\n",
        "        total += new_value\n",
        "        return total / count\n",
        "\n",
        "    return averager\n",
        "\n",
        "def fit(\n",
        "    epochs: int,\n",
        "    train_dl: torch.utils.data.DataLoader,\n",
        "    model: torch.nn.Module,\n",
        "    VGG: torch.nn.Module, \n",
        "    opt: torch.optim.Optimizer,\n",
        "    tag: str,\n",
        "    device: str = \"cuda\",\n",
        "    restart_epoch: int = 0,\n",
        ") -> float:\n",
        "    \"\"\" Train the model and computes metrics on the test_loader at each epoch\n",
        "\n",
        "    :param epochs: number of epochs\n",
        "    :param train_dl: the train dataloader\n",
        "    :param test_dl: the test dataloader\n",
        "    :param model: the model to train\n",
        "    :param opt: the optimizer to use to train the model\n",
        "    :param tag: description of the current model\n",
        "    :param perm: if not None, permute the pixel in each image according to perm\n",
        "\n",
        "    :returns: accucary on the test set in the last epoch\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"VGG.training = \", VGG.training)\n",
        "    print(\"Speech2Face.training = \", model.training)\n",
        "\n",
        "    print(\"Starting training. Global epoch:\", restart_epoch)\n",
        "\n",
        "    lambda1 = 0.025\n",
        "    lambda2 = 200.0\n",
        "\n",
        "    for epoch in trange(epochs, desc=\"train epoch\"):\n",
        "        if restart_epoch != 0:\n",
        "          epoch = restart_epoch\n",
        "          restart_epoch = 0\n",
        "          print(\"Epoch updated, current value:\", epoch)\n",
        "\n",
        "        model.train()\n",
        "        train_loss_averager = make_averager()  # mantain a running average of the loss\n",
        "\n",
        "        # TRAIN\n",
        "        tqdm_iterator = tqdm(\n",
        "            enumerate(train_dl),\n",
        "            total=len(train_dl),\n",
        "            desc=f\"batch [loss: None]\",\n",
        "            leave=False,\n",
        "        )\n",
        "        for batch_idx, (wav, img_normal, img_preproc) in tqdm_iterator:\n",
        "            # send to device\n",
        "            wav, img_normal = wav.to(device), img_normal.to(device)\n",
        "\n",
        "            embedding = VGG(img_normal)# .to(device)\n",
        "\n",
        "            output = model(wav)\n",
        "\n",
        "            fvgg_s = VGG(output, True)\n",
        "            fvgg_f = VGG(embedding, True)\n",
        "\n",
        "            # loss = F.l1_loss(output, embedding)\n",
        "            loss = F.l1_loss(output, embedding) + lambda1 * loss_2(output, embedding) + lambda2 * dist_loss(fvgg_f, fvgg_s)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "            #print('here')\n",
        "\n",
        "            train_loss_averager(loss.item())\n",
        "\n",
        "            tqdm_iterator.set_description(\n",
        "                f\"train batch [avg loss: {train_loss_averager(None):.3f}]\"\n",
        "            )\n",
        "            tqdm_iterator.refresh()\n",
        "\n",
        "            del wav\n",
        "            del img_normal\n",
        "\n",
        "        # TEST\n",
        "        # model.eval()\n",
        "        # test_loss_averager = make_averager()  # mantain a running average of the loss\n",
        "        # correct = 0\n",
        "        # for data, target in test_dl:\n",
        "        #     # send to device\n",
        "        #     data, target = data.to(device), target.to(device)\n",
        "\n",
        "        #     if perm is not None:\n",
        "        #         data = permute_pixels(data, perm)\n",
        "\n",
        "        #     output = model(data)\n",
        "\n",
        "        #     test_loss_averager(F.cross_entropy(output, target))\n",
        "\n",
        "        #     # get the index of the max probability\n",
        "        #     pred = output.max(1, keepdim=True)[1]\n",
        "        #     correct += pred.eq(target.view_as(pred)).cpu().sum().item()\n",
        "\n",
        "        # accuracy = 100.0 * correct / len(test_dl.dataset)\n",
        "\n",
        "        print(\n",
        "            f\"Epoch: {epoch}\\n\"\n",
        "            f\"Train set: Average loss: {train_loss_averager(None):.4f}\\n\"\n",
        "            # f\"Test set: Average loss: {test_loss_averager(None):.4f}, \"\n",
        "            #f\"Accuracy: {correct}/{len(test_dl.dataset)} ({accuracy:.0f}%)\\n\"\n",
        "        )\n",
        "        _save_model(epoch, model, optimizer)\n",
        "        #torch.save(model.state_dict(), join(wandb.run.dir, 'model.pt'))\n",
        "        #wandb.log({\"Train set Average loss:\": train_loss_averager(None)})\n",
        "    # models_accuracy[tag] = accuracy\n",
        "    # return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEyFPE9NBXYP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# taken from https://discuss.pytorch.org/t/is-this-loss-function-for-knowledge-distillation-correct/69863\n",
        "\n",
        "def dist_loss(t, s):\n",
        "  T = 2\n",
        "  prob_t = F.softmax(t/T, dim=1)\n",
        "  log_prob_s = F.log_softmax(s/T, dim=1)\n",
        "  dist_loss = -(prob_t*log_prob_s).sum(dim=1).mean()\n",
        "  return dist_loss\n",
        "\n",
        "def loss_2(vf, vs):\n",
        "  return F.mse_loss(F.normalize(vf), F.normalize(vs))\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jETsH8Nx7JU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "check_path = None #\"/content/drive/My Drive/Speech2Face/models/speech_encoder/adam_epoch_2.pth\"\n",
        "\n",
        "def _load_checkpoint(checkpoint_path):\n",
        "  checkpoint = torch.load(checkpoint_path)\n",
        "  global_ep = checkpoint[\"epoch\"]\n",
        "  model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "  optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "  print(\"Loaded checkpoint, global epoch is: \", global_ep)\n",
        "  return global_ep\n",
        "\n",
        "if check_path is not None:\n",
        "  global_ep = _load_checkpoint(check_path)\n",
        "\n",
        "fit(10, datal, model, vgg, optimizer, \"Speech2Face Training\", restart_epoch=global_ep)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTVgjwV0dHL6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "model = SpeechEncoder()\n",
        "input = torch.unsqueeze(torch.tensor(converted).reshape(2,257,601), 0)\n",
        "\n",
        "model(input.type(torch.float32)).shape\n",
        "summary(model, (2,257,601))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}