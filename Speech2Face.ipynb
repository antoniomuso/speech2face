{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Speech2Face.ipynb",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antoniomuso/speech2face/blob/master/Speech2Face.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Us6ergKlbJbh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jXRWARmcebB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# path = \"/content/drive/My Drive/Speech2Face/vox\"\n",
        "# !curl --user voxceleb1912:0s42xuw6 -o \"/content/drive/My Drive/Speech2Face/ff/vox.zip\" http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_test_wav.zip\n",
        "\n",
        "\n",
        "\n",
        "! cp \"/content/drive/My Drive/Speech2Face/vox1_dataset/vox_audios/vox.zip\" /content\n",
        "! cp \"/content/drive/My Drive/Speech2Face/zippedFaces.tar.gz\" /content\n",
        "! cp \"/content/drive/My Drive/Speech2Face/vox1_dataset/vox1_meta.csv\" /content\n",
        "\n",
        "! tar zxvf zippedFaces.tar.gz\n",
        "! unzip vox.zip\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXILePzTdsI_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !unzip \"/content/drive/My Drive/Speech2Face/ff/vox.zip\" -d \"/content/drive/My Drive/Speech2Face/ff/ext/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBM5q_83fkuE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from os import listdir\n",
        "from os.path import join\n",
        "from torch.utils.data import Dataset\n",
        "import glob\n",
        "import itertools\n",
        "\n",
        "device = 'cuda'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meRq3GNUTrIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wav, sr = librosa.load('/content/drive/My Drive/Speech2Face/vox1_dataset/vox_audios/ext/wav/id10270/OhfKF8FSq3Y/00003.wav',sr = 16000, duration = 6.0 ,mono = True) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arqNwvJpU1pI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! ls -l \"wav\" | wc -l \n",
        "#spectro = librosa.core.stft(wav, n_fft = 512, hop_length = int(np.ceil(0.01 * sr)),win_length = int(np.ceil(0.025 * sr)) , window='hann', center=True,pad_mode='reflect')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xGGujur8DFE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "meta = pd.read_csv('/content/drive/My Drive/Speech2Face/vox1_dataset/vox1_meta.csv',sep='\\t')\n",
        "\n",
        "meta = meta.drop('Gender',axis=1)\n",
        "meta = meta.drop('Nationality',axis=1)\n",
        "meta = meta.drop('Set',axis=1)\n",
        "meta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zZbwKKW-riA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_map_person2paths(path, format='wav'):\n",
        "  actor2data = dict()\n",
        "  \n",
        "  for person in listdir(path):\n",
        "    n_path = join(path, person)\n",
        "    files = glob.glob(n_path + '/**/*.'+format, recursive=True)\n",
        "    actor2data[person] = files\n",
        "  \n",
        "  return actor2data\n",
        "\n",
        "def load_metadata(path):\n",
        "  meta = pd.read_csv(path,sep='\\t')\n",
        "\n",
        "  meta = meta.drop('Gender',axis=1)\n",
        "  meta = meta.drop('Nationality',axis=1)\n",
        "  meta = meta.drop('Set',axis=1)\n",
        "  return meta\n",
        "\n",
        "def couple_data(voice_map, face_map, meta):\n",
        "  count = 0\n",
        "  out = []\n",
        "  for index, row in meta.iterrows():\n",
        "    if (row['VoxCeleb1 ID'] not in voice_map.keys()) or (row['VGGFace1 ID'] not in face_map.keys()):\n",
        "      count += 1\n",
        "      continue\n",
        "    # max(len(voice_map[row['VoxCeleb1 ID']]), face_map[row['VGGFace1 ID']])\n",
        "    coupled = list(zip(voice_map[row['VoxCeleb1 ID']], face_map[row['VGGFace1 ID']]))\n",
        "    out += coupled\n",
        "  \n",
        "  print(\"elements not found:\", count)\n",
        "  return out\n",
        "\n",
        "def create_coupled_list(path_voices, path_faces, metaP):\n",
        "  voice_map = get_map_person2paths(path_voices)\n",
        "  face_map = get_map_person2paths(path_faces, 'jpg')\n",
        "  meta = load_metadata(metaP)\n",
        "  return couple_data(voice_map, face_map, meta)\n",
        "\n",
        "class Dataloader(Dataset):\n",
        "  def __init__(self, path_voices, path_faces, metaP, device, size=64):\n",
        "        super().__init__()\n",
        "        self.path_voices = path_voices\n",
        "        self.path_faces = path_faces\n",
        "        self.size = size\n",
        "        self.coupled_list = create_coupled_list(path_voices, path_faces, metaP)\n",
        "        self.len = len(self.coupled_list)\n",
        "\n",
        "  def __len__(self):\n",
        "        return self.len\n",
        "    \n",
        "  def __getitem__(self, idx):\n",
        "        pass\n",
        "\n",
        "data = Dataloader('wav', 'unzippedFaces','vox1_meta.csv', device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZCVSaO-VHau",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def adjust(stft):\n",
        "  if stft.shape[1] == 601:\n",
        "    return stft\n",
        "  else:\n",
        "    return np.concatenate((stft,stft[:,0:601 - stft.shape[1]]),axis = 1)\n",
        "\n",
        "spectroComplex = adjust(spectro)\n",
        "converted = np.zeros((spectroComplex.shape[0], spectroComplex.shape[1], 2))\n",
        "i = np.arange(spectroComplex.shape[0])\n",
        "j = np.arange(spectroComplex.shape[1])\n",
        "\n",
        "converted[i,j[:,np.newaxis], 0] = spectroComplex[i,j[:,np.newaxis]].real\n",
        "converted[i,j[:,np.newaxis], 1] = spectroComplex[i,j[:,np.newaxis]].imag"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5sDbBkIa7Fm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SpeechEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(SpeechEncoder, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(2, 64, kernel_size=4,stride=1) \n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=4,stride=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4,stride=1) \n",
        "        self.pooling1 = nn.MaxPool2d(kernel_size=(2,1), stride=(2,1))\n",
        "        self.conv4 = nn.Conv2d(128, 128, kernel_size=4,stride=1) \n",
        "        self.pooling2 = nn.MaxPool2d(kernel_size=(2,1), stride=(2,1))\n",
        "        self.conv5 = nn.Conv2d(128, 128, kernel_size=4,stride=1) \n",
        "        self.pooling3 = nn.MaxPool2d(kernel_size=(2,1), stride=(2,1))\n",
        "        self.conv6 = nn.Conv2d(128, 256, kernel_size=4,stride=1) \n",
        "        self.pooling4 = nn.MaxPool2d(kernel_size=(2,1), stride=(2,1))\n",
        "        self.conv7 = nn.Conv2d(256, 512, kernel_size=4,stride=1) \n",
        "        self.conv8 = nn.Conv2d(512, 512, kernel_size=4,stride=2) \n",
        "\n",
        "        self.conv9 = nn.Conv2d(512, 512, kernel_size=3,stride=2) # Queste due celle sono diverse\n",
        "        self.pooling5 = nn.AvgPool2d(kernel_size=(1,1), stride=1)# Queste due celle sono diverse\n",
        "\n",
        "        self.fc1 = nn.Linear(512 * 1 * 144, 4096)\n",
        "        self.fc2 = nn.Linear(4096, 4096)\n",
        "\n",
        "        self.batch_norm1 = nn.BatchNorm2d(64)\n",
        "        self.batch_norm2 = nn.BatchNorm2d(64)\n",
        "        self.batch_norm3 = nn.BatchNorm2d(128)\n",
        "        self.batch_norm4 = nn.BatchNorm2d(128)\n",
        "        self.batch_norm5 = nn.BatchNorm2d(128)\n",
        "        self.batch_norm6 = nn.BatchNorm2d(256)\n",
        "        self.batch_norm7 = nn.BatchNorm2d(512)\n",
        "        self.batch_norm8 = nn.BatchNorm2d(512)\n",
        "        self.batch_norm9 = nn.BatchNorm2d(512)\n",
        "      \n",
        "\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.batch_norm1(self.relu(self.conv1(x)))\n",
        "        out = self.batch_norm2(self.relu(self.conv2(out)))\n",
        "        out = self.batch_norm3(self.relu(self.conv3(out)))\n",
        "        out = self.pooling1(out)\n",
        "        out = self.batch_norm4(self.relu(self.conv4(out)))\n",
        "        out = self.pooling2(out)\n",
        "        out = self.batch_norm5(self.relu(self.conv5(out)))\n",
        "        out = self.pooling3(out)\n",
        "        out = self.batch_norm6(self.relu(self.conv6(out)))\n",
        "        out = self.pooling4(out)\n",
        "        out = self.batch_norm7(self.relu(self.conv7(out)))\n",
        "        out = self.batch_norm8(self.relu(self.conv8(out)))\n",
        "        out = self.batch_norm9(self.relu(self.pooling5(self.conv9(out))))\n",
        "\n",
        "        batch = out.shape[0]\n",
        "        out = out.view((batch, 512 * 1 * 144))\n",
        "        out = self.relu(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        return out\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTVgjwV0dHL6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "model = SpeechEncoder()\n",
        "input = torch.unsqueeze(torch.tensor(converted).reshape(2,257,601), 0)\n",
        "\n",
        "model(input.type(torch.float32)).shape\n",
        "summary(model, (2,257,601))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}